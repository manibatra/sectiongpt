title,description,content,tokens
Kubernetes and Section ,Kubernetes API and Section,"Kubernetes and Section 
How does Section interact with Kubernetes
Kubernetes API and Section
What can you use the Kubernetes API to do with Section?
How can you use the Kubernetes API with Section?
What Kubernetes resources are supported by Section?
Container Resources
What storage systems are available for Section workloads?
Kubernetes and Section 
How does Section interact with Kubernetes

For more information about Kuberbetes please refer to the [Kubernetes Documentation](https://kubernetes.io/docs/home).

Using the Kubernetes API with Section allows you to implement important Kubernetes Resources that are supported by Section.",117
Kubernetes and Section ,What can you use the Kubernetes API to do with Section?,"Kubernetes and Section 
How does Section interact with Kubernetes
Kubernetes API and Section
What can you use the Kubernetes API to do with Section?
How can you use the Kubernetes API with Section?
What Kubernetes resources are supported by Section?
Container Resources
What storage systems are available for Section workloads?
Kubernetes and Section 
How does Section interact with Kubernetes

Once you've created a containerized application, you can use the Kubernetes API to do the following on the Section platform:

* Deploy your container to multiple locations
* Configure service discovery, so that your users will be routed to the best container instance
* Define more complex applications, such as composite applications that consist of more than one container
* Define the resource allocations for your system
* Define scaling factors, such as the number of containers per location, and what signals should be used to scale in and out
* Maintain your application code, configuration and deployment manifests in your own code management systems and image registries
* Control how the Adaptive Edge Engine schedules containers, performs health management, and routes traffic",214
Kubernetes and Section ,How can you use the Kubernetes API with Section?,"Kubernetes and Section 
How does Section interact with Kubernetes
Kubernetes API and Section
What can you use the Kubernetes API to do with Section?
How can you use the Kubernetes API with Section?
What Kubernetes resources are supported by Section?
Container Resources
What storage systems are available for Section workloads?
Kubernetes and Section 
How does Section interact with Kubernetes

Because Section is compatible with the Kubernetes API, you can use standard Kubernetes tools to interact with the system.

The most common tool is `kubectl`, which is the tool that this documentation is centered around.

When you use `kubectl` you will create a configuration context, which will connect your tool to Section. From there you can continue to use `kubectl` as you normally would with a single cluster.

As Section is Kubenetes API compatible, you'll find that many existing tools work without complication. For example, you could use `helm` to manage your system.

Check out [Getting started with Kubernetes API](/docs/guides/kubernetes-ui/kubernetes-api/basics/) for a step-by-step guide.",215
Kubernetes and Section ,What Kubernetes resources are supported by Section?,"Kubernetes and Section 
How does Section interact with Kubernetes
Kubernetes API and Section
What can you use the Kubernetes API to do with Section?
How can you use the Kubernetes API with Section?
What Kubernetes resources are supported by Section?
Container Resources
What storage systems are available for Section workloads?
Kubernetes and Section 
How does Section interact with Kubernetes

In your manifests you can use the following Kubernetes Resources
* Deployment
* ConfigMap
* Secret
* Service (ClusterIP and ExternalName, but not types NodePort nor LoadBalancer)
* HorizontalPodAutoscaler ([example](/explanations/horizontal-pod-autoscaler/))

Section will create and manage the following resources for you, i.e. you cannot create them yourself:
* Namespace
* NetworkPolicy
* ReplicaSet
* Pod

For specification of location strategies, Section recognizes a particular ConfigMap resource with a specific name of `location-optimizer`.

For engaging Section's [HTTP Ingress](/guides/http-extensions/http-ingress/), Section recognizes a particular Service resource with a specific name of `ingress-upstream`.",228
Kubernetes and Section ,Container Resources,"Kubernetes and Section 
How does Section interact with Kubernetes
Kubernetes API and Section
What can you use the Kubernetes API to do with Section?
How can you use the Kubernetes API with Section?
What Kubernetes resources are supported by Section?
Container Resources
What storage systems are available for Section workloads?
Kubernetes and Section 
How does Section interact with Kubernetes

When you define your Deployment objects, you can specify the CPU and RAM requests or limits for each container instance.

Use the standard Kubernetes methods for specifying your container's requirements.

Some notes:
* Please refer to the [plans and pricing](https://section.io/pricing/) to understand how your requests can impact your billing.
* Section may alter your YAML to ensure that request = limit.
  * If request and limit are not equal, Section will use the higher of the two values.
  * If only one of request or limit is specified, that value will be used for both request and limit.
* You cannot request ephemeral storage directly. Section will automatically apply the ephemeral storage limits when the deployment is created.",217
Kubernetes and Section ,What storage systems are available for Section workloads?,"Kubernetes and Section 
How does Section interact with Kubernetes
Kubernetes API and Section
What can you use the Kubernetes API to do with Section?
How can you use the Kubernetes API with Section?
What Kubernetes resources are supported by Section?
Container Resources
What storage systems are available for Section workloads?
Kubernetes and Section 
How does Section interact with Kubernetes

Section supports ephemeral storage.

Section will automatically apply ephemeral storage limits to your containers based on the container size you've selected.

Your application can use the ephemeral storage in its local filesystem to perform disk IO activities.

""Ephemeral"" means that there is no long-term guarantee about durability. You should take this into consideration in your application design.",143
Section API,Section API Learn how to interact with the Section API,"Section API
Learn how to interact with the Section API
Section’s API allows you to interact with all Section objects and resources in the manner you choose.

To get started using Section’s api please start by getting an [API Token](/guides/iam/api-tokens/) for your user. Then you'll be able to give commands such as this one, which returns all of the accounts to which the user belongs.

```bash
curl \
  --header ""Accept: application/json"" \
  --header ""section-token: SECTION_API_TOKEN"" \
  -X GET ""https://aperture.section.io/api/v1/account""
```

For the full documentation of available calls please visit our [interactive API documentation](https://aperture.section.io/api/ui/).",155
Horizontal Pod Autoscaler,Horizontal Pod Autoscaler (HPA),"Horizontal Pod Autoscaler
What is Horizontal Pod Autoscaler?
Horizontal Pod Autoscaler (HPA)
How to use the Horizontal Pod Autoscaler resource with Section?
Horizontal Pod Autoscaler
What is Horizontal Pod Autoscaler?

HPA - the Horizontal Pod Autoscaler is a Kubernetes extension that automatically adjusts the number of replicas of a deployment in response to the resource demand of a workload.

HPA is a feature available on Section Enterprise Accounts.

For more information on Kubernetes and HPA, see [Kubernetes docs](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/).",120
Horizontal Pod Autoscaler,How to use the Horizontal Pod Autoscaler resource with Section?,"Horizontal Pod Autoscaler
What is Horizontal Pod Autoscaler?
Horizontal Pod Autoscaler (HPA)
How to use the Horizontal Pod Autoscaler resource with Section?
Horizontal Pod Autoscaler
What is Horizontal Pod Autoscaler?

After you have [created a Project](/get-started/create-project) in Section, you can use the Horizontal Pod Autoscaler to automatically scale the number of replicas of the deployment.

- Create a yaml file, such as hpa.yaml with the following content:
```yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 50

```

- Apply it to your application:
```bash
kubectl apply -f hpa.yaml
```

- See your HPA object running on Section:
```bash
kubectl get hpa.v2beta2.autoscaling
```

See other supported [kubectl commands](/reference/kubectl-commands) you can use with the HPA resource.

## What parts of the Horizontal Pod Autoscaler spec are supported by Section?

- Section, supports the `autoscaling/v2beta2` version of the Horizontal Pod Autoscaler API object.
- The following fields (including subfields) of the Horizontal Pod Autoscaler spec are supported:
  - `scaleTargetRef`
  - `minReplicas`
  - `maxReplicas`
  - `metrics`
      - `type: Resource`
- When using a `Resource` metric, scaling is only supported based on the `cpu` and `memory` resources.
- The `maxReplicas` field can have the highest value of `20`.


## Adaptive Edge Engine(AEE) and Horizontal Pod Autoscaler (HPA)

The [AEE](/explanations/aee) and the HPA work together to provide a scalable container deployment that scales across
the globe and within a particular edge location.

While AEE deploys the deployment to new edge locations depending on the traffic requirements in a particular region, the HPA is used to scale the number of replicas of the deployment in a particular edge location based on the resource (CPU and/or memory) demand.",543
Section Billing,Plans,"Section Billing
Section Billing Processes
Plans
Free Plans
Standard and Pro Plans
Enterprise Plans
Credit Card Billing
Section Billing
Section Billing Processes",30
Section Billing,Free Plans,"Section Billing
Section Billing Processes
Plans
Free Plans
Standard and Pro Plans
Enterprise Plans
Credit Card Billing
Section Billing
Section Billing Processes

On sign up for a Section account and [creation of your first Free Project](/get-started/create-project/), you will receive a $0 invoice for your Project.   

You may have one Free Project per Account.  Additional Standard or Pro Projects can be added you account at any time.

Free projects are hibernated after a period of time, requiring regular interaction in the Section Console in order to remain active.",116
Section Billing,Standard and Pro Plans,"Section Billing
Section Billing Processes
Plans
Free Plans
Standard and Pro Plans
Enterprise Plans
Credit Card Billing
Section Billing
Section Billing Processes

Pricing and Limits for Standard and Pro Plans can be found at our [Pricing Overview](https://section.io/pricing/).

On creation of a Standard or Pro plan Project, you will be billed for the plan amount on the date of Project creation (or upgrade to the new plan) and you will receive and invoice by email to your User email address.

Thereafter, you will be billed monthly from that date at the Plan rate.",120
Section Billing,Enterprise Plans,"Section Billing
Section Billing Processes
Plans
Free Plans
Standard and Pro Plans
Enterprise Plans
Credit Card Billing
Section Billing
Section Billing Processes

Please [contact us](https://www.section.io/contact-us/) for help with Enterprise Plan options where you need addtiional resources, functionality or wish to combine a larger number of Standard or Pro Projects into a single Enterprise Plan billing structure.",79
Section Billing,Credit Card Billing,"Section Billing
Section Billing Processes
Plans
Free Plans
Standard and Pro Plans
Enterprise Plans
Credit Card Billing
Section Billing
Section Billing Processes

On creation of (or upgrade to) a Standard or Pro Plan you will be asked to enter (or select) a Credit Card for that Project.

![Add Payment](/img/docs/add-payment.png)

You may keep several payment methods on your Account and different payment methods can be used for different Projects.",92
Section Console Reference Application,Overview,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.

Use this codebase to build a more tailored portal experience for your users to interact with your product while abstracting the Section administrative interface.",158
Section Console Reference Application,License to Use,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Section grants permission to users of this Console Reference Application code to use, modify and build upon the code ongoing as needed, free of charge, for the purpose of building a tailored portal experience. Section grants these rights to current paying customers. This portal code is provided for reference purposes only and provided as-is with no warranties.

This is provided on the condition that the code is not:
1. Open sourced.
2. Made available to other parties outside of the conditions highlighted herein.
3. Used for purposes outside the scope of building a tailored portal for access to the Section platform.
4. Separately resold - either as a one-off or subscription service (royalties or license fees may apply without prior approval from Section).",257
Section Console Reference Application,Tools and technologies,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
This project is built upon Laravel, a PHP application framework. Services required to run this application (Nginx webserver, php-fpm, MySQL) are provided as docker images (see `dockerfiles/*` and `docker-compose.yml` for more detail).

Tooling required to develop on this codebase (e.g Composer, NodeJS) are included in the PHP image, and wrapper commands exist for ease of use (e.g ```ahoy composer``` and ```ahoy npm```).

The following tools are required in your local development environment:
-   [Ahoy CLI](https://github.com/ocean/ahoy/releases/tag/2.1.0)
-   [Docker and Docker Compose](https://www.docker.com/)",263
Section Console Reference Application,Getting Started,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.",111
Section Console Reference Application,Initial configuration,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
The majority of configuration required happens in the `.env` file. This file injects environment variables to the running application.

To begin, copy the `.env.example` file to `.env` and edit in your favorite text editor.",157
Section Console Reference Application,Basic configuration,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Start by editing the following values:

-   `APP_NAME`: The application name
-   `APP_COMPANY_NAME`: Your company name
-   `APP_COMPANY_URL`: Your company URL
-   `MAIL_FROM_ADDRESS`: The from email address associated with outgoing mail
-   `MAIL_FROM_NAME`: The from name associated with outgoing mail

These values are used throughout the portal, in email notifications, and on invoices (if Subscriptions are enabled).

Connect to your Section account by modifying the following values:
-   `SECTION_ACCOUNT`: Your Section account ID
-   `SECTION_APPLICATION_STACK`: The name of the stack to provision on create
-   `SECTION_CONFIG_FILE`: Path to custom JSON file in the repository
-   `SECTION_AUTH`: Basic authentication credentials for Aperture, create with ```echo -n user:password | base64```",284
Section Console Reference Application,reCAPTCHA support,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
By default Google reCAPTCHA is enabled on the user registration form. This prevents spam registrations and is generally recommended.

1. Create a new v2 ""I'm not a robot"" tickbox from the [reCAPTCHA dashboard](https://www.google.com/u/1/recaptcha/admin).
2. Add ""localhost"" as a domain for local development, plus any other domains the application will run on.
3. Add the site key to the NOCAPTCHA_SITEKEY value in .env.
4. Add the secret key to the NOCAPTCHA_SECRET value in .env.

If you wish to disable reCAPTCHA set the NOCAPTCHA_ENABLED value to false.",246
Section Console Reference Application,Building the project,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Once all required software packages and basic configuration values are present we can build the project. To do this run the following command:

    ahoy build

This will build docker images and run local containers. It will also import database schema and import seed data. You should see the following info on success:

      --- App Info ---

      Site:  http://localhost:8002

      Mailhog:  http://localhost:8026

      Maria port: 3307

Visit the project on [http://localhost:8002](http://localhost:8002/) to get started.",231
Section Console Reference Application,Configuring look & feel,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
The starter kit provides some simple variables to alter color scheme and logo. Edit the `resources/sass/_variables.scss` file to update path to logo file (SVG recommended) and colors.

Once changes have been made you will need to recompile the frontend assets. Do this with:

    ahoy npm run production

If you wish to build non-minified versions for debugging build with:

    ahoy npm run development

You should commit the resulting built artefacts to the repository.",208
Section Console Reference Application,Mailhog & Email configuration,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Mailhog is an application that traps outgoing emails to make local development easier. This application runs Mailhog and is configured to trap email by default.

To view the Mailhog interface run ```ahoy info``` for the local service URL. Any verification, password reset, notification emails will be sent here.",171
Section Console Reference Application,Configuring email,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Many email providers are supported, including Mailgun, Postmark, AWS SES, SMTP, sendmail. Read the [Laravel docs](https://laravel.com/docs/8.x/mail) for detail on configuring the mail driver in the .env file.

To set the email sender address and name, use the following settings:

    EMAIL_ADDRESS_FROM=info@example.com

    EMAIL_ADDRESS_NAME=Info",192
Section Console Reference Application,Email verification,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
To require users to verify their email address for their account to have full access set the following in the .env file:

    EMAIL_VERIFY_REQUIRED=true",140
Section Console Reference Application,Billing and subscriptions,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
A simple subscriptions, billing and invoices solution is included in the starter kit. This allows for simple SaaS-like products to be built on the Section platform with minimal effort.

By default, subscription services are enabled. Subscription services are broken into two main categories: individuals and organizations. Organizations allow for teams to be created and users to be invited to access an account with varying levels of permission.

Out of the box, subscription features place limits on the numbers of projects and team members that can be created based on a number of customizable tiers defined in code.

To disable the need for a subscription to enable all features, set the following in the .env file. Alternatively, a specific individual/organization can be assigned the 'unlimited' subscription.

    SUBSCRIPTIONS_ENABLE=false",264
Section Console Reference Application,Production deployments,"Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
To run in a production environment the following changes should be made in the .env file.

    APP_DEBUG=false

    APP_URL=https://www.production-domain.com

    PROD_MODE=true

Ensure the following SECTION_AUTH env variable contains valid authentication with access to your production applications and environments.

If Subscriptions/Billing is active ensure the following values are provided:

    STRIPE_KEY_PROD 

    STRIPE_SECRET_PROD

Database values (`DB_*`) should be updated to point to a production database service. While it is possible to use the provided MySQL container it is not recommended, as it represents a SPOF (single point of failure). A cloud service like AWS, Azure, Google Cloud.

Email should be configured as per the ""Configuring email"" section above.",269
Section Console Reference Application,"Help, Support and Professional Services","Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
Overview
License to Use
Tools and technologies
Getting Started
Initial configuration
Basic configuration
reCAPTCHA support
Building the project
Configuring look & feel
Mailhog & Email configuration
Configuring email
Email verification
Billing and subscriptions
Production deployments
Help, Support and Professional Services
Section Console Reference Application
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.
This reference portal application is provided as-is with no Section specific support provided. If you would like help building on the scope of this codebase outside the reference application itself, we can facilitate a relationship with our partner [QuantCDN](https://www.quantcdn.io/) who are experts in building portal applications on top of this reference application. They are able to work with you in a Professional Services capacity to customize this console experience.

Contact Section at se@section.io and we will be happy to set up that connection.",216
HTTP Anycast,Conditions,"HTTP Anycast
How and why to use the Anycast location policy for your application
Conditions
LocationOptimizer configuration
HTTP Anycast
How and why to use the Anycast location policy for your application
You can use the Section Anycast networks to route traffic to your application.  Our Anycast networks are avaialble to Enterprise plans and is recommended when all of the following are true:
* your site domain is a zone apex (aka bare domain), e.g. `<your-domain-name>.com` instead of `www.<your-domain-name>.com`, and
* your DNS hosting does not support using an ANAME or ALIAS record at the zone apex to simulate a CNAME record, and
* you cannot change your DNS hosting to another DNS provider that does support ANAME records.",168
HTTP Anycast,LocationOptimizer configuration,"HTTP Anycast
How and why to use the Anycast location policy for your application
Conditions
LocationOptimizer configuration
HTTP Anycast
How and why to use the Anycast location policy for your application
To use Anycast on Section, you must set your location-optimizer configuration to use the `anycast` policy. The anycast policy has a required parameter called `network`.
The network parameter indicates which anycast IP you will be using (see below). The default value for the network parameter is ""global-045"".

A minimal LocationOptimizer configuration for anycast looks like this:
```json title=""minimal locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast""
  }
}
```

The minimal configuration is equivalent to:
```json title=""equivalent locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast"",
    ""network"":""global-045""
   }
}
```

An alternative configuration is:
```json title=""equivalent locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast"",
    ""network"":""global-103""
  }
}
```
### Anycast Networks and IP addresses
Both anycast networks are suitable to global traffic and the global-045 network is preferred for the majority of customers. However, if you have a preponderance of traffic in Australia and would benefit from finer geographic resolution there, then the global-103 network would be desirable.

The table below shows the IP address that you would enter into your DNS records to access these networks:

**Network** | **IP Address**
:-----------|:--------------|
global-045|45.154.183.183
global-103|103.107.226.226",384
Composable Edge Cloud,Current Providers,"Composable Edge Cloud
What is Section’s Composable Edge Cloud (CEC)?
Current Providers
Special Use CEC Options
Composable Edge Cloud
What is Section’s Composable Edge Cloud (CEC)?

We have selected a range of compute providers to deliver:
* Global Reach and Local Proximity
* Multi Provider Redundancy
* Extreme Scalability

The many locations available from these providers form a heterogeneous compute footprint on which the Section endpoint clusters (and hence your applications) may be run.

Current compute providers in the Composable Edge Cloud (CEC) include:
* Lumen
* DigitalOcean
* Equinix
* AWS
* GCP
* Azure
* Rackcorp

Section is constantly reviewing additional providers to add to the CEC and will update this list as new providers are added.",167
Composable Edge Cloud,Special Use CEC Options,"Composable Edge Cloud
What is Section’s Composable Edge Cloud (CEC)?
Current Providers
Special Use CEC Options
Composable Edge Cloud
What is Section’s Composable Edge Cloud (CEC)?

In addition to targeting workload generally at the Section CEC and letting the Section [Adaptive Edge Engine](/explanations/aee/) optimize the placement of the workload across the entire network, workload can be targeted at a subset of the compute locations for Special Use Options by declaring specific placement policies through the [Kubernetes API](/explanations/kubernetes/).

Examples may include restricting workload placement to specific locations, regions or providers in the Section CEC to achieve your desired performance, cost, security and compliance outcomes.

Read more about [how to define placement policies with Section](/guides/projects/set-edge-locations/).

In addition, compute locations and providers can be added to the CEC by customers to achieve their product, security or compliance goals. Once added to the CEC, those locations can be specifically identified for the customer’s workload by the CEC and workload addressed to that network by the AEE. [Contact Us](https://www.section.io/contact-us/) to find out more about bringing custom locations to the CEC.",253
Persistent Volumes,Persistent Storage on Section,"Persistent Volumes
Persistent Storage on Section
Persistent Storage on Section
How it Works
Managing Data Between Locations
Note
Persistent Volumes
Persistent Storage on Section

Application developers are able to use standard Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) to give their applications persistent storage that lives beyond the lifetime of a pod. Without persistent volumes, pods are able to read and write data to [ephemeral disk](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/), but the lifecycle of such data ends when the pod terminates. So a database of any kind placed on an ephmeral disk only lasts as long as the pod and then it is gone. Persistent storage solves this problem by giving data a lifecycle longer than that of a single pod.

Persistent storage can be used for:
* horizontal scaling of a pod, so that the multiple replicas have access to common data, such as a cache
* different pods of a microservice application, giving those pods a common source of truth for whatever data they might need
* data that needs to survive a pod that crashes and restarts
* a database for your distributed application, such as [Postgres](/tutorials/data/postgres-on-pvc), MySQL, SQLite, or others
* a document store
* a [persistent cache](https://www.varnish-software.com/solutions/varnish-enterprise/persistence/), which might be used with our [Varnish](/tutorials/varnish-caching.md) tutorial
* a KV store
* an object store, such as [MinIO](https://min.io/)",340
Persistent Volumes,How it Works,"Persistent Volumes
Persistent Storage on Section
Persistent Storage on Section
How it Works
Managing Data Between Locations
Note
Persistent Volumes
Persistent Storage on Section
Persistent volumes are managed by standard Kubernetes tooling, such as `kubectl` or .yaml files that contain the required fields and object specification. Section supports the storage class ""nfs-client"".

Persistent volumes in Section are created dynamically as a result of a claim: when your deployment to Section includes a [Persistent Volume Claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim), then a persistent volume is created dynamically. Then your pod mounts a volume with that claim to gain access to the data.

The claim may fail if resouces are not available. And writes to the volume can fail if you exceed the capacity.",170
Persistent Volumes,Managing Data Between Locations,"Persistent Volumes
Persistent Storage on Section
Persistent Storage on Section
How it Works
Managing Data Between Locations
Note
Persistent Volumes
Persistent Storage on Section

Persistent volumes are created within a cluster, within a Section location. Note that a persistent volume does not relocate with your project when [Section moves it due to changes in traffic](/explanations/aee.md). 

Strategies for sharing data between locations include configuring replication to occur between persistent volumes residing in different locations. We have a guide that explains how you can set this up (coming). You might choose to use a static location configuration for your project and then replicate data between them in order to provide a globally-distributed persistent data.

Strategies for making your persistent data available in new locations as Section creates them include the idea of continuously streaming changes to AWS S3, Azure Blob Storage, etc. [Litestream.io](https://litestream.io/) supports this idea for SQLite, allowing you to quickly restore data when your project becomes live in a new location.",210
Persistent Volumes,Note,"Persistent Volumes
Persistent Storage on Section
Persistent Storage on Section
How it Works
Managing Data Between Locations
Note
Persistent Volumes
Persistent Storage on Section
Section provisions the volume as resources allow. But the application developer is responsible for replication, data backup, disaster recovery, compliance and cryptographic requirements, and disk destruction.",66
Adaptive Edge Engine,LocationOptimizer,"Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
LocationOptimizer
Policy descriptions and examples
Dynamic
Static
Development
Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
The LocationOptimizer calls the **SolverServiceV1** strategy to solve for the locations of servers for your Project. This strategy implements various policies to deliver a variety of different deployment styles. You define parameters to pass to the policy to modify its operation to best deliver the desired results. The policies and their parameters are discussed in more detail below.",108
Adaptive Edge Engine,Policy descriptions and examples,"Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
LocationOptimizer
Policy descriptions and examples
Dynamic
Static
Development
Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine",39
Adaptive Edge Engine,Dynamic,"Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
LocationOptimizer
Policy descriptions and examples
Dynamic
Static
Development
Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
This policy uses traffic data (e.g., http requests per second) to find an optimal set of locations for your Project subject to your parameter specifications. The optimization function selects locations to minimize the geographic distance travelled by the requests sent by your end users. With the dynamic policy, the selected set of locations is expected to change over time as your Project's traffic patterns change. See [here](/explanations/traffic-signal/) for more information on the traffic signal used for dynamic location selection. 

Special handling is required when the Project traffic data is NULL or 0. In cases of no traffic signal (e.g., the Project is not receiving any HTTP requests) the LocationOptimizer will meet the **minimumLocations** condition by selecting arbitrary locations. The **mustInclude** parameters will be honored if present, but if there are fewer of these than **minimumLocations**, additional locations will be chosen regardless of geography.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""dynamic""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
* Optional parameters
  * **minimumLocations**: minimumLocations ensures that the Project will be deployed to a minimum number of locations to ensure availability. Default is 2.
  * **maximumLocations**: maximumLocations can be used to set a coarse upper limit on the number of locations. Default is 5.
  * **mustInclude**: mustInclude conditions represent “include this in my solution” conditions. Multiple conditions can be specified as key:value pairs. The LocationOptimizer selects one location per condition specified in the mustInclude array. For example, mustInclude = [{""region"": ""europe""},{""region"": ""northamerica""},{""region"": ""northamerica""}] will result in 3 locations, one in Europe and 2 in North America. See table of available terms [here](/reference/solver-service-parameters/). With the dynamic policy, your selected locations may include additional locations determined by traffic. Default is NULL.
  * **mustNotInclude**: These represent conditions that must not appear in the solution set. They are specified in the same manner as the **mustInclude** conditions. Default is NULL.
  * **chooseFrom**: This feature is only available on the Enterprise plan.

A dynamic LocationOptimizer configuration specifying all parameters looks like this (note that **minimumLocations** and **maximumLocations** are equal to their defaults in this example):

```yaml title=""location-optimizer.yaml""
{
apiVersion: v1
kind: ConfigMap
metadata:
  name: location-optimizer
  namespace: default
data:
  strategy: |
    {
      ""strategy"":""SolverServiceV1"",
      ""params"": {
        ""policy"": ""dynamic"",
        ""mustInclude"": [
          {""region"":""europe""}
        ],
        ""mustNotInclude"": [
          {""region"":""asia""}
        ],
        ""minimumLocations"": 2,
        ""maximumLocations"": 5
      }
    }
}
```",652
Adaptive Edge Engine,Static,"Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
LocationOptimizer
Policy descriptions and examples
Dynamic
Static
Development
Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
This policy results in a fixed set of locations that meet additional, stated requirements. Deploys your Project to a fixed set of locations that meet the **mustInclude** conditions. Upon the first implementation of the static policy, the SolverService will solve for a set of locations that meets the specifications. This set will continue to be used as long as it meets the specifications. If it does not, as when your specifications or the underlying network have been changed, then a new set is obtained.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""static"", ""mustInclude"":[{""region"":""europe""},{""region"":""oceania""}]}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
  * **mustInclude**: In this policy, the mustInclude parameter is the only input used to define the desired result. mustInclude conditions represent “include this in my solution” conditions. Multiple conditions can be specified as key:value pairs. The LocationOptimizer selects one location per condition specified in the mustInclude array. For example, mustInclude = [{""region"": ""europe""},{""region"": ""northamerica""},{""region"": ""northamerica""}] will result in 3 locations, one in Europe and 2 in North America. See table of available terms [here](/reference/solver-service-parameters/).
* Optional parameters
  * **chooseFrom**: This feature is only available on the Enterprise plan.

#### Anycast
This policy is available on the Enterprise plan. It should only be used under specific circumstances and requires supporting actions such as changing your DNS records. See the [Anycast explanation](/explanations/anycast/) to know if and how you should use our Anycast networks.

This policy results in a set of locations that are part of our Anycast network.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""anycast"",""network"":""global-045""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
  * **network**: Indicates which Anycast IP space you are using in your DNS records as explained [here](/explanations/anycast/).
* Optional parameters
  * None",504
Adaptive Edge Engine,Development,"Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
LocationOptimizer
Policy descriptions and examples
Dynamic
Static
Development
Adaptive Edge Engine
Learn About Section's Adaptive Edge Engine
This policy results in a set of locations that are dedicated to non-production and micro-environments.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""development""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
* Optional parameters
  * None

## TrafficDirector
The TrafficDirector is responsible for routing traffic to edge deployments and it has multiple strategies it can execute to manage this. Two DNS-based strategies are currently available with the default being a geo-DNS strategy that selects routes based on geographic proximity.

## HealthChecker
The HealthChecker executes one or more strategies for each Project to determine if the Project has been deployed/scheduled successfully and is ready to accept traffic. The HealthChecker also executes additional background strategies to monitor the health of the locations hosting Projects.

Two strategies are currently available to the HealthChecker configuration. Those strategies are:
* **deploymentMetricsHealthCheck**: Monitors platform metrics to detect that the minimum replicas per container are running for each deployment.
* **envHTTPHealthCheck**: An agent queries the Project with an HTTP POST request and monitors and interprets the response

The deploymentMetricsHealthCheck is included by default for all Project. The envHTTPHealthCheck is included by default for HTTP Projects.",298
Traffic Signal,Metric,"Traffic Signal
A reference of how the traffic signal is processed for consumption by the LocationOptimizer
Metric
Processing
Traffic Signal
A reference of how the traffic signal is processed for consumption by the LocationOptimizer
The base metric is the count of incoming HTTP requests per second (rps) collected at the ingress of the environment. The metric has labels for the 2-digit geohash encompassing the geo-ip of the end-user. The metric identifies the incoming traffic rate per 2-digit geohash quadrangle.",105
Traffic Signal,Processing,"Traffic Signal
A reference of how the traffic signal is processed for consumption by the LocationOptimizer
Metric
Processing
Traffic Signal
A reference of how the traffic signal is processed for consumption by the LocationOptimizer
The metric is smoothed with a moving average of 2 minute increments over a 30 minute span. In cases of low traffic, the smoothing span is extended to 24 hours to ensure that a stable signal is obtained. The ""low traffic"" condition is met when the total traffic rate for the environment is less than the **minimumUsagePerLocation** * **minimumLocations**. When total traffic is less than that product, the solution is dominated by the **minimumLocations** parameter and the challenge is to identify a stable selection of locations over time. At low levels, the traffic signal is prone to a low signal:noise ratio. Minor variations in traffic rate, variation that is not systematically related to valuable end-user behavior, can induce flapping in the location selections. To ameliorate this, we extend the smoothing period for low-traffic environments to enable better signal qualities and better location optimization.",223
Recommended Partners,Security,"Recommended Partners
See a list of recommended partners from Section and learn more about their tools
Security
Performance and Optimization
Recommended Partners
See a list of recommended partners from Section and learn more about their tools

| Partner                                                                  | Function                 | Description                                                                                 |
|--------------------------------------------------------------------------|--------------------------|---------------------------------------------------------------------------------------------|
| [PerimeterX](https://www.perimeterx.com)                                 | Bot management           | Non-human bot traffic detection and management.                                             |
| [Radware Bot Manager](https://www.radwarebotmanager.com)                 | Bot management           | Non-human bot traffic detection and management.                                             |
| [Signal Sciences](https://www.signalsciences.com)                        | Web application firewall | Real-time protection for an application under attack and integrates into DevOps toolchains. |
| [ThreatX](https://www.threatx.com)                                       | Web application firewall | Web application firewall based on dynamic rules.                                            |
| [Wallarm](https://www.wallarm.com)                                       | Web application firewall | Web application firewall based on dynamic rules.                                            |
| [QuantWAF](https://www.section.io/docs/classic/guides/modules/quantwaf/) | Web application firewall | Web application firewall based on dynamic rules.                                            |",256
Recommended Partners,Performance and Optimization,"Recommended Partners
See a list of recommended partners from Section and learn more about their tools
Security
Performance and Optimization
Recommended Partners
See a list of recommended partners from Section and learn more about their tools

| Partner                                | Function           | Description                                          |
|----------------------------------------|--------------------|------------------------------------------------------|
| [Optidash](https://optidash.ai)        | Image optimization | Optimize images to reduce page weight and load time. |
| [SiteSpect](https://www.sitespect.com) | A/B testing        | JavaScript and tag free A/B testing.                 |",121
Support,Platform Status,"Support
Find the best help and support options
Platform Status
Community Support
Lodge a Support Request
Other Questions?
Support
Find the best help and support options

View current platform status on our [Status Page](https://status.section.io/) to be alerted to any platform wide issues.",60
Support,Community Support,"Support
Find the best help and support options
Platform Status
Community Support
Lodge a Support Request
Other Questions?
Support
Find the best help and support options

Join our [Slack Community](https://sectionio-community.slack.com/) and chat about all things Edge, Apps, Kubernetes and the rest of the stuff you need help with.",72
Support,Lodge a Support Request,"Support
Find the best help and support options
Platform Status
Community Support
Lodge a Support Request
Other Questions?
Support
Find the best help and support options

In your Section Console, contact us for help or with any suggestions.

![Settings](/img/docs/support.png)",58
Support,Other Questions?,"Support
Find the best help and support options
Platform Status
Community Support
Lodge a Support Request
Other Questions?
Support
Find the best help and support options

You can always contact us here at Section by dropping us a line from [our Contact Form](https://www.section.io/contact-us/)",62
Security,SOC 2 Type II Compliance,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Section has successfully completed a System and Organization Controls (SOC) 2 Type II audit, performed by [Sensiba San Filippo, LLP](https://ssfllp.com/) (SSF).",116
Security,DDoS Protection,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Network-layer DDoS protection is included by default across the entire Section network to protect against all Layer 3/4 attacks. Section’s DDoS protection includes dually redundant DDoS protection including two of the world’s largest DDoS networks.",128
Security,Compute Framework Security,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information",75
Security,Container Isolation,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Applications cannot view or access processes outside of their isolated environment.",88
Security,Namespace NetworkPolicy Control,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Kubernetes NetworkPolicies restrict communications across namespaces.",86
Security,Private Repositories & Registries,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Maintain your application code, configuration and deployment manifests in your own code management systems and image registries.",97
Security,Security Platform Extensions,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information",75
Security,HTTP Extensions,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Section supports several containerized solutions that are available for general use by Section customers and include security-focused features.  These include:
* Activate IP blocking (via Section HTTP Ingress)
* Geo IP range blocking, and User Agent detection and blocking (via Varnish Cache)
* TLS Certificate Management (via Section HTTP Ingress)",142
Security,Additional Security Features,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information",75
Security,Geographic Delivery Control,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Control delivery to locations consistent with your GDPR or other compliance requirements.",89
Security,Vendor Delivery Control,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Restrict delivery nodes to a specific provider consistent with your compliance and security requirements.",92
Security,PCI Compliance,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Section is a certified PCI DSS Level 1 Service Provider.  Section utilizes [Tevora](https://www.tevora.com/) a Qualified Security Assessor (QSA) to conduct an annual compliance audit and provide a PCI DSS Attestation of Compliance (AOC). 

Section offers PCI DSS Level 1 Compliant Service as a premium service, enabling customers to build PCI-compliant systems that leverage all the benefits of Section.",168
Security,GDPR Compliance,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
Section’s privacy practices align to compliance with GDPR.",86
Security,Access Control,"Security
Security options and information
SOC 2 Type II Compliance
DDoS Protection
Compute Framework Security
Container Isolation
Namespace NetworkPolicy Control
Private Repositories & Registries
Security Platform Extensions
HTTP Extensions
Additional Security Features
Geographic Delivery Control
Vendor Delivery Control
PCI Compliance
GDPR Compliance
Access Control
Security
Security options and information
* SSO
* API tokens",83
Service Level Agreement,Service Level Agreement,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement",94
Service Level Agreement,"Contract Entered After January 1, 2022","Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
(for contract entered prior to Jan 1 2022 see relevant [terms and conditions](/docs/about/terms-of-service/sla/#contract-entered-prior-to-january-1-2022))",137
Service Level Agreement,SERVICE COMMITMENT,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
Section will use commercially reasonable efforts to make available the Customer’s Application Edge Network (defined below) with a Monthly Uptime Percentage (defined below) of at least 99.99% during any Measurement Period (the “Service Commitment”). In the event Section does not meet the Service Commitment, Customer will be eligible to receive a Service Credit as described below.",168
Service Level Agreement,DEFINITIONS,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
- ""Excluded Downtime” means:
    - Any unavailability caused by circumstances beyond Section’s reasonable control, including, for example, an act of God, act of government, flood, fire, earthquake, civil unrest, act of terror, strike, or other labor problem (other than one involving Section employees), Internet service provider failure or delay, non-Section service or application, or denial of service attack, any force majeure event, Internet access or related problems beyond the demarcation point of Section.
    - Any unavailability that results from any actions or inactions of Customer or any third party.
    - Any unavailability that results from Customer’s equipment, software or other technology and/or third-party equipment, software or other technology (other than third party equipment within our direct control); 
- “Monthly Uptime Percentage” is calculated by subtracting from 100% the percentage of minutes during the applicable Measurement Period in which all Utilized PoPs running Customer applications had no external connectivity to the Internet as a result of circumstances other than Excluded Downtime.   
- A “Service Credit” is a dollar credit, calculated as set forth below, that Section may credit back to a Customer account.  
- A “Utilized PoP” is a PoP to which the Customer’s application is deployed.
- A “PoP” is any Section infrastructure location available for hosting Customer applications.
- “Customer’s Application Edge Network” is defined as a Customer specification of 2 or more Utilized PoPs.
- “Measurement Period” means a calendar month.",415
Service Level Agreement,SERVICE CREDIT,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
Service Credits are calculated as a percentage of the total monthly services charges (other than professional service fees) paid by Customer for the Section service(s) that did not meet the Service Commitment for the Measurement Period in which the impact occurred in accordance with the schedule below.

| Monthly Uptime Percentage              | Service Credit Percentage           | 
|----------------------------------------|--------------------|
| Less than 99.99% but greater than or equal to 99.0% | 10% |
| Less than 99.0% but greater than or equal to 95.0% | 25% |
| Less than 95.0% | 100% | 

Section will apply any Service Credits for the impacted application(s) for the applicable Measurement Period, in the amount specified in the table above. A Service Credit will be applicable and issued only if the credit amount for the applicable monthly billing cycle is greater than one dollar ($1 USD). Unless otherwise provided, Customer’s sole and exclusive remedy for any unavailability, non-performance, or other failure by Section to meet the Uptime Percentage is the receipt of a Service Credit (if eligible) in accordance with the terms of this SLA.",334
Service Level Agreement,CREDIT REQUESTS AND PAYMENT PROCEDURES,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
To receive a Service Credit, Customer must submit a request to Section (support@section.io). To be eligible, the credit request must be received by us by the end of the second calendar month (UTC) after which the incident occurred.

If the Monthly Uptime Percentage applicable to the Measurement Period of such a request is confirmed by Section, then Section will issue the Service Credit to Customer within one Measurement Period following the Measurement Period in which Customer’s request is confirmed by Section.",189
Service Level Agreement,Service Level Agreement,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement",94
Service Level Agreement,"Contract Entered Prior to January 1, 2022","Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement",94
Service Level Agreement,SUMMARY,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
Section utilizes multiple, redundant data centres to ensure the Service is always available to you. Section shall use all reasonable commercial efforts to limit an outage to less than 30 minutes in a calendar month. In the unlikely event that an outage exceeds 30 minutes, you will be eligible to receive a credit as set forth below.

Section Service Level Agreement declares that the Section Network will be available 100% of the time. If Section fails to meet this SLA during any given calendar month, Customer's account will be credited.

Upon Customer's request, Section will issue a credit to Customer for Network Outages in an amount equal to one day’s worth of the Monthly Fee paid by Customer, multiplied by 10 minute period (or portion thereof rounded to the next hour) of the cumulative duration of such Network Outages during a particular month.",262
Service Level Agreement,DEFINITIONS,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
1. ""Section Network"" means the Section owned and operated website acceleration equipment.

2. ""Network Outage"" means a period of time that the Section Network was not available to deliver content to the internet for 2 or more consecutive 2 minute periods.

3. ""Monthly Fee"" consists solely of the base monthly fee paid by Customer for the affected Section service.",169
Service Level Agreement,CREDIT REQUEST AND PAYMENT PROCEDURES,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
Each request in connection with a Network Outage must be received by Section within thirty (30) days of the date the SLA was not met.

Each valid credit will be applied to an invoice of Customer no later than two billing cycles after Section receipt and verification of Customer's request. Credits are exclusive of any applicable taxes, duties, fees, or surcharges imposed by any controlling taxing authority.

The total amount credited to Customer shall not exceed the Monthly Fee paid by Customer for such month",192
Service Level Agreement,EXCEPTIONS,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
An Outage does not include website inaccessibility due to:

1. Failure or errors with your hardware, network, or website code;

2. DNS issues beyond Section’s direct control;

3. Your failure to report an Outage to Section support once you have discovered it;

4. Section scheduled or emergency maintenance;

5. Any actions or inactions of you or any third parties that are outside of Section’s direct control; or

6. Other events outside of Section’s reasonable control, including but not limited to Force Majeure.",204
Service Level Agreement,TERMINATION,"Service Level Agreement
Service Level Agreement
Service Level Agreement
Contract Entered After January 1, 2022
SERVICE COMMITMENT
DEFINITIONS
SERVICE CREDIT
CREDIT REQUESTS AND PAYMENT PROCEDURES
Service Level Agreement
Contract Entered Prior to January 1, 2022
SUMMARY
DEFINITIONS
CREDIT REQUEST AND PAYMENT PROCEDURES
EXCEPTIONS
TERMINATION
Service Level Agreement
Service Level Agreement
If you experience Outages for three consecutive calendar months, then either Party may terminate the Subscription Agreement in the following (fourth) month. Your Credits for Outages, which shall survive termination, shall be your sole and exclusive remedy and Section’s sole and exclusive liability for any failure to maintain the availability of the Services, or any claim based on an issue covered by this SLA. If you repeatedly submit false or unsubstantiated claims for Credits, Section may terminate your Subscription Agreement.",192
Acceptable Use Policy,A. THE FOLLOWING ACTIVITIES ARE EXPRESSLY PROHIBITED:,"Acceptable Use Policy
Acceptable Use Policy
A. THE FOLLOWING ACTIVITIES ARE EXPRESSLY PROHIBITED:
B. DISRUPTIONS & SECURITY:
C. REPORTING AUP VIOLATIONS:
Acceptable Use Policy
Acceptable Use Policy

1. Spam / Unsolicited Commercial Email.
Section does not tolerate spam, whether legal or illegal.
Bulk unsolicited commercial email is automatically considered spam, unless sent to recipients who have requested such messages through an opt-in procedure.
Take no action that leads to a Spamhaus listing for any Section-provided IP address, including your own IP addresses. If any action on your part causes a Spamhaus listing, you are required promptly to cooperate with Section, at your own expense, in clearing the IP address(es) in question from the Spamhaus RBL (and such assistance does not limit any other rights or remedies of Section).
Do not fake e-mail header information to misdirect or otherwise ""spoof"" another party for any reason, including without limitation to evade detection.
Do not set up a mail relay that is not password protected or that could allow anyone to send spam.
Do not host “payload” websites that are linked to spam emails, even if a third party sent those emails.
Do not provide domain name services to entities that violate any of these rules.

2. Intellectual property infringement, including violations of copyright, trademark, and patent rights, and use or distribution of pirated software.

3. Hacking or perpetration of any security breach.

4. Unauthorized access to any computer or system, including intrusion into or scanning of other Section accounts.

5. Dissemination of deliberately offensive material, including any message or information that is or may be threatening, libelous, obscene, or harassing.

6. Child pornography or any other activity harmful to minors.

7. Fraud.

8. Violation of privacy rights, publicity rights, trade secret rights, or information security, including dissemination of material non-public information about companies without authorization, as well as harvesting or collecting information about Website visitors without their express consent.

9. Use of third party software without a proper license or other appropriate permission, including Microsoft software.

10. Network attacks or any unfriendly effort to interfere with service on another network or server.

11. Virus distribution or distribution of any worm or other harmful code.

12. Use of Internet relay chat (“IRC”), including hosting of an IRC server, running IRC bots, use of a Section server as an IRC client or proxy, and use of IRC scripts or programs that interfere with service to other users on any server or network.

13. Illegal activities of any kind.",533
Acceptable Use Policy,B. DISRUPTIONS & SECURITY:,"Acceptable Use Policy
Acceptable Use Policy
A. THE FOLLOWING ACTIVITIES ARE EXPRESSLY PROHIBITED:
B. DISRUPTIONS & SECURITY:
C. REPORTING AUP VIOLATIONS:
Acceptable Use Policy
Acceptable Use Policy
Section may suspend the Service in whole or in part if it reasonably suspects an AUP violation. Customer will reimburse Section for any expenses resulting from Customer’s violation of the AUP, including legal fees. Section may also disable Customer’s service if Section suspects that such service is the target of an attack or in any way interferes with services provided to other customers, even if Customer is not at fault. Section does not issue refunds for terminating service due to any of the causes above. Customer is responsible for the use of its service, including use by hackers and other unauthorized third parties. Customer’s responsibility includes payment for exceeding transfer and bandwidth limits.

Except where Section specifically accepts such responsibility in Customer’s Signup (pursuant to the Terms of Service), Customer is responsible for maintaining security, including disaster recovery systems and backups.",218
Acceptable Use Policy,C. REPORTING AUP VIOLATIONS:,"Acceptable Use Policy
Acceptable Use Policy
A. THE FOLLOWING ACTIVITIES ARE EXPRESSLY PROHIBITED:
B. DISRUPTIONS & SECURITY:
C. REPORTING AUP VIOLATIONS:
Acceptable Use Policy
Acceptable Use Policy
Section requests that anyone with information about a violation of this AUP, or of Section’s Terms of Service, report it by contacting Section at its then publicised contact details.",90
Privacy Policy,WHAT THIS PRIVACY POLICY COVERS,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
This policy covers how Section treats personal information that Section collects and receives, including through the www.Section web site (the “Site”). As the Section site is only to be accessed by persons 18 years or older, we do not intend to collect any personal information from children under 13. Personal information is information about you that is personally identifiable, including your name, email address, billing address, phone number, credit card information, birth date, and other information, that is not otherwise publicly available. This policy does not apply to the practices of companies that Section does not own or control, or to people that Section does not employ or manage.",201
Privacy Policy,WHY WE COLLECT PERSONAL INFORMATION,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
Section collects your personal information because it helps us deliver a superior level of customer service. It enables us to give you convenient access to and/or ordering of our products and services, as well as to focus on categories of greatest interest to you. In addition, your personal information helps us keep you posted on the latest product announcements, software updates, special offers, and events that you might like to hear about. Our emails to you should always contain an unsubscribe option.",164
Privacy Policy,INFORMATION COLLECTION AND USE,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
Section may ask for your personal information when you’re purchasing a product, registering a product, participating in a forum, discussing service issues, downloading a software update, registering for an event or participating in an online survey. In such cases, we may collect personal information relevant to the situation, such as your name, mailing address, phone number, email address, and contact information; your credit card information and information about the products you own; and information relating to a support or service issue. Section gathers certain generic information with respect to your use of the Site, such as the frequency with which you visit the Site, the applications and the services you use, and the areas on the Site that you favour. We only use this type of data in aggregate – that is, we look at the data on a collective basis, in summary form, rather than on an individual basis. This data helps us determine the extent to which our customers use certain parts of our Site which, in turn, enables us to make it as appealing as possible. We may also provide statistical information about how our customers collectively use our Site to partners, advertisers, sponsors, and other companies with which we do business. We do this so they, too, can understand how often people use their areas of the Site in order for them to provide you with the best possible experience. This statistical information does not contain any personally identifiable information about you or any of our users. We may use also personal information to provide products that you have requested as well as for auditing, research, and analysis to improve our products.

Section may set and access cookies on your computer. A cookie is a small text file containing a unique identification number that is transferred from a Web site to the hard drive of your computer so that the site administrator may identify your computer and passively track its activities on the Site. This unique number identifies your web browser to our computer system. A cookie will not allow a Web site to learn any personally identifiable information (such as your real name and address) that you have not otherwise disclosed. Cookies allow us to automatically remember your web browser when you visit our site or Service. The use of cookies is an industry standard, and they are currently used on most major Web sites. It is possible to adjust your Web browser preferences to alert you when a cookie is sent to your hard drive, or to refuse cookies altogether. While we do not require you to use cookies, keep in mind that certain sites and services may not function properly if you set your browser to refuse all cookies.",580
Privacy Policy,DISCLOSURE EXCEPTIONS,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
Any information given to us shall be stored and managed with our best possible care, and will not be used in any ways to which you have not consented. We will not sell, rent or exchange such personality identifiable information to any other organization or entities, unless the user is first notified and expressly agreed to. Notwithstanding the above, we may indeed disclose personal information if required to do so in response to legal process, such as a court order or subpoena, or when such disclosure is made for establishment or exercise of legal rights or in defending against legal claim or as otherwise required by law. And, as we mention above, we may share with aggregated statistical information about the use of Section, its services and other future services.",217
Privacy Policy,MINORS,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
If a parent believes that his or her minor chilled has submitted personal information to us, he or she can contact us via e-mail or postal mail as listed below. We believe that parents should supervise their children’s online activities and consider using parental control tools available from online services and software manufacturers that help provide a kid-friendly online environment. These tools can also prevent children from otherwise disclosing online their name, address and other personal information without parental permission.",160
Privacy Policy,THIRD PARTY SITES,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
Links on this Site to other Web sites are provided as a convenience to you. Such linked sites are outside our control and responsibility and are not covered by this policy. If you use any such linked sites, you should consult the privacy policies posted on those Web sites.  Our site contains links to other Web sites. We are not responsible for the privacy practices or the content of such Web sites. Our site may also link to Web sites that feature our trademarks and names along with trademarks and names of other companies. You should look at the privacy policy on that co-branded Web site, as the co-branded Web site may not be under our control.

Applications you use or download from our site may contain third party digital rights management systems (“DRMS”) which may allow for communication between your software and the third party and utilize security features (e.g., preventing distribution of or access to the applications in the event of unauthorized use). These DRMS are subject to their own license agreements and you agree that we shall not be responsible for any loss or damage of any sort relating to the use of the DRMS or your dealings with such third parties. The applications may also contain DRMS produced by us which allow for communication between the applications you use or download and our systems and which collect information describing your computer system in order to prevent illegal or unauthorized use of the applications.

For reasons such as improving user experience and providing customized communications to our users, we may receive information about you from third party sources and our Web logs and add it to the information that we have received from you. Web logs automatically record anything a Web server sees, which may include e-mail addresses you enter into a form or pages viewed by a user at a particular IP address.",420
Privacy Policy,INFORMATION SHARING,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
We will not share your personally identifiable information with third parties, aside from entities that perform services for us, such as fulfilling orders or processing payment, that either are bound to comply with our privacy policy or have privacy policies that protect your information unless you have ‘opted-in’ to such sharing. If you have previously opted-in to such sharing under a prior privacy policy version, you are still considered to have opted-in under this privacy policy. As stated, and whether or not you have opted-in, we may use third parties to accept and process orders and payments for merchandise and products, including software, and such third parties may get access to your personal information for the purposes of providing services or products to you on our behalf. In addition, if you opt-out, we may share that information with third parties who send e-mails on our behalf so that they do not e-mail you.

However, we may disclose information you provide if required to do so by law or if we have a good faith belief that disclosure is necessary to (1) comply with the law or with legal process served on us; (2) protect and defend our rights or property; or (3) act in an emergency to protect someone’s safety.

We may request demographic information from you (for example, your age, education level or household income) from time to time. We will not share that information in a manner that identifies you as an individual with any other entity, unless we let you know that at the time of collection or we have your permission. When we share demographic information with third parties, we will give them aggregate information only.",396
Privacy Policy,SALE OF ASSETS,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
In the event that Section is ever sold, acquired, merged, liquidated, reorganized, or otherwise transferred, we reserve the right to transfer our user databases together with any personally identifiable information contained therein, to a third-party acquiring Section’s assets.",122
Privacy Policy,CHANGES TO THIS PRIVACY POLICY,"Privacy Policy
Privacy Policy
WHAT THIS PRIVACY POLICY COVERS
WHY WE COLLECT PERSONAL INFORMATION
INFORMATION COLLECTION AND USE
DISCLOSURE EXCEPTIONS
MINORS
THIRD PARTY SITES
INFORMATION SHARING
SALE OF ASSETS
CHANGES TO THIS PRIVACY POLICY
Privacy Policy
Privacy Policy
We may change our privacy policy from time to time by updating the posting, provided however that you will always know what information we gather, how we might use that information and whether we will disclose it to anyone.",114
Security and Compliance,SOC 2 TYPE II COMPLIANCE,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
Section has successfully completed a System and Organization Controls (SOC) 2 Type II audit, performed by [Sensiba San Filippo, LLP](https://ssfllp.com/) (SSF). Developed by the American Institute of Certified Public Accountants (AICPA), the SOC 2 information security standard is an audit report on the examination of controls relevant to the trust services criteria categories covering security, availability, processing integrity, confidentiality and privacy. A SOC 2 Type II report describes a service organization's systems and whether the design of specified controls meets the relevant trust services categories at a point-in-time. Section’s SOC 2 Type II report did not have any noted exceptions and therefore was issued with a “clean” audit opinion from SSF.

![SOC2 logo](/img/docs/soc2-logo.png)

We leverage [Drata](https://drata.com/)'s compliance automation platform to continuously monitor and report on security controls across the organization, ensuring that we are always meeting and exceeding the most up-to-date and highest security standards.",319
Security and Compliance,PCI DSS COMPLIANCE,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
Section is a certified PCI DSS Level 1 Service Provider.  Section utilizes [Tevora](https://www.tevora.com/) a Qualified Security Assessor (QSA) to conduct an annual compliance audit and provide a PCI DSS Attestation of Compliance (AOC).   Developed by the Payment Card Industry Security Standards Council, the PCI DSS standard was created to increase controls around cardholder data and to reduce credit card fraud. The AOC report describes a service organization’s systems and whether the design of specified controls protect cardholder data. An annual AOC is important to Section because it is an independent assessment that assures customers that Section meets all of our PCI compliance requirements.  If you wish to receive a copy of Sections AOC please work with your sales representative.  Section requires an NDA signed by senior management to receive a copy of our AOC.",285
Security and Compliance,CORPORATE GOVERNANCE,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Policies.** The Section security policies and procedures are focused on preserving security in our systems, processes, and practices.

**Information Security Team.** Our security team works across the entire business to secure and protect any sensitive information related to the Section service. This team also formally reviews policies and procedures.

**Risk Management.** Section undertakes risk assessment practices to understand, prevent and manage and information security risks.",190
Security and Compliance,PEOPLE,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Employee Screening.** Section screens new employees before they join the Section team and these screening activities may include criminal background and reference checks.

**Confidentiality.** Section employees are required to agree to protect and preserve any information they may view, process, or transmit as part of their job functions where that information may be deemed sensitive.

**Security Training.** Section trains our team to protect sensitive information and the devices they use. This training will include new hire awareness training and annual or ad-hoc training as required.",211
Security and Compliance,DATA PRIVACY,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Privacy Policy.** The Section [privacy policy](/docs/about/terms-of-service/privacy/) describes how Section will collect, use, and protect the customer information where customer may use or interact with Section websites or configuring services in the Section Console.

**Personal Data Transfer.** See our [terms of service](/docs/about/terms-of-service/terms-and-conditions/) for additional information about regarding processing of personal data. The Section services by default do not process personal data.",202
Security and Compliance,CHANGE MANAGEMENT,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Change Processes.** Section follows a procedural process when developing and deploying changes to technologies. Changes considered include systems and software which form the Section service

**Change Testing.** During the stages of development, Section will test changes. In advance of moving a proposed change to a production system, Section's team will confirm tests are successful.

**Change Notification.** Section prepare change notices to maintain awareness among the team. These notices are reviewed and approved by relevant team members involved with system management.

**Change Review.** Following the introduction of changes to the production systems, Section review and agree changes have been successful.",229
Security and Compliance,ACCESS MANAGEMENT,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Access Requests.** Section documents requests for access to the Section systems. Our team responsible for security, will then approve and grant access only where appropriate.

**Access Management.** Section amend any employees' levels of access to the Section systems subject to any change in an employee's role and/or responsibilities at Section.

**Access Review Process.** Section reviews access levels across the team and systems to ensure the appropriate access to Section systems and data is maintained.",198
Security and Compliance,SYSTEM AUTHENTICATION AND ACCOUNT ACCESS,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**User Accounts and Privileges.** Section assigns unique accounts per user to each user who needs to access Section systems so we can manage, understand and enforce user-level accountability.

**User Roles and Access Privileges.** Roles and access levels are assigned to users to restrict access per system to the level required for each individual users to conduct their responsibilities.

**Two factor authentication.** Section systems require two factor authentication.",190
Security and Compliance,APPLICATION SECURITY,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Secure development practices and processes.** Section trains our development and operations teams to prevent common vulnerabilities.",127
Security and Compliance,NETWORK AND INFRASTRUCTURE,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Security Scans and Tests.** Section performs vulnerability scans and security tests on the Section systems. Section considers and deals with the findings of these scans and tests in an appropriate way in order to assist with maintaining system security.

**Standards for System Configuration.** For maintenance of system security, Section has documented Standards for System Configuration which the team is required to follow. These standards cover a range of system configuration elements including (but not limited to) ports, services and protocols.

**Security Patching.** Section monitors lists of security vulnerabilities so that when new items are raised, those vulnerabilities can be addressed immediately. Patches and updates are applied subject to the time frame necessary for criticality of the vulnerability identified.",250
Security and Compliance,DATA ENCRYPTION,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Encrypted Data Transmission.** Section's platform supports TLS and will provide customers with a solution to encrypt connections to end users and to the customers' origin servers.

**Encryption Keys.** To maintain security of customer Encryption Keys, Section protects access to the encryption keys provisioned by Section customers.",166
Security and Compliance,CONTINUITY AND AVAILABILITY,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Fail Over.** Section's network is built to support fail over of traffic from any individual delivery node (PoP) within a network should that PoP become unavailable for any reason. In addition, Section can move customer data to alternate networks without any interruption should for some reason an individual network provided by Section become unresponsive.

**Redundancy.** Leveraging major cloud providers such as AWS and Azure, Section has multiple services and peering access points available to the Section networks.

**Monitoring.** Section's operations teams monitor a wide range of alerting interfaces to detect, monitor and understand degraded or otherwise detrimentally affected services 24x7x365.

**Reporting.** Section keep customers updated using real time alerting tools and methods (such as status.section.io). For specific customer issues, Section may contact a customer directly.",276
Security and Compliance,INCIDENTS,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Response Plan.** Section has a documented response plan to bring to bear in the case of an incident on the Section platform or systems. The plan is reviewed and updated subject to the changing nature of the Section platform and threat profile of the Internet. the plan include communication processes, systems and team management.

**Notifications of Unauthorized Access.** Section will notify customers who may be affected by any validated breach of the Section systems or any unauthorized disclosure of that customer's confidential information.",202
Security and Compliance,"ANALYSIS, MONITORING AND DETECTION","Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Analysis.** Section aggregate and securely store logs reflecting the activity on Section systems. Section monitor these logs to understand, alert, diagnose and manage security threats and or incidents.

**Monitoring.** Section use a number of systems to track system changes and ensure accountability and enforcement of the Section security standards.

**Detection.** Section has systems to help surface potential threats, incidents and intrusions. The Section team will be alerted to anomalies in these detection systems.",198
Security and Compliance,CUSTOMER DATA,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
**Cached Data.** Temporarily cached data (what data, where and for how long) is managed by Section customers. From time to time, Section may directly manage these settings on behalf of the customer should the need arise per law as customers permit.

**IP Addresses.** Section may retain indefinitely any non-anonymized, non-aggregated client or subscriber IP addresses associated with suspicious activity that may pose a risk to the Section network or our customers, or that are associated with administrative connections to the Section service.

**HTTP Requests.** Customer and end-user content which passes through the Section network in response to requests launched by end-users creates data in the Section systems which Section use over time to monitor and manage the Section system reliability, availability, performance and security.

**Physical Security.** Section relies on major cloud infrastructure providers such as AWS and Azure which have the physical environment security which accompanies these standards.

**Business Continuity.** Section has deployed PoPs across a number of zones and networks and as such can seamlessly move customer traffic between nodes and/or networks without customer downtime.",323
Security and Compliance,QUESTIONS?,"Security and Compliance
Security and Compliance
SOC 2 TYPE II COMPLIANCE
PCI DSS COMPLIANCE
CORPORATE GOVERNANCE
PEOPLE
DATA PRIVACY
CHANGE MANAGEMENT
ACCESS MANAGEMENT
SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
APPLICATION SECURITY
NETWORK AND INFRASTRUCTURE
DATA ENCRYPTION
CONTINUITY AND AVAILABILITY
INCIDENTS
ANALYSIS, MONITORING AND DETECTION
CUSTOMER DATA
QUESTIONS?
Security and Compliance
Security and Compliance
If you have any questions about security on the Section website, please email us at support@section.io.

Last updated: 28th June 2022",137
Kubernetes Dashboard,Kubernetes Dashboard Section Provides the Kubernetes Dashboard for Basic Monitoring,"Kubernetes Dashboard
Section Provides the Kubernetes Dashboard for Basic Monitoring
The open source Kubernetes Dashboard is deployed when you create your project and can be viewed with the ""Launch Dashboard"" button on the Project card. It is a general purpose, web-based UI for Kubernetes. It allows users to manage applications and troubleshoot them. You can read more about it on [GitHub](https://github.com/kubernetes/dashboard) or in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/).

![Kubernetes Dashboard](/img/docs/k8s-monitoring-dashboard.png)

We expect that most application developers will want to connect their favorite observability tooling to Section and we've provided [guides](/guides/monitor/exporting-telemetry/) to show how this is done.",163
Sample Datadog Dashboard,Datadog Dashboard,"Sample Datadog Dashboard
Importing and Using a Sample Datadog Dashboard
Datadog Dashboard
Sample Datadog Dashboard
Importing and Using a Sample Datadog Dashboard
Section provides a sample dashboard that you can import into your [Datadog account](https://app.datadoghq.com/) so that you can view metrics for your running projects.

![Datadog dashboard](/img/docs/datadog-dashboard.png)

In order to use it, first make a new dashboard.  Then copy the JSON below and paste it into the ""Import dashboard JSON..."" command that you'll find in the settings menu (gear icon) of your new dashboard.

```json title=""sample-dashboard.json""
{
   ""description"" : ""Example dashboard for monitoring workloads on Section."",
   ""id"" : ""vtr-j9w-3qj"",
   ""is_read_only"" : false,
   ""layout_type"" : ""ordered"",
   ""notify_list"" : [],
   ""reflow_type"" : ""fixed"",
   ""template_variables"" : [
      {
         ""available_values"" : [],
         ""default"" : ""*"",
         ""name"" : ""Environment_ID"",
         ""prefix"" : ""section_io_environment_id""
      },
      {
         ""available_values"" : [],
         ""default"" : ""*"",
         ""name"" : ""section_io_account_id"",
         ""prefix"" : ""section_io_account_id""
      }
   ],
   ""title"" : ""Section Demo"",
   ""widgets"" : [
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""Japan"",
                        ""formula"" : ""query2""
                     },
                     {
                        ""alias"" : ""US West"",
                        ""formula"" : ""query1""
                     },
                     {
                        ""alias"" : ""Europe West"",
                        ""formula"" : ""query3""
                     },
                     {
                        ""alias"" : ""Australia East"",
                        ""formula"" : ""query4""
                     },
                     {
                        ""alias"" : ""Australia West"",
                        ""formula"" : ""query5""
                     },
                     {
                        ""alias"" : ""Europe South"",
                        ""formula"" : ""query6""
                     },
                     {
                        ""alias"" : ""US East"",
                        ""formula"" : ""query7""
                     },
                     {
                        ""alias"" : ""Spain"",
                        ""formula"" : ""query8""
                     },
                     {
                        ""alias"" : ""Canada West"",
                        ""formula"" : ""query9""
                     },
                     {
                        ""alias"" : ""Canada East"",
                        ""formula"" : ""query10""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query2"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:x*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:9*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query3"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:u*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query4"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:r*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query5"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:q*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query6"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:s*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query7"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:d*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query8"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:e*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query9"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:c*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query10"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:f*,$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Traffic Rate by User Location (GeoHash)"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 1717949282816228,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 0
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""line"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""Memory Bytes"",
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_container_memory_usage_bytes{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Memory Usage"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 9006760416875348,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 4,
            ""y"" : 0
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{$Environment_ID,$section_io_account_id} by {traffic_monitor_region}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Traffic Rate by Hosting Region"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 7753833068014912,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 2
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""CPU Seconds"",
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_container_cpu_usage_seconds_total{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""CPU Usage"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 3621217572418110,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 4,
            ""y"" : 2
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_kube_pod_status_ready{$Environment_ID,$section_io_account_id} by {cluster_name}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Pod Replica Counts per PoP"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 7859494613804218,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 4
         }
      },
      {
         ""definition"" : {
            ""autoscale"" : true,
            ""precision"" : 2,
            ""requests"" : [
               {
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""aggregator"" : ""sum"",
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""avg:section.section_container_memory_usage_bytes{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""scalar""
               }
            ],
            ""time"" : {
               ""live_span"" : ""1mo""
            },
            ""timeseries_background"" : {
               ""type"" : ""area""
            },
            ""title"" : ""Past One Month Memory-(Hour??) Consumption"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""query_value""
         },
         ""id"" : 531138098289886,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 2,
            ""x"" : 4,
            ""y"" : 4
         }
      },
      {
         ""definition"" : {
            ""legend"" : {
               ""type"" : ""automatic""
            },
            ""requests"" : [
               {
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1"",
                        ""limit"" : {
                           ""order"" : ""desc""
                        }
                     }
                  ],
                  ""queries"" : [
                     {
                        ""aggregator"" : ""last"",
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_kube_pod_status_ready{$Environment_ID,$section_io_account_id} by {traffic_monitor_region}""
                     }
                  ],
                  ""response_format"" : ""scalar""
               }
            ],
            ""title"" : ""Current Pod Replica Counts per Hosting Region"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""sunburst""
         },
         ""id"" : 3390659206599044,
         ""layout"" : {
            ""height"" : 4,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 6
         }
      }
   ]
}
```",3006
Exporting to Grafana Cloud using Section,Monitoring with Grafana Cloud,"Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
Monitoring with Grafana Cloud
Useful Links
Deployment
Location Strategy
Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
Here we provide a simple example of how to scrape Section metrics into Grafana Cloud by using a deployment of the Prometheus Agent on Section itself as a project separate from your production workload project. The basic idea is that you need to run a Prometheus Agent that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics for your entire account, and writes the results into Grafana Cloud using “remote write”. (At the time of this writing Grafana Cloud has no way to do the scraping itself.) We have [another guide](/guides/monitor/exporting-telemetry/grafana-docker) that shows how to do this with `docker run` if you prefer.  But here we show how to make a deployment to Section that runs 24x7 in a single location. A single Prometheus agent collects metrics for all projects in your account.

![Grafana](/img/docs/monitor-grafana.png)

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`:
  * Is of the form: https://…/api/prom/push",352
Exporting to Grafana Cloud using Section,Useful Links,"Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
Monitoring with Grafana Cloud
Useful Links
Deployment
Location Strategy
Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)
* Learn about how to setup your [Prometheus Agent](https://grafana.com/docs/grafana-cloud/metrics-prometheus/)",106
Exporting to Grafana Cloud using Section,Deployment,"Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
Monitoring with Grafana Cloud
Useful Links
Deployment
Location Strategy
Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
The following deployment will run the Prometheus agent on Section. 

```yaml title=""grafana-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: grafanaagent
  name: grafanaagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafanaagent
  template:
    metadata:
      labels:
        app: grafanaagent
    spec:
      containers:
      - image: prom/prometheus
        imagePullPolicy: Always
        name: grafanaagent
        volumeMounts:
          - name: grafanaagent-config
            mountPath: /etc/prometheus
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
      volumes:
        - name: grafanaagent-config
          configMap:
            name: grafanaagent-config
```

Deploy it with `kubectl apply -f grafana-deployment.yaml`.

## Configuration
The following YAML file defines a ConfigMap with configuration for the Prometheus agent. Replace the following accordingly: `SECTION_ACCOUNT_ID`, `SECTION_API_TOKEN`, `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`, `GRAFANA_METRICS_INSTANCE_ID`, `GRAFANA_API_KEY`. Learn how to obtain the SECTION items [here](/guides/monitor/exporting-telemetry/metrics-endpoint/).

```yaml title=""configmap.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafanaagent-config
data:
  prometheus.yml: |
    # my global config
    global:
      scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).

      # Attach these labels to any time series or alerts when communicating with
      # external systems (federation, remote storage, Alertmanager).
      external_labels:
          monitor: 'section-monitor'

    # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
    rule_files:
      # - ""first.rules""
      # - ""second.rules""

    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
      - job_name: 'section-federation'
        metrics_path: '/prometheus/account/SECTION_ACCOUNT_ID/federate'
        params:
          'match[]':
            - '{__name__=~"".+""}'
        scheme: 'https'
        authorization:
          type: Bearer
          credentials: SECTION_API_TOKEN
        static_configs:
          - targets: ['console.section.io']
    remote_write:
    - url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
      basic_auth:
        username: GRAFANA_METRICS_INSTANCE_ID
        password: GRAFANA_API_KEY
```
Deploy it with `kubectl apply -f configmap.yaml`.",732
Exporting to Grafana Cloud using Section,Location Strategy,"Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
Monitoring with Grafana Cloud
Useful Links
Deployment
Location Strategy
Exporting to Grafana Cloud using Section
Exporting to Grafana Cloud by Deploying Your Agent to Section
By default, Section will run this project in 2 locations. We only need to collect metrics from your account once, so let's provide a location optimizer strategy that runs the project in only a single location. Read more about [location strategies](/explanations/aee/).
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```
Deploy it with `kubectl apply -f location-optimizer.yaml`. 

## View Metrics in Grafana
Login to your [Grafana Cloud account](https://grafana.com/products/cloud) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/grafana-sample-dashboard/) to get you started.",271
Exporting to Datadog using Docker,Monitoring with Datadog,"Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
Monitoring with Datadog
Useful Links
Configuration for a Datadog Agent Docker Container
View Metrics in Datadog
Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
Here we provide a simple example of how to scrape Section metrics into Datadog with `docker run`. The basic idea is that you need to run a [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics, and writes the results into your Datadog cloud instance. You may run this agent on any infrastructure of your choice. It could be in a docker container on your PC, hosted on a cloud provider, or even [hosted at Section](/guides/monitor/exporting-telemetry/datadog-section/). A single agent is used to scrape all of the metrics for your account.

![Datadog](/img/docs/monitor-datadog.png)

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.",309
Exporting to Datadog using Docker,Useful Links,"Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
Monitoring with Datadog
Useful Links
Configuration for a Datadog Agent Docker Container
View Metrics in Datadog
Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Specifically, you are using the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/), so you will want to learn about that.",130
Exporting to Datadog using Docker,Configuration for a Datadog Agent Docker Container,"Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
Monitoring with Datadog
Useful Links
Configuration for a Datadog Agent Docker Container
View Metrics in Datadog
Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
The following YAML file defines a docker container that will run the Datadog agent, scrape Section’s /federate endpoint, and write the results into your Datadog cloud instance.
* Replace `SECTION_ACCOUNT_ID` with your Section Account ID, typically of the form `1234`.
* Note that the ```Bearer TOKEN``` is not something that you are supposed to replace, just leave it alone.

```yaml title=""conf.yaml""
init_config:
instances:
  - namespace: section
    openmetrics_endpoint: https://console.section.io/prometheus/account/SECTION_ACCOUNT_ID/federate
    auth_token:
      reader:
        type: file
        path: /etc/datadog-agent/conf.d/openmetrics.d/token
        pattern: ^(.+)$
      writer:
        type: header
        name: Authorization
        value: ""Bearer <TOKEN>""
    metrics:
      - .+:
          type: gauge
```

The `token` file should be filled with one line containing your `SECTION_API_TOKEN`. Shown below is an obfuscated Section API token.

``` title=""token""
*********************************************************************4c57
```

The command to run the container above follows. Note the following:
* You'll need to properly specify the location of the files `datadog/openmetrics.d/conf.yaml` and `datadog/openmetrics.d/token`. In the example below they are specified as being in `/home/myhome`.
* And be sure to replace `DATADOG_API_KEY`.

```
docker run -d --name dd-agent \
    -v /var/run/docker.sock:/var/run/docker.sock:ro \
    -v /proc/:/host/proc/:ro \
    -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \
    -v /home/myhome/datadog/openmetrics.d/conf.yaml:/etc/datadog-agent/conf.d/openmetrics.d/conf.yaml \
    -v /home/myhome/datadog/openmetrics.d/token:/etc/datadog-agent/conf.d/openmetrics.d/token \
    -e DD_API_KEY=DATADOG_API_KEY \
    -e DD_SITE=""datadoghq.com"" \
    gcr.io/datadoghq/agent:7
```

If you are having trouble, try adding debugging to the list of command line arguments.

```
    -e DD_LOG_LEVEL=debug \
```",563
Exporting to Datadog using Docker,View Metrics in Datadog,"Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
Monitoring with Datadog
Useful Links
Configuration for a Datadog Agent Docker Container
View Metrics in Datadog
Exporting to Datadog using Docker
Support for Cloud-Native Monitoring Tools such as Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/datadog-sample-dashboard/) to get you started.",122
Exporting to New Relic using Section,Monitoring with New Relic,"Exporting to New Relic using Section
Running the New Relic Agent on Section
Monitoring with New Relic
Exporting to New Relic using Section
Running the New Relic Agent on Section
Coming.",44
Sample Grafana Cloud Dashboard,Grafana Cloud Dashboard,"Sample Grafana Cloud Dashboard
Importing and Using a Sample Grafana Cloud Dashboard
Grafana Cloud Dashboard
Sample Grafana Cloud Dashboard
Importing and Using a Sample Grafana Cloud Dashboard
Section provides a [sample dashboard](https://grafana.com/grafana/dashboards/17393-section-project/) that you can import into your [Grafana Cloud account](https://grafana.com/products/cloud/) so that you can view metrics for your running projects.

![Grafana Cloud dashboard](/img/docs/grafana-dashboard.png)

In order to use it, click on the link above and follow Grafana's instructions.

In order to understand the panels on the dashboard, it is useful to understand the [AEE workflow](/explanations/aee/).

Panels on the dashboard:
* Locations Selected - locations that are selected by the [AEE](/explanations/aee/), but maybe have not yet become ready yet.
* Is Ready - locations that are deployed and healthy and ready for traffic to be directed.
* Is Traffic Directed - the AEE has updated Internet records so that traffic will find the workload.
* Average RPS by geohash (map view) - average requests per second by 2-letter geohash of the end-user location (the origination of the request). The size of the circles are the magnitude of the signal. There are two circles representing two points in time. Learn about [geohash](https://chrishewett.com/blog/geohash-explorer/).
* RPS by Location - requests per second arriving at each location (from where the requests are served).
* Average RPS by geohash (time series view) - average requests per second by 2-letter geohash of the end-user location (the origination of the request). It is a 30 minute moving average. The intent of the 30 minute moving average is that it creates a clearer signal. This is the same metric as shown in the map view.
* Additional panels show traditional Prometheus metrics for your pods.",423
Exporting to Grafana using Docker,Monitoring with Grafana Cloud,"Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Monitoring with Grafana Cloud
Useful Links
Configuration for a Prometheus Docker Container
my global config
Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
A scrape configuration containing exactly one endpoint to scrape:
Here it's Prometheus itself.
Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Here we provide a simple example of how to scrape Section metrics into Grafana Cloud using `docker run`. The basic idea is that you need to run a Prometheus agent that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics, and writes the results into Grafana Cloud using “remote write”. (At the time of this writing Grafana Cloud has no way to do the scraping itself.)

![Grafana Cloud](/img/docs/monitor-grafana.png)

You may run the Prometheus agent on any infrastructure of your choice. It could be in a docker container on your PC, hosted on a cloud provider, or even hosted at Section. A single agent can be used to scrape all of the metrics for your account.

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`:
  * Is of the form: https://…/api/prom/push",344
Exporting to Grafana using Docker,Useful Links,"Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Monitoring with Grafana Cloud
Useful Links
Configuration for a Prometheus Docker Container
my global config
Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
A scrape configuration containing exactly one endpoint to scrape:
Here it's Prometheus itself.
Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)
* Learn about how to setup your [Prometheus Agent](https://grafana.com/docs/grafana-cloud/metrics-prometheus/)",137
Exporting to Grafana using Docker,Configuration for a Prometheus Docker Container,"Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Monitoring with Grafana Cloud
Useful Links
Configuration for a Prometheus Docker Container
my global config
Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
A scrape configuration containing exactly one endpoint to scrape:
Here it's Prometheus itself.
Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
The following YAML file configures a docker container that will run Prometheus, scrape Section’s /federate endpoint, and write the results into Grafana Cloud. Replace the following accordingly: `SECTION_ACCOUNT_ID`, `SECTION_API_TOKEN`, `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`, `GRAFANA_METRICS_INSTANCE_ID`, `GRAFANA_API_KEY`.

```yaml title=""prometheus.yaml""",174
Exporting to Grafana using Docker,my global config,"Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Monitoring with Grafana Cloud
Useful Links
Configuration for a Prometheus Docker Container
my global config
Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
A scrape configuration containing exactly one endpoint to scrape:
Here it's Prometheus itself.
Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
global:
  scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'section-monitor'",197
Exporting to Grafana using Docker,Load rules once and periodically evaluate them according to the global 'evaluation_interval'.,"Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Monitoring with Grafana Cloud
Useful Links
Configuration for a Prometheus Docker Container
my global config
Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
A scrape configuration containing exactly one endpoint to scrape:
Here it's Prometheus itself.
Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
rule_files:
  # - ""first.rules""
  # - ""second.rules""",109
Exporting to Grafana using Docker,A scrape configuration containing exactly one endpoint to scrape:,"Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Monitoring with Grafana Cloud
Useful Links
Configuration for a Prometheus Docker Container
my global config
Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
A scrape configuration containing exactly one endpoint to scrape:
Here it's Prometheus itself.
Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana",91
Exporting to Grafana using Docker,Here it's Prometheus itself.,"Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
Monitoring with Grafana Cloud
Useful Links
Configuration for a Prometheus Docker Container
my global config
Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
A scrape configuration containing exactly one endpoint to scrape:
Here it's Prometheus itself.
Exporting to Grafana using Docker
Support for Cloud-Native Monitoring Tools such as Grafana
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'section-federation'
    metrics_path: '/prometheus/account/SECTION_ACCOUNT_ID/federate'
    params:
      'match[]':
        - '{__name__=~"".+""}'
    scheme: 'https'
    authorization:
      type: Bearer
      credentials: SECTION_API_TOKEN
    static_configs:
      - targets: ['console.section.io']
remote_write:
- url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
  basic_auth:
    username: GRAFANA_METRICS_INSTANCE_ID
    password: GRAFANA_API_KEY
```

The command to run the container is as follows. Replace `/home/myhome` accordingly.

```
docker run \
    -p 9090:9090 \
    -v /home/myhome/prometheus.yml:/etc/prometheus/prometheus.yml \
    prom/prometheus
```",299
Exporting to Datadog using Section,Monitoring with Datadog,"Exporting to Datadog using Section
Running the Datadog Agent on Section
Monitoring with Datadog
Useful Links
Deployment
Location Strategy
Exporting to Datadog using Section
Running the Datadog Agent on Section
Here we provide a simple example of how to scrape Section metrics into Datadog by using a deployment of the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) on Section itself as a project separate from your production workload project. The basic idea is that you need to run a [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics for your entire account, and writes the results into your Datadog cloud instance. We have [another guide](/guides/monitor/exporting-telemetry/datadog-docker) that shows how to do this with `docker run` if you prefer.  But here we show how to make a deployment to Section that runs 24x7 in a single location. A single Datadog agent collects metrics for all projects in your account.

![Datadog](/img/docs/monitor-datadog.png)

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.",334
Exporting to Datadog using Section,Useful Links,"Exporting to Datadog using Section
Running the Datadog Agent on Section
Monitoring with Datadog
Useful Links
Deployment
Location Strategy
Exporting to Datadog using Section
Running the Datadog Agent on Section
* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Specifically, you are using the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/), so you will want to learn about that.",110
Exporting to Datadog using Section,Deployment,"Exporting to Datadog using Section
Running the Datadog Agent on Section
Monitoring with Datadog
Useful Links
Deployment
Location Strategy
Exporting to Datadog using Section
Running the Datadog Agent on Section
The following deployment will run the agent on Section.  Substitute `DATADOG_API_KEY` accordingly.
```yaml title=""datadog-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ddagent
  name: ddagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ddagent
  template:
    metadata:
      labels:
        app: ddagent
    spec:
      containers:
      - image: gcr.io/datadoghq/agent:7
        imagePullPolicy: Always
        name: ddagent
        env:
          - name: DD_API_KEY
            value: DATADOG_API_KEY
          - name: DD_SITE
            value: ""datadoghq.com""
          - name: DD_HOSTNAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName            
        volumeMounts:
          - name: ddagent-config
            mountPath: /etc/datadog-agent/conf.d/openmetrics.d
        resources:
          requests:
            memory: ""250Mi""
            cpu: ""250m""
          limits:
            memory: ""250Mi""
            cpu: ""250m""
      volumes:
        - name: ddagent-config
          configMap:
            name: ddagent-config
```

Deploy it with `kubectl apply -f datadog-deployment.yaml`.

## Configuration
The following YAML file defines a ConfigMap with configuration for the Datadog agent. It identifies the Section /federate endpoint for your account, names the metrics to collect, and identifies the SECTION_API_TOKEN.
* Replace `SECTION_ACCOUNT_ID` with your Section Account ID, typically of the form `1234`.
* The `token` section should be filled with one line containing your `SECTION_API_TOKEN`. It will be of the form `*********************************************************************4c57`.
* Learn how to obtain the SECTION items [here](/guides/monitor/exporting-telemetry/metrics-endpoint/).
* Note that the ```Bearer <TOKEN>``` is not something that you should replace, just leave it alone.
```yaml title=""configmap.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: ddagent-config
data:
  conf.yaml: |
    init_config:
    instances:
      - namespace: section
        openmetrics_endpoint: https://console.section.io/prometheus/account/SECTION_ACCOUNT_ID/federate
        auth_token:
          reader:
            type: file
            path: /etc/datadog-agent/conf.d/openmetrics.d/token
            pattern: ^(.+)$
          writer:
            type: header
            name: Authorization
            value: ""Bearer <TOKEN>""
        metrics:
          - section.+:
              type: gauge
  token: |
    SECTION_API_TOKEN
```
Deploy it with `kubectl apply -f configmap.yaml`.",657
Exporting to Datadog using Section,Location Strategy,"Exporting to Datadog using Section
Running the Datadog Agent on Section
Monitoring with Datadog
Useful Links
Deployment
Location Strategy
Exporting to Datadog using Section
Running the Datadog Agent on Section
By default, Section will run this project in 2 locations. We only need to collect metrics from your account once, so let's provide a location optimizer strategy that runs the project in only a single location. Read more about [location strategies](/explanations/aee/).
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```
Deploy it with `kubectl apply -f location-optimizer.yaml`. 

## View Metrics in Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/datadog-sample-dashboard/) to get you started.",262
Metrics Endpoint,Exporting Metrics,"Metrics Endpoint
Section Metrics Endpoint used by All Tools
Exporting Metrics
Connecting to Section’s /federate Endpoint
Useful Links
Metrics Endpoint
Section Metrics Endpoint used by All Tools
Section provides a /federate endpoint from which you can gather live metrics pertaining to your account. The systems underlying this endpoint are Prometheus systems. But the metrics returned are OpenMetrics compatible, and therefore you can use any compatible tooling in order to process them.

We have a number of guides to help you scrape your metrics from our endpoint and export them to [Grafana Cloud](https://grafana.com/products/cloud/), [Datadog](https://www.datadoghq.com/), and others.",145
Metrics Endpoint,Connecting to Section’s /federate Endpoint,"Metrics Endpoint
Section Metrics Endpoint used by All Tools
Exporting Metrics
Connecting to Section’s /federate Endpoint
Useful Links
Metrics Endpoint
Section Metrics Endpoint used by All Tools
To setup remote collection for any of the following 3rd party systems you will need to collect the following information from Section Console:

* `SECTION_ACCOUNT_ID` is 4 or more digits, as in “1234”. Get your account ID by following [these instructions](/docs/guides/kubernetes-ui/kubernetes-api/create-environment/#get-your-account-id).
* URL: console.section.io/prometheus/account/`SECTION_ACCOUNT_ID`/federate
* Authentication type: Bearer
* `SECTION_API_TOKEN`: get an API token using instructions [here](/guides/iam/api-tokens).",164
Metrics Endpoint,Useful Links,"Metrics Endpoint
Section Metrics Endpoint used by All Tools
Exporting Metrics
Connecting to Section’s /federate Endpoint
Useful Links
Metrics Endpoint
Section Metrics Endpoint used by All Tools
* Learn more about Prometheus and OpenMetrics at this [primer](https://scoutapm.com/blog/prometheus-metrics).",66
Using kubectl logs,Using kubectl logs Logging support by using kubectl,"Using kubectl logs
Logging support by using kubectl
Logs for your application can be seen by using `kubectl logs`. A few examples of using this command are shown below. See the [Kubernetes documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs) for complete details.

| Command                                          | Result                                                                                                            |
|--------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| kubectl logs MY_POD                              | # dump pod logs (stdout)                                                                                          |
| kubectl logs -l name=myLabel                     | # dump pod logs, with label name=myLabel (stdout)                                                                 |
| kubectl logs MY_POD --previous                   | # dump pod logs (stdout) for a previous instantiation of a container                                              |
| kubectl logs MY_POD -c MY_CONTAINER              | # dump pod container logs (stdout, multi-container case)                                                          |
| kubectl logs -l name=myLabel -c MY_CONTAINER     | # dump pod logs, with label name=myLabel (stdout)                                                                 |
| kubectl logs MY_POD -c MY_CONTAINER --previous   | # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container              |
| kubectl logs -f MY_POD                           | # stream pod logs (stdout)                                                                                        |
| kubectl logs -f MY_POD -c MY_CONTAINER           | # stream all pods logs with label name=myLabel (stdout)# stream pod container logs (stdout, multi-container case) |
| kubectl logs -f -l name=myLabel --all-containers | # stream all pods logs with label name=myLabel (stdout)                                                           |

Note that if a pod is deleted, manually or due to scaling, rescheduling, or if its node fails, then earlier logs will be unavailable for that pod. So [log streaming](/guides/monitor/logs/log-streaming/) is a technique you should leverage.",391
Log Streaming to External Tools,Differences from Metric Collection,"Log Streaming to External Tools
Logging support for cloud-native monitoring tools
Differences from Metric Collection
Log Streaming to External Tools
Logging support for cloud-native monitoring tools
It is worth noting that our [metric collection guides](/guides/monitor/exporting-telemetry/) are presented as a single deployment that scrapes metrics from all projects in an account. Whereas our log forwarders are presented as a deployment in each project to be logged.",89
New Relic Logs,Log Streaming to New Relic,"New Relic Logs
Logging streaming support for New Relic
Log Streaming to New Relic
Useful Links
Deployment
New Relic Logs
Logging streaming support for New Relic
Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for New Relic.

Obtain the following information from your instance of New Relic:
* `NEWRELIC_LICENSE_KEY`: Learn about New Relic [API Keys](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys).",124
New Relic Logs,Useful Links,"New Relic Logs
Logging streaming support for New Relic
Log Streaming to New Relic
Useful Links
Deployment
New Relic Logs
Logging streaming support for New Relic
* Get started with [New Relic](https://docs.newrelic.com/).
* Section has already built the container for [Fluentd with New Relic Output Plugin](https://github.com/section/fluentd-newrelic) for you use, named in the YAML below.",99
New Relic Logs,Deployment,"New Relic Logs
Logging streaming support for New Relic
Log Streaming to New Relic
Useful Links
Deployment
New Relic Logs
Logging streaming support for New Relic
The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project. Substitute `NEWRELIC_LICENSE_KEY` accordingly.

```yaml title=""newrelic-logs-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: ghcr.io/section/fluentd-newrelic:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: ""200Mi""
            cpu: ""200m""
          limits:
            memory: ""200Mi""
            cpu: ""200m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag all_section_logs
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>

    <filter all_section_logs> 
      @type record_transformer 
      <record> 
        offset ${record[""log""][""offset""]} 
      </record> 
      enable_ruby true
      remove_keys log 
    </filter>

    <match all_section_logs>
      @type newrelic
      license_key NEWRELIC_LICENSE_KEY
    </match>
```

Apply the above resources with `kubectl apply -f newrelic-logs-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in New Relic
Login to your [New Relic account](https://one.newrelic.com/) in order to see your logs.",605
Grafana Loki Logs,Log Streaming to Grafana Cloud,"Grafana Loki Logs
Logging streaming support for Grafana Loki
Log Streaming to Grafana Cloud
Useful Links
Deployment
Grafana Loki Logs
Logging streaming support for Grafana Loki
Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for Grafana Loki.

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_LOGGING_ENDPOINT`:
  * It is something like: https://logs-prod3.grafana.net",164
Grafana Loki Logs,Useful Links,"Grafana Loki Logs
Logging streaming support for Grafana Loki
Log Streaming to Grafana Cloud
Useful Links
Deployment
Grafana Loki Logs
Logging streaming support for Grafana Loki
* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)",59
Grafana Loki Logs,Deployment,"Grafana Loki Logs
Logging streaming support for Grafana Loki
Log Streaming to Grafana Cloud
Useful Links
Deployment
Grafana Loki Logs
Logging streaming support for Grafana Loki
The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project.

```yaml title=""grafana-loki-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: grafana/fluent-plugin-loki:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag udpsource
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>


    <match udpsource>
      @type loki
      url GRAFANA_LOGGING_ENDPOINT
      username GRAFANA_METRICS_INSTANCE_ID
      password GRAFANA_API_KEY
      extra_labels {""env"":""prod""}
    </match>
```

Apply the above resources with `kubectl apply -f grafana-loki-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in Grafana Loki
Login to your [Grafana Cloud account](https://grafana.com/products/cloud) in order to see your logs.",567
Datadog Logs,Log Streaming to Datadog,"Datadog Logs
Logging streaming support for Datadog
Log Streaming to Datadog
Useful Links
Deployment
Datadog Logs
Logging streaming support for Datadog
Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for Datadog.

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.",137
Datadog Logs,Useful Links,"Datadog Logs
Logging streaming support for Datadog
Log Streaming to Datadog
Useful Links
Deployment
Datadog Logs
Logging streaming support for Datadog
* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Section has already built the container for [Fluentd with Datadog Output Plugin](https://github.com/section/fluentd-datadog) for you use, named in the YAML below.",103
Datadog Logs,Deployment,"Datadog Logs
Logging streaming support for Datadog
Log Streaming to Datadog
Useful Links
Deployment
Datadog Logs
Logging streaming support for Datadog
The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project. Substitute `DATADOG_API_KEY` accordingly.

```yaml title=""datadog-logs-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: ghcr.io/section/fluentd-datadog:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag all_section_logs
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>

    <match all_section_logs>
      @type datadog
      @id awesome_agent
      api_key DATADOG_API_KEY

      <buffer>
              @type memory
              flush_thread_count 4
              flush_interval 3s
              chunk_limit_size 5m
              chunk_limit_records 500
      </buffer>
    </match>
```

Apply the above resources with `kubectl apply -f datadog-logs-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your logs.",604
HTTP Ingress,HTTP Ingress Overview,"HTTP Ingress
Enable Section HTTP Ingress
HTTP Ingress Overview
View or Modify HTTP Ingress
HTTP Ingress
Enable Section HTTP Ingress
For each Project deployed on Section, we deploy an [HTTP Ingress](/reference/http-extensions/http-ingress/), so the Project will accept inbound HTTP traffic from the Internet.",69
HTTP Ingress,View or Modify HTTP Ingress,"HTTP Ingress
Enable Section HTTP Ingress
HTTP Ingress Overview
View or Modify HTTP Ingress
HTTP Ingress
Enable Section HTTP Ingress
To view or modify the HTTP Ingress controller for your Section Project, view the **Kubernetes Service** we deploy for your Project with the specific name of `ingress-upstream`.

![Settings](/img/docs/ingress.png)

Use the [K8s Dashboard](/docs/guides/projects/manage-resources/) we provide or Kubectl to deploy or modify the Service Project if you wish to change items such as the Port your Application needs to communicate with the upstream Ingress.



````yaml title=""http-ingress.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
    namespace: default
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
        app: YOUR_APP_NAME
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
````",239
HTTP Egress,Egress Module Deployment,"HTTP Egress
Enable Section Egress Module
Egress Module Deployment
ConfigMap
Service
HTTP Egress
Enable Section Egress Module

This guide will walk you through deploying the Section Egress reverse proxy. The Egress reverse proxy is a simple reverse proxy that normalizes and routes requests to an external service. You can read more about the [Egress technical details](/reference/http-extensions/http-egress/). You will need the following Kubernetes resources to deploy the Egress reverse proxy:",103
HTTP Egress,ConfigMap,"HTTP Egress
Enable Section Egress Module
Egress Module Deployment
ConfigMap
Service
HTTP Egress
Enable Section Egress Module

Create the following ConfigMap defining an egress.json file:

```yaml title=""egress-configmap.yaml""
kind: ConfigMap
apiVersion: v1
metadata:
    name: egress-config
    namespace: default
data:
    egress.json: |
        {
            ""origin"": {
                ""address"":""my-external-service.example.com"",
                ""enable_sni"":true
            },
            ""alternate_origins"": {
                ""another_endpoint"": {
                    ""address"":""my-external-service2.example.com""
                }
            }
        }
```

This example will define a default origin routing traffic to **my-external-service.example.com**. The default origin will be used when no other origin is specified. The **section-origin** HTTP request header can be used to specify an alternate origin based on the matching key and value, e.g. *another_endpoint*.

#### Deployment

The deployment object is defined via the following YAML example:

```yaml title=""egress-deployment.yaml""
kind: Deployment
apiVersion: apps/v1
metadata:
  name: egress
  namespace: default
  labels:
    app: egress
spec:
  replicas: 2
  selector:
    matchLabels:
      app: egress
  template:
    metadata:
      labels:
        app: egress
    spec:
      volumes:
        - name: config-mount
          configMap:
            name: egress-config
      containers:
        - name: proxy
          image: 'gcr.io/section-io/k8s-egress:2.1.1'
          ports:
            - containerPort: 80
              protocol: TCP
          env:
            - name: SECTION_PROXY_NAME
              value: egress
            - name: REDIS_HOST
              value: 127.0.0.1
            - name: PROXY_REGO_KEY
              value: abc
            - name: LIST_KEY_PREFIX
              value: abc
            - name: LIST_KEY_SUFFIX
              value: abcingress
          resources:
            limits:
              cpu: '0.2'
              memory: 400Mi
            requests:
              cpu: '0.2'
              memory: 400Mi
          volumeMounts:
            - name: config-mount
              readOnly: true
              mountPath: /opt/proxy_config
          livenessProbe:
            httpGet:
              path: /.well-known/section-io/egress-status
              port: 80
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 10
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /.well-known/section-io/egress-status
              port: 80
              scheme: HTTP
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          imagePullPolicy: Always
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
```

*Note:* The environment variables are not explicitly used but are required for the container to function properly.",724
HTTP Egress,Service,"HTTP Egress
Enable Section Egress Module
Egress Module Deployment
ConfigMap
Service
HTTP Egress
Enable Section Egress Module

In order to route traffic to the egress container, we need to define a service. The service is defined via the following YAML example:

```yaml title=""egress-service.yaml""
apiVersion: v1
kind: Service
metadata:
  name: egress-service
spec:
  selector:
    app: egress
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 80
```

In order to route traffic to the egress service we will use the following Nginx example below.

```nginx
location / {
    proxy_set_header X-Forwarded-For $http_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $http_x_forwarded_proto;
    proxy_set_header Host $host;
    proxy_pass http://egress-service:8080;
}
```",205
Upgrade Plan,Upgrade Plan How to Update the Billing Plan for your Project,"Upgrade Plan
How to Update the Billing Plan for your Project
You may upgrade (or downgrade) your Project Plan at any time.

Simply select the settings for the particular Project you would like to Upgrade and choose ""Update Plan"".

![Upgrade Plan](/img/docs/upgrade-plan.png)

[Choose the plan](https://section.io/pricing/) which makes sense for your application's needs.  

You may be prompted to add a credit card if you do not have one associated with your account already.

![Delete](/img/docs/add-payment.png)

Add your payment details then confirm by clicking ""Add Payment Method""

Then confirm the Project Plan upgrade by clicking ""Update Project Plan"".

That's it - you are ready to make use of the new resource limits and locations.",155
Delete Project,Delete Project How to Delete a Project,"Delete Project
How to Delete a Project
You may Delete any of your active Projects at any time.

Simply select the settings for the particular Project you would like to delete and follow the prompts under the General Settings Tab.

![Settings](/img/docs/settings.png)

![Delete](/img/docs/delete.png)",62
Set Pod Size (CPU+RAM),Example Configuration,"Set Pod Size (CPU+RAM)
Learn to manage resources for Section Projects, set pod resource limits and replica counts
Example Configuration
Set Pod Size (CPU+RAM)
Learn to manage resources for Section Projects, set pod resource limits and replica counts
The following Deployment has one container defined with a request for 0.5 GiB RAM and 0.5 vCPU.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        ports:
        - containerPort: 80
```

Refer to Kubernetes Docs on [managing resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) to learn more.

### Additional Information

* There are no minimum sizes.
* Maximum container sizes are 4 vCPU and 12 GiB.
* For more information please refer to the [product pricing information](https://section.io/pricing/) to understand how your requests are related to your billing.
* Section may alter your YAML to ensure that request = limit, which gives a quality of service [""Guaranteed""](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/).
  * Both request and limit must be specified
  * If request and limit are not equal, Section will use the higher of the two values.
* You cannot request ephemeral storage directly. Section will automatically apply the ephemeral storage limits when the deployment is created.",404
SSL Certificates ,Adding Custom SSL Certificates,"SSL Certificates 
How to upload a custom SSL certificate in the Section Console.
Adding Custom SSL Certificates
1) Navigate to HTTPS page
2) Select your desired domain
3) Paste certificate and key
4) Upload
SSL Certificates 
How to upload a custom SSL certificate in the Section Console.",63
SSL Certificates ,1) Navigate to HTTPS page,"SSL Certificates 
How to upload a custom SSL certificate in the Section Console.
Adding Custom SSL Certificates
1) Navigate to HTTPS page
2) Select your desired domain
3) Paste certificate and key
4) Upload
SSL Certificates 
How to upload a custom SSL certificate in the Section Console.

Log into Section Console, navigate to the project, then **Settings** then **TLS certificates**.

![Settings](/img/docs/custom-cert.png)",93
SSL Certificates ,2) Select your desired domain,"SSL Certificates 
How to upload a custom SSL certificate in the Section Console.
Adding Custom SSL Certificates
1) Navigate to HTTPS page
2) Select your desired domain
3) Paste certificate and key
4) Upload
SSL Certificates 
How to upload a custom SSL certificate in the Section Console.

The dropdown will automatically fill with the first domain listed in your application. If that's not the domain you want, select the correct one from the dropdown. 

If you have multiple sub-domains in this application and wish to upload a wild card certificate or a certificate with multiple domains in the Subject Alternate Name(SAN), you will need to upload the certificate for each domain.

You will see a button entitled ""Specify a custom certificate"", or ""Change custom certificate"" depending on whether you have previously uploaded a custom certificate for the application. This will render two input boxes on the page — one for ""Public certificate & chain"" and the other for ""Private key.""",196
SSL Certificates ,3) Paste certificate and key,"SSL Certificates 
How to upload a custom SSL certificate in the Section Console.
Adding Custom SSL Certificates
1) Navigate to HTTPS page
2) Select your desired domain
3) Paste certificate and key
4) Upload
SSL Certificates 
How to upload a custom SSL certificate in the Section Console.

You will need to copy the site certificate and any intermediate certificates into the ""Public certificate & chain"" box.

The order of certificates needs to be domain certificate first, followed by any intermediate certificate(s) in order. Make sure NOT to include the root certificate. The certificates should be PEM encoded and will look like this in a text editor:

    
    -----BEGIN CERTIFICATE-----
    /* contents of domain certificate */
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate */
    -----END CERTIFICATE-----

If there are multiple intermediate certificates, you will need to make sure they are in the correct order.

    -----BEGIN CERTIFICATE-----
    /* contents of domain certificate */
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate 1*/
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate 2*/
    -----END CERTIFICATE-----

Now copy and paste your private key in to the ""Private key"" input area. the private key should look like:

    -----BEGIN RSA PRIVATE KEY-----
    /* contents of private key */
    -----END RSA PRIVATE KEY-----",296
SSL Certificates ,4) Upload,"SSL Certificates 
How to upload a custom SSL certificate in the Section Console.
Adding Custom SSL Certificates
1) Navigate to HTTPS page
2) Select your desired domain
3) Paste certificate and key
4) Upload
SSL Certificates 
How to upload a custom SSL certificate in the Section Console.

Once you have copy and pasted the certificates and private key, click the ""Save Changes"" button. The portal will perform a check to make sure the certificate is for the correct domain, and the private key is a match for the certificate. Once that is accepted, a deployment is made to the Section platform and you should see the new certificate on the site in moments. You will also see the uploaded certificate information at the right hand side of the HTTPS page.",156
Set Location Preferences,Examples,"Set Location Preferences
Learn to configure edge locations using the Kubernetes API
Examples
Set Location Preferences
Learn to configure edge locations using the Kubernetes API

The first and simplest scenario is when you want your app to be deployed according to current traffic patterns. You are satisfied with the default values of 2, 5, and 20 for the minimumLocations, maximumLocations, and minimumUsagePerLocation, respectively, so you don't need to enter any values for those. Also, you don't have any demands such as ""always have at least one location in Europe"", so the LocationOptimizer is free to choose from any existing location to host your app. That situation would be captured by this ConfigMap:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic""
        }
    }
metadata:
  name: location-optimizer
```

Apply the above ConfigMap to your project using either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or the following `kubectl` command:

```bash
kubectl apply -f location-optimizer.yaml
```

You can adjust the defaults as needed. For example, if you want more coverage across the globe, you could increase minimumLocations from 2 to 5, giving you exactly 5 locations at all times, dynamically positioned according to traffic:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 5
        }
    }
metadata:
  name: location-optimizer
```

A drawback with such a simple ConfigMap above is when you don't have reliable traffic being served by your app. This is common in the early stages of development. The LocationOptimizer is being told to select locations according to traffic but the traffic signal doesn't exist. We have methods to handle these ""fallback"" situations, but you can exert more control to obtain a more desirable, predictable outcome. The best way to handle this is to add add the **mustInclude** parameter to your ConfigMap. (See Tips section below for low/no traffic apps). 

For example, we could add 2 elements to the **mustInclude** array to ensure that we will always have a presence in these locations. If there is no traffic, then these will be your only locations. If there is traffic, you will have these locations and anything else that is warranted by the traffic. Here is what the ConfigMap would look like if we forced a presence in Europe and North America:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

The **mustInclude** parameter accepts an array of objects. Supported keys for the objects are listed in our [Location Parameters](/reference/solver-service-parameters/) reference guide.

It is worth taking a moment to restate what this ConfigMap represents. This is a dynamic config, so traffic patterns will be analysed to choose suitable locations. And there are **mustInclude** conditions, so these must be met under any conditions. It gives you a few known locations but others can be added as needed according to the traffic analysis. If you want your **mustInclude** locations to be more specific, you can consult our [Location Parameters](/reference/solver-service-parameters/) reference guide and use suitable ""locationcode"" references like this:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""locationcode"": ""sfo""
                },
                {
                    ""locationcode"": ""ams""
                },
                {
                    ""locationcode"": ""syd""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

This will cause the selection of Amsterdam and Sydney no matter what, and up to 3 more locations will be added where and when they are needed according to traffic patterns, because the default value for maximumLocations is 5.

Finally, Enterprise plans allow you to require locations according to special capabilities. For example, the following ConfigMap constrains your workload to running at only those locations that support [PCI](https://www.pcisecuritystandards.org/) (Payment Card Industry) compliance: 

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""chooseFrom"": ""pci""
        }
    }
metadata:
  name: location-optimizer
```

Another suitable ConfigMap for apps that do not serve HTTP traffic or for development scenarios (low or no traffic), is to use a static policy. A static policy is simply one in which you specify characteristics of locations that you want and this entirely defines the locations that are selected for you. No traffic signal is analysed. Your app will be deployed to suitable locations and stay there until the ConfigMap is changed. The parameters for static look very similar to the example given above. As noted [here](/explanations/aee/), the static policy does not honor any concept of minimum or maximum locations. The LocationOptimizer will try to meet your **mustInclude** array as exactly as possible. So a static deployment with locations in Europe and North America would be configured like this:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""static"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

The only explicit change is the name of the policy. Implicitly, you are saying: 'pay no attention to traffic and stay in only these locations until I tell you otherwise by updating my ConfigMap.' With static, the locations are stable and ""static"" over time, to the extent that is possible. So the first two locations selected will be used consistently unless some issue forces us to move your app. If we move it, we will again choose locations that meet the **mustInclude** conditions. 

In the dynamic version above where your app will always be in one European and one North American location, but there may be additional locations as well, if traffic is sufficient to warrant additional locations. Also, the locations within Europe and North America will change over time in response to traffic patterns.  

We offer multiple levels of geographic abstraction when specifying **mustInclude** conditions. You can exert more fine-grained control with something like ""locationcode"" and coarser control with ""region"". Beware that if you specify a fine-scale entity, like a city, then the LocationOptimizer has fewer options in meeting this condition. If, for some reason, there are no available facilities in that city, then your app cannot be scheduled there. Fall-back locations in such a scenario may seem arbitrary to the stated COnfigMap, depending on conditions. So the more general you can be, the better in terms of reliably obtaining your desired results.

# Tips
When the LocationOptimizer cannot obtain a solution based on your ConfigMap and current network and traffic conditions, it will check to see where your app is currently deployed. If it has current locations (i.e., is currently deployed), the LocationOptimixer will select those current locations, leaving your app where it is. If your app does not have any current locations, then two fall-back locations are returned, typically in NYC and SFO.

To avoid these situations and take more control over your outcomes:

1. Use a static policy when you initially deploy your app and when there is no meaningful traffic. This forces your app to deploy to locations of your choosing regardless of traffic. 

2. Once your app has been deployed and you want to do some load-testing or you are going to start serving traffic, then switching to a dynamic policy could make sense.

3. When using a dynamic policy with relatively little traffic, the selected locations can change often over a given time period. At low traffic levels, the signal is highly variable and can cause location flapping. If you are doing relatively little traffic, consider using a static policy to get in the places you want and stay there. The dynamic policy works best with strong, geographically-correlated traffic signals.",1904
DNS,Add Domains,"DNS
Manage DNS and Domains on Section
Add Domains
Change DNS
DNS
Manage DNS and Domains on Section

Once you have created a Section Project, you can define the domain/s you would like to point at that Project in addition to or instead of the domain created automatically by our platform for that project. 

Navigate to the settings tab for the Project you would like to add the Domain.

Click Add Domain and enter the details of the Domain you would like to add.

![Settings](/img/docs/add-domain.png)",109
DNS,Change DNS,"DNS
Manage DNS and Domains on Section
Add Domains
Change DNS
DNS
Manage DNS and Domains on Section

After the Domain has been added, you can change your public DNS records to direct traffic for that Domain to be sent to your application on this Section Project.

1. Select the domain name you wish to use.
2. Go to your DNS provider and modify the existing CNAME and A records. 
3. Add the CNAME as per instructions in the Section Console.
4. Click Verify to confirm engagment.

![Settings](/img/docs/changedns.png)

:::note
Propagation of DNS records throughout the Internet and ISP systems can take some time.  Follow our [guide to tracking DNS Propagation](/docs/get-started/dns-propagate/) to understand when your DNS changes are live through the Internet.
:::",175
Using K8s Dashboard,Launch the Kubernetes Dashboard,"Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
Launch the Kubernetes Dashboard
Add a K8s ""Resource""
Delete Deployment
Delete Service
Edit Deployment
Change RAM and CPU Allocation
Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page. 

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.",127
Using K8s Dashboard,"Add a K8s ""Resource""","Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
Launch the Kubernetes Dashboard
Add a K8s ""Resource""
Delete Deployment
Delete Service
Edit Deployment
Change RAM and CPU Allocation
Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
Our tutorial pages have YAML files for Deployments, ConfigMaps, Services, and others. All of these are Kubernetes **resources** that can be applied to your Project using the Kubernetes Dashboard. 

From any of our tutorials, copy the YAML, then go to the dashboard and select the **+** from the upper right hand corner, choose ""Create from input"", and then paste your YAML into the text field. Select **Upload**. 

![location-optimizer](/img/docs/getting-started-location-optimizer.png)

Kubernetes resources such as [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/) and [Services](https://kubernetes.io/docs/concepts/services-networking/service/) can all be applied in this way.",235
Using K8s Dashboard,Delete Deployment,"Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
Launch the Kubernetes Dashboard
Add a K8s ""Resource""
Delete Deployment
Delete Service
Edit Deployment
Change RAM and CPU Allocation
Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
To delete a resource from the Kubernetes Dashboard, in the list of deployments select the Actions menu (three vertical dots at the far right) and select Delete.

![Delete deployment](/img/docs/delete-deployment.png)",107
Using K8s Dashboard,Delete Service,"Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
Launch the Kubernetes Dashboard
Add a K8s ""Resource""
Delete Deployment
Delete Service
Edit Deployment
Change RAM and CPU Allocation
Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
To delete a Service from the Kubernetes Dashboard (such as `ingress-upstream`, as required to begin many of our tutorials), in the list of Services select the Actions menu (three vertical dots at the far right) and select Delete.

![Delete Service](/img/docs/delete-service.png)",123
Using K8s Dashboard,Edit Deployment,"Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
Launch the Kubernetes Dashboard
Add a K8s ""Resource""
Delete Deployment
Delete Service
Edit Deployment
Change RAM and CPU Allocation
Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
On the Kubernetes Dashboard you should see your deployment. In this example and if you chose to deploy our example container the deployment is named `section-project-deployment`. If you wanted to edit the resource consumption you will need to edit the deployment. 

![edit-deployment](/img/docs/getting-started-edit-deployment.png)

Edit the deployment YAML Section generated when you deployed your first project. For more information on configuration of resources see our guide on [Setting Resource Limits](/guides/projects/set-resource-limits).",168
Using K8s Dashboard,Change RAM and CPU Allocation,"Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
Launch the Kubernetes Dashboard
Add a K8s ""Resource""
Delete Deployment
Delete Service
Edit Deployment
Change RAM and CPU Allocation
Using K8s Dashboard
Use the Kubernetes Dashboard to manage the Deployment Configuration your application
To make changes to your resource allocation you will need to update the resources array.

![set-resources](/img/docs/getting-started-set-resources.png)

Select **Update** and your changes will be applied to your deployment. You can then use the Kubernetes dashboard to monitor the changes happening to your workload in real time. 

<!--- Next we will walk through how to [Configure your Project's Analytics Tooling](/get-started/project-analytics/) so you can make full use of Section's dynamic and distributed platform.--->",169
Load Testing Projects,Things to Keep in Mind,"Load Testing Projects
Some tips on running load tests against your applcation on Section.
Things to Keep in Mind
Load Tests vs Production Traffic
Apache Bench (ab)
Load Testing Projects
Some tips on running load tests against your applcation on Section.
The traffic signal, described [here](/explanations/traffic-signal/), is smoothed. This has 2 impacts on load testing. First, it dilutes small, brief spikes. This is desirable in production, but it means small, brief blips of traffic will not result in very high traffic rates due to the smoothing. Second, if you switch load source locations abruptly, the traffic signal will smooth that out over the span window. You will have to wait while the signal from the first source location decays and that from the second source location grows. An effective load test would aim to deliver a strong signal (10+ rps) over a reasonable duration (5+ minutes) so that your signal is clear. You would then wait approximately 15 minutes before sending traffic from a new source location. The traffic rate you simulate needs to also account for the **minimumUsagePerLocation**.

The **minimumUsagePerLocation** parameter exists to dampen the reactions of the AEE:LO to minor variations in traffic rate and location. Generally, we do not want to spin up in a new location due to a trivial increase in the traffic rate from some distance source. For production traffic, the default of 20 rps has proven to be very valuable and effective. For load testing, you may want to reduce this to 0 to make the system maximally sensitive to the traffic you generate. Again, this is likely too sensitive for production use as it will result in thrashing, but could serve well in load testing.

The **minimumLocations** is another important parameter for production traffic. However, to most clearly see the impact of a load test, you might consider setting this parameter to 1. This way, the selected locations will be more directly attributable to your test traffic.",414
Load Testing Projects,Load Tests vs Production Traffic,"Load Testing Projects
Some tips on running load tests against your applcation on Section.
Things to Keep in Mind
Load Tests vs Production Traffic
Apache Bench (ab)
Load Testing Projects
Some tips on running load tests against your applcation on Section.
Load testing typically involves sending traffic to your Project from one location. Production traffic typically originates from thousands of different geographic locations simultaneously. It is difficult to evaluate the complex, dynamic response of the LocationOptimizer. As with all good testing, we will show you how to partition out and simplify your ConfigMap in order to evaluate it under even a simple, single-source testing scenario.

First, consider this ConfigMap:
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```
This specifies 2 locations, in Europe and North America, and the implicit default value for the **minimumUsagePerLocation** parameter is 20 rps. Now, suppose we want to test this ConfigMap using a tool like ApacheBench and a VPN. Let's say we want to see a location spin up in Asia, we simulate traffic from Singapore. 

Under these conditions, we will only see a new Asia location in the solution if the load results in 60 rps or greater in the traffic metric over several minutes. This is because we require the European and North American locations. For the given value of **minimumUsagePerLocation**, we will only get a third location if all existing locations can serve at least 20 rps, so it is only when we hit 60 rps that a third location is warranted. At that point, the location of the traffic finally starts to impact my solution and we should get a location close to the load test origin.

So we need to make sure we send a lot of traffic, or we could tailor the ConfigMap so that the solution is more sensitive, as illustrated in the examples below.

# Examples
The following ConfigMap will give a very quick response to simulated traffic from anywhere outside of Europe:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
      ""strategy"": ""SolverServiceV1"",
      ""params"": {
        ""policy"": ""dynamic"",
        ""mustInclude"": [
          {
            ""region"": ""europe""
          }
        ],
        ""minimumLocations"": 1,
        ""minimumUsagePerLocation"": 0
      }
    }
metadata:
  name: location-optimizer
```

The ConfigMap above will require at least one location at all times and that one location must be in Europe, even in the absence of traffic. This is a nice way to ensure that you know where your Project will be running while you prepare to simulate traffic. We can now examine two aspects of the configured behavior with a few different load tests. 

First, we can simulate traffic from various locations inside Europe and see the one location moving to be close to the traffic source. If you are switching the source location of your load quickly from one place to another, you may have multiple traffic signals due to the inherent smoothing. This would result in multiple locations being selected, with a new one added with each new traffic signal location. Because we reduced the **minimumUsagePerLocation** to 0, we can expect to see new locations spin up near any source of traffic.

Second, we can simulate traffic from outside Europe and we will see a nearby location spin up, while we also maintain a single location in Europe.",790
Load Testing Projects,Apache Bench (ab),"Load Testing Projects
Some tips on running load tests against your applcation on Section.
Things to Keep in Mind
Load Tests vs Production Traffic
Apache Bench (ab)
Load Testing Projects
Some tips on running load tests against your applcation on Section.
[Apache Bench](https://httpd.apache.org/docs/2.4/programs/ab.html) is a tool for benchmarking HTTP servers. You may find it useful in generating traffic against your new Section Project so that you can observe the behavior of the [AEE](/explanations/aee/).  When you use a tool such as this, make sure that you are generating enough traffic relative to your **minimumUsagePerLocation** parameter, as discussed earlier.  Use the ```-c concurrency``` argument to generate multiple requests at a time, using 10 or more as the concurrency value.  For example:

```
$ ab -n 10000 -c 10 https://myenv.myapp.com
```",201
Project Details,Rename Project,"Project Details
View and Change general settings for your Project
Rename Project
Section Project ID
Kubernetes API Endpoint
Project Details
View and Change general settings for your Project

Every Project will have a unique name and a unique URL which we will create and assign to your project when initally launched.

To rename your Project, select the Settings for that project and then the icon to change the project name.  Note, this will not change the URL assigned to your project.

![Settings](/img/docs/rename.png)",106
Project Details,Section Project ID,"Project Details
View and Change general settings for your Project
Rename Project
Section Project ID
Kubernetes API Endpoint
Project Details
View and Change general settings for your Project

This is your unique Project ID which is not to be confused with your Section Account ID.  You can also see both the Account ID and the Project ID in your browser's URL indicator

`https://console.section.io/overview/account/SECTION_ACCOUNT_ID/project/PROJECT_ID`.",92
Project Details,Kubernetes API Endpoint,"Project Details
View and Change general settings for your Project
Rename Project
Section Project ID
Kubernetes API Endpoint
Project Details
View and Change general settings for your Project

You will also see on the General Settings Tab, a Kubernetes API Endpoint.  This is unique Kubnernetes API to which you can point your existing Kubernetes tooling such as other Kubernetes UIs or your CI/CD tooling.",83
2 Factor Authentication,Enable two-factor authentication,"2 Factor Authentication
Learn how to enable Two Factor Authentication for your Users.
Enable two-factor authentication
Disable two-factor authentication
2 Factor Authentication
Learn how to enable Two Factor Authentication for your Users.

Follow the steps below to enable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your My Profile setting, toggle the 2FA switch from **Disabled** to **Enabled**.
4. Select how you would like to enable 2FA, either through SMS or an authentication app, and then click **Save**.

![Settings](/img/docs/2fa.png)

:::note
Submit a [support request](https://support.section.io/hc/en-us/requests/new) if you would like 2FA to be required for all users invited to your account.
:::",177
2 Factor Authentication,Disable two-factor authentication,"2 Factor Authentication
Learn how to enable Two Factor Authentication for your Users.
Enable two-factor authentication
Disable two-factor authentication
2 Factor Authentication
Learn how to enable Two Factor Authentication for your Users.

Follow the steps below to disable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your user setting, toggle the 2FA switch from **Enabled** to **Disabled** and then click **Save**.

:::note
Submit a [support request](https://support.section.io/hc/en-us/requests/new) from your user email address if you are no longer able to provide the second form of authentication and need 2FA to be disabled by Section. Please keep in mind our 2FA reset process requires extensive verification and can take up to 7 days to be completed.
:::",179
Account,Account Details,"Account
Learn how to Manage your Section Account.
Account Details
User Access Management
Delete Account
Account
Learn how to Manage your Section Account.

You can view and modify your details for the account under the My Account tab

![Settings](/img/docs/account.png)",55
Account,User Access Management,"Account
Learn how to Manage your Section Account.
Account Details
User Access Management
Delete Account
Account
Learn how to Manage your Section Account.

Please see our article on [User Management](/guides/iam/users/) to learn more about how to add users to your account and how to set the appropriate permissions.",65
Account,Delete Account,"Account
Learn how to Manage your Section Account.
Account Details
User Access Management
Delete Account
Account
Learn how to Manage your Section Account.

If you would like to have your account deleted, please submit a [support request](https://support.section.io/hc/en-us/requests/new).",60
Projects,Add a Section Project,"Projects
Learn how to manage your Section Projects.
Add a Section Project
For Other Project Actions
Projects
Learn how to manage your Section Projects.

To create a new Section Project follow our getting started [Creating Project](/get-started/create-project/) guide.",53
Projects,For Other Project Actions,"Projects
Learn how to manage your Section Projects.
Add a Section Project
For Other Project Actions
Projects
Learn how to manage your Section Projects.

View our [Project Guides](/guides/projects/) for more Project actions.",46
API Tokens,Create API token,"API Tokens
Learn how to manage API tokens in the Section console.
Create API token
Delete API token
API Tokens
Learn how to manage API tokens in the Section console.

Follow the steps below to create an API token:

1. Log in to the Section console.
2. In the left sidebar, click **API Tokens**.
3. In the **Token Description** field, enter a description for what this API token will be used for and 
4. Copy and Securely store the created API token.
5. then click **Add**.

![Settings](/img/docs/api-token.png)",122
API Tokens,Delete API token,"API Tokens
Learn how to manage API tokens in the Section console.
Create API token
Delete API token
API Tokens
Learn how to manage API tokens in the Section console.

Follow the steps below to delete an API token:

1. Log in to the Section console.
2. In the left sidebar, click **API Tokens**.
3. In the API tokens list, find the API token you would like to delete and then click the **X** button in the **Delete** column.",100
Manage Users,Add Users to Your Account,"Manage Users
Learn how to Manage users for your Account.
Add Users to Your Account
Manage Users
Learn how to Manage users for your Account.

Follow the steps below to enable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your Manage Users setting, add the details of the User you would like to invite to your Account.
4. Then click **Invite User**.

![Settings](/img/docs/add-user.png)",104
Single Sign On (SSO),Single Sign On (SSO) Enable SSO for your Section account.,"Single Sign On (SSO)
Enable SSO for your Section account.
SSO and RBAC are available to Enterprise Customers.

To enable SSO for your Enterprise Section Account please submit a [support request](https://support.section.io/hc/en-us/requests/new).",55
Tunnel Client on Section,Section Client,"Tunnel Client on Section
...
Section Client
Script
Tunnel Client on Section
...
This example demonstrate the essence of:
* Having a MySQL client in k8s that connects to a k8s ssh-tunnel client and communicate locally as if it was the MySQL server.

It relies on:
* Existing tunnel server & MySQL server.
* `kubectl` configured to connect to your Section Project.",81
Tunnel Client on Section,Script,"Tunnel Client on Section
...
Section Client
Script
Tunnel Client on Section
...
```bash
 # Produces TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY
 generate_tunnel_keys() {
    while read -r line; do
        [ ""${line}"" == ""SERVER (PUBLIC) KEY:"" ] && read -r TUNNEL_SERVER_KEY
        [ ""${line}"" == ""CLIENT (PRIVATE) KEY:"" ] && read -r TUNNEL_CLIENT_KEY
    done < <(docker run --rm ghcr.io/section/section-secure-tunnel:sha-05d9f6a keygen)
    export TUNNEL_CLIENT_KEY
    export TUNNEL_SERVER_KEY
    echo ""Exported TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY""
}

 # Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS
 setup_k8s_client(){
    kubectl delete secret ssh-tunnel || true
    kubectl create secret generic ssh-tunnel \
        --from-literal=TUNNEL_ADDRESS=""${TUNNEL_ADDRESS}"" \
        --from-literal=TUNNEL_CLIENT_KEY=""${TUNNEL_CLIENT_KEY}"" \
        --from-literal=REMOTE_SERVICE_ADDRESS=""${REMOTE_SERVICE_ADDRESS}""

    kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ssh-tunnel
  labels:
    app: ssh-tunnel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ssh-tunnel
  template:
    metadata:
      labels:
        app: ssh-tunnel
    spec:
      containers:
        - name: ssh-tunnel
          image: 'ghcr.io/section/section-secure-tunnel:sha-05d9f6a'
          env:
          - name: REMOTE_SERVICE_ADDRESS
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: REMOTE_SERVICE_ADDRESS
                optional: false
          - name: TUNNEL_ADDRESS
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: TUNNEL_ADDRESS
                optional: false
          - name: TUNNEL_CLIENT_KEY
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: TUNNEL_CLIENT_KEY
                optional: false
          args: [
            ""client"",
            ""$(TUNNEL_ADDRESS)"",
            ""key"",
            ""remote_user_name"",
            ""$(TUNNEL_CLIENT_KEY)"",
            ""3306:$(REMOTE_SERVICE_ADDRESS):3306""
            ]
          resources:
            requests:
              cpu: 0.5
              memory: 512Mi
            limits:
              cpu: 0.5
              memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ssh-tunnel
  name: ssh-tunnel
spec:
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    app: ssh-tunnel
EOF
}

 setup_workload(){
    kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-client
  labels:
    app: mysql-client
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql-client
  template:
    metadata:
      labels:
        app: mysql-client
    spec:
      containers:
        - name: mysql-client
          image: 'mysql:8.0'
          imagePullPolicy: Always
          env:
          - name: MYSQL_PWD
            value: masteruserpassword
          command: [""/bin/sh""]
          args: [
            ""-c"",
            ""/usr/bin/mysql --silent --host=ssh-tunnel --user=masterusername --execute='SELECT VERSION()' && tail -f /dev/null""
            ]
          resources:
            requests:
              cpu: 0.5
              memory: 512Mi
            limits:
              cpu: 0.5
              memory: 512Mi
EOF
}

setup_k8s_client
setup_workload
```

Resulting workload pods:
```
$ kubectl logs mysql-client-db67b59ff-ktzvz
8.0.28
```",919
Tunnel Server using Docker and AWS,Script,"Tunnel Server using Docker and AWS
...
Script
!/bin/bash
Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS
Tunnel Server using Docker and AWS
...
```bash",44
Tunnel Server using Docker and AWS,!/bin/bash,"Tunnel Server using Docker and AWS
...
Script
!/bin/bash
Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS
Tunnel Server using Docker and AWS
...

 # Produces REMOTE_SERVICE_ADDRESS
 create_database(){
    local db_instance_name=""demo-privatedb-$(date +%s)""

    aws rds create-db-instance \
        --db-instance-identifier ""${db_instance_name}"" \
        --db-instance-class db.t4g.micro \
        --engine mysql \
        --master-username masterusername \
        --master-user-password masteruserpassword \
        --allocated-storage 20 \
        --no-publicly-accessible \
        >/dev/null

    aws rds wait db-instance-available \
        --db-instance-identifier ""${db_instance_name}"" 

    REMOTE_SERVICE_ADDRESS=$(aws rds describe-db-instances \
         --db-instance-identifier ""${db_instance_name}"" \
         --query ""DBInstances[0].Endpoint.Address"" \
         --output text)
    export REMOTE_SERVICE_ADDRESS
    echo ""Exported REMOTE_SERVICE_ADDRESS=${REMOTE_SERVICE_ADDRESS}""
}

 # Produces TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY
 generate_tunnel_keys() {
    while read -r line; do
        [ ""${line}"" == ""SERVER (PUBLIC) KEY:"" ] && read -r TUNNEL_SERVER_KEY
        [ ""${line}"" == ""CLIENT (PRIVATE) KEY:"" ] && read -r TUNNEL_CLIENT_KEY
    done < <(docker run --rm ghcr.io/section/section-secure-tunnel:sha-05d9f6a keygen)
    export TUNNEL_CLIENT_KEY
    export TUNNEL_SERVER_KEY
    echo ""Exported TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY""
}

 # Consumes TUNNEL_SERVER_KEY
 # Produces TUNNEL_ADDRESS
 setup_remote_server(){
    local cluster_name=default
    local tunnel_server_service_name=""demo-ssh-tunnel-$(date +%s)""

    # ensure cluster
    aws ecs create-cluster \
        --cluster-name ""${cluster_name}"" \
        >/dev/null

    # crete task definition
    local task_definition_arn=$(aws ecs register-task-definition \
        --requires-compatibilities FARGATE \
        --family ""${tunnel_server_service_name}"" \
        --network-mode awsvpc \
        --cpu=256 \
        --memory=512 \
        --container-definitions ""[{\""name\"":\""ssh-tunnel\"",\""image\"":\""ghcr.io/section/section-secure-tunnel:sha-05d9f6a\"",\""cpu\"":256,\""command\"":[\""server\"",\""key\"",\""remote_user_name\"",\""${TUNNEL_SERVER_KEY}\"",\""any\""],\""memory\"":512,\""essential\"":true}]"" \
        --query ""taskDefinition.taskDefinitionArn"" \
        --output text \
        )

    # create security group
    local group_id=$(aws ec2 create-security-group \
        --group-name ""${tunnel_server_service_name}"" \
        --description ""ssh-tunnel inbound ${tunnel_server_service_name}"" \
        --query ""GroupId"" \
        --output text)

    # allow all to port 2022
    aws ec2 authorize-security-group-ingress \
        --group-id ""${group_id}"" \
        --protocol tcp \
        --port 2022 \
        --cidr ""0.0.0.0/0"" \
        >/dev/null

    # allow to talk to default SG

    # get all subnets
    local allsubnet=$(aws ec2 describe-subnets \
        --query ""join(',',Subnets[].SubnetId)"" \
        --output text \
        )

    # default_group_id
    local default_group_id=$(aws ec2 describe-security-groups \
        --group-names default \
        --query ""SecurityGroups[0].GroupId"" \
        --output text \
        )

    # create service
    aws ecs create-service \
        --no-cli-pager \
        --service-name ""${tunnel_server_service_name}"" \
        --task-definition ""${task_definition_arn}"" \
        --desired-count 1 \
        --launch-type ""FARGATE"" \
        --network-configuration ""awsvpcConfiguration={subnets=[${allsubnet}],assignPublicIp=ENABLED,securityGroups=[${default_group_id},${group_id}]}"" \
        > /dev/null

    # wait for service to start
    aws ecs wait services-stable --services ""${tunnel_server_service_name}""

    # get the task
    local taskArn=$(aws ecs list-tasks \
        --service-name ""${tunnel_server_service_name}"" \
        --query 'taskArns[0]' \
        --output text) 
 
    # get the network interface 
    local networkInterfaceId=$(aws ecs describe-tasks \
        --tasks ""${taskArn}"" \
        --query ""tasks[0].attachments[0].details[?name=='networkInterfaceId'].value"" \
        --output text)

    # get the public IP
    TUNNEL_ADDRESS=$(aws ec2 describe-network-interfaces \
        --network-interface-ids ${networkInterfaceId} \
        --query ""NetworkInterfaces[0].Association.PublicIp"" \
        --output text)

    export TUNNEL_ADDRESS
    echo ""Exported TUNNEL_ADDRESS: ${TUNNEL_ADDRESS}""
}",1114
Tunnel Server using Docker and AWS,"Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS","Tunnel Server using Docker and AWS
...
Script
!/bin/bash
Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS
Tunnel Server using Docker and AWS
...
 test_local_client(){
    # start local ssh-tunnel client
    docker run \
        --name ssh-tunnel-client \
        -d --restart unless-stopped \
        -e TUNNEL_CLIENT_KEY \
        ghcr.io/section/section-secure-tunnel:sha-05d9f6a \
            client \
            ""${TUNNEL_ADDRESS}"" \
            key \
            remote_user_name \
            '${TUNNEL_CLIENT_KEY}' \
            ""3306:${REMOTE_SERVICE_ADDRESS}:3306"" \
        > /dev/null

    # get IP address of client
    local client_ip=$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' ssh-tunnel-client)

    # give  tunnel chance to connect
    sleep 1
    
    # Connect and use!
    echo -n ""Talking to remote db via tunnel to get server version: ""
    MYSQL_PWD=masteruserpassword docker run -it --rm -e MYSQL_PWD --entrypoint ""/bin/sh"" mysql:8.0 ""-c"" ""/usr/bin/mysql --silent --host=${client_ip} --user=masterusername --execute='SELECT VERSION()' && read""

    # clean up
    docker rm -f ssh-tunnel-client > /dev/null
}

create_database
generate_tunnel_keys
setup_remote_server
test_local_client
```

Running the above script should result in:
```
Running with Tunnel Public IP: n.n.n.n
Talking to remote db via tunnel to get server version: 8.0.28
```",363
Kubernetes Dashboard,Kubernetes Dashboard Using Kubernetes Dashboard on Section,"Kubernetes Dashboard
Using Kubernetes Dashboard on Section
We provide you with a Kubernetes Dashboard for every project you launch on Section.

![K8s Dashboard](/img/docs/ui-dashboard.png)

While the underlying number and location of pods running your application may be adjusted by our AEE from time to time based on your [location preferences](/docs/guides/projects/set-edge-locations/) the Kubernetes Dashboard we provide will always represent the running pods as a single pane of glass; **regardless of how many pods are running and where they may be running at any particular moment.**

You can use this Dashboard as described in the [Kubernetes Documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/) 

See also our [Guide for use of the Kubernetes Dashboard](/docs/guides/projects/manage-resources/) to manage some Section specific items.


To launch the Kubernetes Dashboard simply click on the project and choose Launch Dashboard.

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)",204
Setup Ingress for a Project,Setup Ingress to the Section Edge for HTTP workload,"Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project
Setup Ingress to the Section Edge for HTTP workload
Create a Service object
See your service running on Section
Setup DNS
Congratulations!
Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project

To setup ingress you will:

1. Create the necessary Kubernetes Service object to expose the web server to the Internet.
1. Use your DNS provider to direct traffic to your web server with a CNAME record.",108
Setup Ingress for a Project,Create a Service object,"Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project
Setup Ingress to the Section Edge for HTTP workload
Create a Service object
See your service running on Section
Setup DNS
Congratulations!
Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project

Create a yaml file, such as `my-service.yaml`

```yaml title=""my-service.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
        -   name: 80-8080
            port: 80
            protocol: TCP
            targetPort: 80
    selector:
        app: nginx
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: { }
```

## Use kubectl to create your ingress service

### Deploy your service

```bash
kubectl apply -f my-service.yaml
```

The Service called `ingress-upstream` causes the deployment of special ingress pods into your environment that route
traffic into your app. Read more about the Section [HTTP Ingress](/guides/http-extensions/http-ingress/).",256
Setup Ingress for a Project,See your service running on Section,"Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project
Setup Ingress to the Section Edge for HTTP workload
Create a Service object
See your service running on Section
Setup DNS
Congratulations!
Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project

  ```bash
  kubectl get services
  ```

You will see a new `ingress-upstream` service, as in the following example.

```
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes         ClusterIP   10.xx.xx.xx     <none>        443/TCP   6m55s
ingress-upstream   ClusterIP   10.xx.xx.xx     <none>        80/TCP    1s
```

### See the pods running on Section's network

  ```bash
  kubectl get pods -o wide
  ```

Note that you didn't set a [location optization strategy](/guides/projects/set-edge-locations/), so Section will run
your application with the default (2 locations).",239
Setup Ingress for a Project,Setup DNS,"Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project
Setup Ingress to the Section Edge for HTTP workload
Create a Service object
See your service running on Section
Setup DNS
Congratulations!
Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project

* On the overview page of the [Section Console](https://console.section.io), find and click your environment.
* Under Settings select Domains.
* Confirm that the domain name you provided when you created the environment is listed.
* Visit your DNS provider and create a CNAME record to point to Section as instructed on the Section Console Domains
  page.
* After some time you will be able to click the Verify button to confirm that DNS is engaged.

Read more about [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",188
Setup Ingress for a Project,Congratulations!,"Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project
Setup Ingress to the Section Edge for HTTP workload
Create a Service object
See your service running on Section
Setup DNS
Congratulations!
Setup Ingress for a Project
Use the Kubernetes API to setup ingress for a Section Project

When you visit your hostname in a web browser, you'll see ""Welcome to nginx!"". This tells you that you application is
successfully deployed!

<!--- Your Project Settings page gives access to [Traffic Monitor](/explanations/traffic-monitor/), which is a great way to see the traffic coming into your web server.--->",133
Kubernetes API Basics,Creating a Project,"Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section
Creating a Project
Kubernetes API URL
Use kubectl to Deploy a Container
Get Logs
Describe
Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section

To get started using the Kuberenetes API you will first need to create an Section Project. For more information on the steps to create a Section Project and get your Kubernetes API follow our [Getting Started Guides](/get-started/create-project/).",106
Kubernetes API Basics,Kubernetes API URL,"Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section
Creating a Project
Kubernetes API URL
Use kubectl to Deploy a Container
Get Logs
Describe
Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section

Every project you create in Section will automatically generate a Kubernetes API endpoint. This is the URL you will use to communicate with Section when using kubectl. This endpoint is displayed in the Section console in Project Settings > Kubernetes.

![Kubernetes API URL](/img/docs/kube_api_url.gif)",115
Kubernetes API Basics,Use kubectl to Deploy a Container,"Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section
Creating a Project
Kubernetes API URL
Use kubectl to Deploy a Container
Get Logs
Describe
Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section

Once you have created a Project and retrieved your Kubernetes API endpoint you can now deploy containers to the Section platform using the Kubernetes API. To do this please follow the [Get Started with Kubernetes](/docs/guides/kubernetes-ui/kubernetes-api/get-started-k8/) guide.",111
Kubernetes API Basics,Get Logs,"Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section
Creating a Project
Kubernetes API URL
Use kubectl to Deploy a Container
Get Logs
Describe
Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section

To print the logs for a container running on Section you can use the kubectl logs command:

```kubectl logs [pod name]```

For more information on kubectl commands please see the [Kubernetes Documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs)",116
Kubernetes API Basics,Describe,"Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section
Creating a Project
Kubernetes API URL
Use kubectl to Deploy a Container
Get Logs
Describe
Kubernetes API Basics
Learn the Basics of Working with the Kubernetes API and Section

To show details of a specific resource or group of resources running on Section you can use the **kubectl describe** command:

 ```kubectl describe [pod name]```


For more information on kubectl commands please see the [Kubernetes Documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe)",121
Get Started with Kubernetes API,Use kubectl config to create a context,"Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Use kubectl config to create a context
Define Section as a cluster using your `Kubernetes API URL`:
Save your API token
Switch to the new context
Deploy a web server to the Section Edge for HTTP workload
Create a Deployment object
See your deployment running on Section
Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Let's configure `kubectl` to communicate with your environment.

* If you haven't already, obtain your [API token](https://console.section.io/configure/user/tokens).
* Use the Section Console to navigate to the Projects page where you will see your `Kubernetes API` URL on your Prokect.",158
Get Started with Kubernetes API,Define Section as a cluster using your `Kubernetes API URL`:,"Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Use kubectl config to create a context
Define Section as a cluster using your `Kubernetes API URL`:
Save your API token
Switch to the new context
Deploy a web server to the Section Edge for HTTP workload
Create a Deployment object
See your deployment running on Section
Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 

* **Ubuntu**
    ```bash
    KUBERNETES_API=""https://0123456789.kube.api.section.io/"" # Retrieve from https://console.section.io environment page
    kubectl config set-cluster section \
      --certificate-authority=/etc/ssl/certs/ca-certificates.crt \
      --server=$KUBERNETES_API_URL/
    ```

* **MacOS**
   ```bash
    KUBERNETES_API=""https://0123456789.kube.api.section.io/"" # Retrieve from https://console.section.io environment page
    kubectl config set-cluster section \
      --certificate-authority=/usr/local/etc/ca-certificates/cert.pem \
      --server=$KUBERNETES_API_URL/
   ```

If you don't have the file `/etc/ssl/certs/ca-certificates.crt` because you're on non-WSL-Windows, you can obtain an equivalent file here: [CA certificates](https://curl.se/docs/caextract.html)",301
Get Started with Kubernetes API,Save your API token,"Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Use kubectl config to create a context
Define Section as a cluster using your `Kubernetes API URL`:
Save your API token
Switch to the new context
Deploy a web server to the Section Edge for HTTP workload
Create a Deployment object
See your deployment running on Section
Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
  ```bash
  kubectl config set-credentials section-user \
     --token=$SECTION_API_TOKEN
  ```

### Create the execution context
  ```bash
  kubectl config set-context my-section-application \
     --cluster=section \
     --user=section-user \
     --namespace=default
  ```",159
Get Started with Kubernetes API,Switch to the new context,"Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Use kubectl config to create a context
Define Section as a cluster using your `Kubernetes API URL`:
Save your API token
Switch to the new context
Deploy a web server to the Section Edge for HTTP workload
Create a Deployment object
See your deployment running on Section
Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
  ```bash
  kubectl config use-context my-section-application
  ```

### Validate your setup
  ```bash
    kubectl version
  ```

More information about cluster, credential, and context information can be found in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#define-clusters-users-and-contexts)",174
Get Started with Kubernetes API,Deploy a web server to the Section Edge for HTTP workload,"Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Use kubectl config to create a context
Define Section as a cluster using your `Kubernetes API URL`:
Save your API token
Switch to the new context
Deploy a web server to the Section Edge for HTTP workload
Create a Deployment object
See your deployment running on Section
Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Next we'll place an nginx webserver at the edge using a Kubernetes Deployment object. The nginx webserver container used in this example comes from the official nginx images on the DockerHub registry.",132
Get Started with Kubernetes API,Create a Deployment object,"Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Use kubectl config to create a context
Define Section as a cluster using your `Kubernetes API URL`:
Save your API token
Switch to the new context
Deploy a web server to the Section Edge for HTTP workload
Create a Deployment object
See your deployment running on Section
Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 

Create a yaml file, such as `my-first-edge-application.yaml`

```yaml title=""my-first-edge-application.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21.6
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        ports:
        - containerPort: 80
```

:::note
You can also use an image from a private registry as a part of your deployment. You can achieve this by creating a secret object containing the image pull credentials and specifying the same in your deployment object. You can read more about how to do this [here](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
:::

## Use kubectl to apply your Deployment

### Deploy your application

```bash
  kubectl apply -f my-first-edge-application.yaml
```",368
Get Started with Kubernetes API,See your deployment running on Section,"Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 
Use kubectl config to create a context
Define Section as a cluster using your `Kubernetes API URL`:
Save your API token
Switch to the new context
Deploy a web server to the Section Edge for HTTP workload
Create a Deployment object
See your deployment running on Section
Get Started with Kubernetes API
Configure kubectl to interact with the Kubernetes API to make 

```bash
  kubectl get deployment nginx-deployment
```
### See the pods running on Section's network

```bash
  kubectl get pods -o wide
```

The ```-o wide``` switch shows where the pod is running according to the default AEE location optimization strategy. Ultimately you will have NxM pods running in the Section [Composable Edge Cloud](/explanations/cec/), where N is the number of replicas, and M is the number of edge locations where your workload is present.",202
Create a Project with Section API,Get Your Account ID,"Create a Project with Section API
Create a Project using the Section Application Programming Interface (API) before starting with the Kubernetes API.
Get Your Account ID
Create a Project with Section API
Create a Project using the Section Application Programming Interface (API) before starting with the Kubernetes API.
In addition to your token you will also need to your `SECTION_ACCOUNT_ID` to identify which account the Environment will be added to. Your Account ID can easily be seen in the browser's URL for the [Section Console](https://console.section.io), as in:

`https://console.section.io/overview/account/SECTION_ACCOUNT_ID`.

Alternatively, use the [Account API Call](https://aperture.section.io/api/ui/#!/Account/accountList) to get your Account's ID.

```bash title=""get_account_id.sh""
SECTION_API_TOKEN="""" # Create/retrieve from https://console.section.io/configure/user/tokens

curl \
  --header ""Accept: application/json"" \
  --header ""section-token: $SECTION_API_TOKEN"" \
  -X GET ""https://aperture.section.io/api/v1/account""
```

You will receive a JSON response with an `""id""`, this is your `SECTION_ACCOUNT_ID`.

### Create an Environment
Using the [Create Application](https://aperture.section.io/api/ui/#!/Application/applicationCreate) API call Section will automatically provision your Project.

You will send a JSON object with the following fields:
* The **hostname** This will be the domain name used to initialize the [managed ingress controller](/guides/http-extensions/http-ingress/) if you opt-in to using it. This field will also act as the name of your environment. *The **hostname** is limited to alphanumeric characters and the hyphen, and separated by dots.*
* The **origin** field is optional and not applicable to environments created for KEI
* The **stackName** field must be set to **kei**

The curl command below sends the needed JSON object, embedded right into the command line.
```bash title=""create_section_project.sh""
SECTION_API_TOKEN=""""  # See section above
SECTION_ACCOUNT_ID="""" # See section above
YOUR_ENVIRONMENT_HOSTNAME=""example.domain.io""

curl \
  --header ""section-token: $SECTION_API_TOKEN"" \
  --header ""Content-Type: application/json"" \
  --header ""Accept: application/json"" -d ""{ \
  \""hostname\"": \""$YOUR_ENVIRONMENT_HOSTNAME\"", \
  \""origin\"": \""blank\"", \
  \""stackName\"": \""kei\"" \
}"" ""https://aperture.section.io/api/v1/account/$SECTION_ACCOUNT_ID/application/create""
```

Any empty JSON response means that the command did not work.

You will now see a Project in the [Section Console](https://console.section.io/).",571
Other Web UIs,Other Web UIs Using Other Dashboards on Section,"Other Web UIs
Using Other Dashboards on Section
Because our platform exposes the standard Kubernetes API you can integrate your favourite Kubernetes Web UI where it already leverages the Kubernetes API. 

Examples include:

 * [Lens](https://k8slens.dev/) - 
 	A powerful Kubernetes UI which has recently been opensourced.
 * [Octant](https://octant.dev/) - 
   Simple to install and a dev friendly interface.
 * [KubeNav](https://kubenav.io/) - 
   A relatively new project that is mobile device friendly.

See our [guide to integration with Lens](/docs/guides/kubernetes-ui/other-ui/lens-integration/).",141
Lens Integration,Introduction,"Lens Integration
Integrating Lens with Section
Introduction
Lens Integration
Integrating Lens with Section

[Lens](https://k8slens.dev/) is a a really powerful UI for Kubernetes with support for Windows, macOS and Linux. It was recently released as an open source project.

![Lens](/img/docs/lens.png)

After you have downloaded Lens, you will want to add your Section project to your Lens Catalog.

![Lens](/img/docs/lensc1.png)

Choose settings for your project

![Lens](/img/docs/settings.png)

Then copy the Kubernetes Config for your project and save it to a local drive. 

![Lens](/img/docs/kubeconfig.png)

From Lens Dashboard choose your Catalog general preferences

![Lens](/img/docs/prefs.png)

Then sync the file you just downloaded (oth the whole folder if you have several project config files in that folder)

![Lens](/img/docs/sync.png)


And Voila!  Your Section projects are now availble in Lens to manage entirely from the Lens Dashboard.  

Click on the cluster the connect.  You may see a brief warning message from Lens which should be ignored while connecting.  Just hang 5 and Lens will connect.

![Lens](/img/docs/lenslive.png)",264
Multidomain and Path Routing with Nginx,Multidomain and Path Routing with Nginx,"Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Multidomain and Path Routing with Nginx
Add the Domains
Setup Project Structure
Use `configMapGenerator` to apply the Nginx Configuration
Nginx configuration
Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx

In this tutorial we will use Section and Nginx to create microservices with multidomain and multipath routing. We will outline how we can have multiple services and deployments all talking to each other within a Section application. 

You  will create two different deployments - `hello-node-deployment.yaml` and `hello-ruby-deployment.yaml`, as well as a service for each. Each deployment will return “hello world” in either node or ruby based on the routing.

There are many scenarios that can benefit from this architecture. For example Section has three different frontend microservices all serving different applications (repositories) to the same website. These include www.section.io, www.section.io/docs and www.section.io/engineering-education which are all static websites. Our single Section application also includes an application programming interface (API) that we use to fetch data from the Google Analytics API in order to display authors number of page views on the EngEd program. Each of these repositories have their own CI/CD pipelines allowing us to have a combination of public and private repos with added flexibility. 

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",365
Multidomain and Path Routing with Nginx,Add the Domains,"Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Multidomain and Path Routing with Nginx
Add the Domains
Setup Project Structure
Use `configMapGenerator` to apply the Nginx Configuration
Nginx configuration
Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Add the domains you’d like to handle the routing for in your Section application. In this example we will add the domains www.example-domain-1.com and www.example-domain-2.com.",125
Multidomain and Path Routing with Nginx,Setup Project Structure,"Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Multidomain and Path Routing with Nginx
Add the Domains
Setup Project Structure
Use `configMapGenerator` to apply the Nginx Configuration
Nginx configuration
Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Follow the Getting Started steps in Docs if you’re just starting out with Section. If you already have a Section application, point your `ingress-service` container to a Nginx container. It may help to use Kustomization if you aren’t already. With the use of kustomization, we can utilize the `configMapGenerator` to volume mount our nginx configuration.

Create the following `/k8s` directory and structure:
- /k8s
/base
hello-node-deployment.yaml
hello-ruby-deployment.yaml
hello-node-service.yaml
hello-ruby-service.yaml
ingress-service.yaml
kustomization.yaml
router.yaml
router.conf
For this example we will create `deployment-app1.yaml` and deployment-app2.yaml` as two nginx pods running different versions. 

```yaml title=""hello-node-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-node-deployment
  labels:
    app: hello-node
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-node
  template:
    metadata:
      labels:
        app: hello-node
    spec:
      containers:
        - name: hello-node
          image: pvermeyden/nodejs-hello-world:a1e8cf1edcc04e6d905078aed9861807f6da0da4
          imagePullPolicy: Always
          ports:
          - containerPort: 80
          resources:
            requests:
              memory: ""1Gi""
              cpu: ""500m""
            limits:
              memory: ""1Gi""
              cpu: ""500m""
 
```

```yaml title=""hello-ruby-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-ruby-deployment
  labels:
    app: hello-ruby
spec:
 replicas: 1
  selector:
    matchLabels:
      app: hello-ruby
  template:
    metadata:
      labels:
        app: hello-ruby
    spec:
      containers:
        - name: hello-ruby
          image: sebp/ruby-hello-world
          imagePullPolicy: Always
          ports:
          - containerPort: 80
          resources:
            requests:
              memory: ""1Gi""
              cpu: ""500m""
            limits:
              memory: ""1Gi""
              cpu: ""500m""
```

```yaml title=""hello-world-service.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello-world-service
  name: hello-world-service
  namespace: default
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hello-world
  sessionAffinity: None
  type: ClusterIP
  
```

```yaml title=""hello-ruby-service.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello-ruby-service
  name: hello-ruby-service
  namespace: default
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hello-ruby
  sessionAffinity: None
  type: ClusterIP

```",799
Multidomain and Path Routing with Nginx,Use `configMapGenerator` to apply the Nginx Configuration,"Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Multidomain and Path Routing with Nginx
Add the Domains
Setup Project Structure
Use `configMapGenerator` to apply the Nginx Configuration
Nginx configuration
Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Create a volume mount and configmap to load a custom nginx configuration for `router.yaml` and define the resources you’d like kustomization to manage.

```yaml title=""kustomization.yaml""
configMapGenerator:
 - name: router-config-mount
   files:
     - ./router.conf
 
resources:
 - hello-world-deployment.yaml
 - hello-world-service.yaml
 - hello-ruby-deployment.yaml
 - hello-ruby-service.yaml
 - router.yml
 - ingress-service.yml

```

The `configMapGenerator` defines the name and location of the file to mount. In this case the file path is `./router.conf` with the name `router-config-mount`.

Apply the `configMap` via `volumes` and `volumeMounts`
In our `router.yaml` below we will add the configMap we just created via `volumes` and `volumeMounts`.

```yaml title=""router.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.21.6
          imagePullPolicy: Always
          volumeMounts:
            - name: router-config
              mountPath: ""/etc/nginx/conf.d""
          resources:
            requests:
              memory: "".5Gi""
              cpu: ""500m""
            limits:
              memory: "".5Gi""
              cpu: ""500m""
          ports:
            - containerPort: 80
      volumes:
        - name: router-config
          configMap:
            name: router-config-mount
```",460
Multidomain and Path Routing with Nginx,Nginx configuration,"Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Multidomain and Path Routing with Nginx
Add the Domains
Setup Project Structure
Use `configMapGenerator` to apply the Nginx Configuration
Nginx configuration
Multidomain and Path Routing with Nginx
Learn to configure multidomain and path routing with Nginx
Create a `router.conf` file in the `k8s/base` directory. This configuration controls the routing for the two different domains. Both domains return the same applications for each location block. However these can point to different services. The important thing to note here is that we can use `proxy_pass` to point our routing to different services that we’ve created. 

```conf title=""router.conf""
server {
 listen       80;
 listen  [::]:80;
 server_name  www.example-domain-1.com;
 
 location /node {
   proxy_pass http://hello-world-service;
 }
 
 location /ruby {
   proxy_pass http://hello-ruby-service;
 }
}
 
server {
 listen       80;
 listen  [::]:80;
 server_name  www.example-domain-2.com;
 
 location /node {
   proxy_pass http://hello-world-service;
 }
 
 location /ruby {
   proxy_pass http:/hello-ruby-service;
 }
}

```

By adding multiple server blocks, we can create routing for different domains. Each server block can contain multiple location blocks. In each location block we can make use of Nginx Reverse Proxy by using the  `proxy_pass` key we can specify the name of a service we want the location to route to.
`
location / {
   proxy_pass http://hello-world-service;
 }
`

In the above example we are handling the routing for both domains  `www.example-domain-1.com` and `www.example-domain-2.com`. When a request comes from `www.example-domain-1.com/node` we route the request using `proxy_pass` to our service `hello-node-service.yaml`, this returns “hello world” from NodeJS. 

Similarly, a request from `www.example-domain-1.com/ruby` points to our service `hello-ruby-service.yaml`, returning us “hello world” from Ruby. As you can see the code above has the same configuration for both domains. However, in a real world scenario these can all have different configurations with different types of applications.

This architecture allows us to have multiple applications and microservices all in one Section application. With relatively simple configuration we can communicate back and forth between different pods and services.",536
Image Manipulation,Image Manipulation with imgproxy,"Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
Image Manipulation with imgproxy
Create a Deployment for imgproxy
Experiment with imgproxy
Original
imgproxy result
Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images

[imgproxy](https://imgproxy.net/) is a fast and secure standalone server for resizing and converting remote images. With this tutorial you'll deploy the open-source imgproxy container [from DockerHub](https://hub.docker.com/r/darthsim/imgproxy) to enable low-latency image manipulation close to your end users.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",188
Image Manipulation,Create a Deployment for imgproxy,"Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
Image Manipulation with imgproxy
Create a Deployment for imgproxy
Experiment with imgproxy
Original
imgproxy result
Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
Create the deployment for imgproxy as `imgproxy-deployment.yaml`.  This will direct Section to run the imgproxy open source container.

```yaml title=""imgproxy-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: imgproxy
  name: imgproxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: imgproxy
  template:
    metadata:
      labels:
        app: imgproxy
    spec:
      containers:
      - image: darthsim/imgproxy:latest
        imagePullPolicy: Always
        name: imgproxy
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f imgproxy-deployment.yaml`.

## Expose imgproxy on the Internet

We want to expose the imgproxy on the Internet so that it can serve image manipulation requests. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: imgproxy
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

![imgproxy pods](/img/docs/imgproxy-pods.png)",534
Image Manipulation,Experiment with imgproxy,"Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
Image Manipulation with imgproxy
Create a Deployment for imgproxy
Experiment with imgproxy
Original
imgproxy result
Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
Now, you can start using imgproxy. To resize the following image:
```
https://m.media-amazon.com/images/M/MV5BMmQ3ZmY4NzYtY2VmYi00ZDRmLTgyODAtZWYzZjhlNzk1NzU2XkEyXkFqcGdeQXVyNTc3MjUzNTI@.jpg
```

You use the following URL:

```
http://YOUR_ENVIRONMENT_HOSTNAME.section.app/insecure/rs:fill:300:400/g:sm/aHR0cHM6Ly9tLm1l/ZGlhLWFtYXpvbi5j/b20vaW1hZ2VzL00v/TVY1Qk1tUTNabVk0/TnpZdFkyVm1ZaTAw/WkRSbUxUZ3lPREF0/WldZelpqaGxOemsx/TnpVMlhrRXlYa0Zx/Y0dkZVFYVnlOVGMz/TWpVek5USUAuanBn.jpg
```

Using the URL above, imgproxy is instructed to resize it to fill an area of 300x400 size with “smart” gravity. “Smart” means that imgproxy chooses the most “interesting” part of the image.",353
Image Manipulation,Original,"Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
Image Manipulation with imgproxy
Create a Deployment for imgproxy
Experiment with imgproxy
Original
imgproxy result
Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
![Original](https://m.media-amazon.com/images/M/MV5BMmQ3ZmY4NzYtY2VmYi00ZDRmLTgyODAtZWYzZjhlNzk1NzU2XkEyXkFqcGdeQXVyNTc3MjUzNTI@.jpg)",139
Image Manipulation,imgproxy result,"Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
Image Manipulation with imgproxy
Create a Deployment for imgproxy
Experiment with imgproxy
Original
imgproxy result
Image Manipulation
Learn to deploy an imgproxy service for resizing and converting remote images
![imgproxy result](/img/docs/imgproxy-result.jpg)",74
Varnish Cache,Web App Caching with Varnish,"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters

You've decided to distribute your web app closer to your users for increased performance. Why not make it even faster by leveraging caching? In this tutorial we will use [Varnish](https://varnish-cache.org/) to enable caching for your container running a Webapp. You can learn all about Varnish from their own [documentation](https://varnish-cache.org/docs) or from our comprehensive [guide](https://www.section.io/blog/varnish-cache-tutorial-vcl/).

The pattern we'll demonstrate is beneficial for use cases such as the following:
- The web app uses a [Server Side Rendering](https://www.section.io/blog/server-side-rendering-edge-nodejs/) framework.
- The web app is a multi-page static site.
- The web app is a [Single Page App](https://www.section.io/blog/five-use-cases-nodejs-edge/). 

In all cases, you'll put less load on the web app container by putting Varnish in front.

![Varnish Cache](/img/docs/varn1.png)

The above diagram illustrates the case where Varnish and the WebApp are deployed together on Section.

The pattern also works when the app is centrally located in a single location, in which case you've instantly built a modern [CDN](https://en.wikipedia.org/wiki/Content_delivery_network) for a web app. If you are using a [static site generator](https://jamstack.org/generators/) like Next.js, Hugo, Gatsby, Jekyll, etc., then you absolutely need to put a CDN in front of it. And by using Section for your CDN, you'll get Section's [enhanced security](/about/security/#ddos-protection), plus you can customize your CDN with components of your choice, such as WAFs, image optimization, bot mitigation, and more.

![Web app CDN](/img/docs/varn3.png)

In addition to [Varnish](https://varnish-cache.org/), this tutorial introduces the use of [Kustomize](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/), a technology for keeping your ""kustomizations"" separate from generic YAML templates, and [NATS](https://nats.io), a cloud and edge native messaging system.",567
Varnish Cache,Traffic flow diagram,"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
*Client requests example-domain.section.app (when it has not been cached)*
```
example-domain.section.app
└─> Section's managed public ingress
    └─> Varnish
        └─> App
```

*Once the page has been cached, the response will be returned by Varnish, which reduces the load on the app itself*
```
example-domain.section.app
└─> Section's managed public ingress
    └─> Varnish
        └x App
```

We will be deploying the resources via `kubectl`, and leveraging [Kustomize's](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/) `configMapGenerator` to convert configuration files (Varnish's `default.vcl`) into a Kubernetes ConfigMap resource. We're using a demo image from Section for the 'app', but you can swap out the container image for one that you prefer.

This tutorial assumes that the reader has a basic understanding of the Varnish configuration language, and so will simply gloss over the `default.vcl` file. You may refer to the official [Varnish documentation](https://varnish-cache.org/docs/) for more details.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",411
Varnish Cache,Project structure,"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters

We show you how to pull the pieces together here. The entire project is available by cloning from Section's Github repo: [https://github.com/section/varnish-tutorial](https://github.com/section/varnish-tutorial). It uses the Section Varnish container on [Docker Hub](https://hub.docker.com/r/sectionio/varnish-cache-kei).

The folder structure will be as follows:

```
.
├── app
│   ├── kustomization.yaml
│   ├── webapp-deployment.yaml
│   └── webapp-service.yaml
├── varnish
│   ├── default.vcl
│   ├── kustomization.yaml
│   ├── varnish-deployment.yaml
│   └── varnish-service.yaml
└── kustomization.yaml

2 directories, 8 files
```

This tutorial will result in the creation of the following Kubernetes resources in your Section Project:
- 2 Deployments
- 2 Services
- 1 ConfigMap",305
Varnish Cache,Root 'kustomization.yaml',"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
This lists the resources/folders to be deployed when the `kubectl apply -k .` is run. The following code will deploy the resources declared within the `varnish` and `app` folders.

```yaml title=""kustomization.yaml""
resources:
- varnish/
- app/
```

### Folder 'app' 
This is where you would declare your app that sits after Varnish. Depending on your Varnish configuration, the returned responses will then be cached by Varnish.

```yaml title=""app/kustomization.yaml""
resources:
- webapp-deployment.yaml
- webapp-service.yaml
```",223
Varnish Cache,Folder 'varnish',"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
This is where you would configure the Varnish deployment, such as resource allocation and number of replicas. Kustomize's `configMapGenerator` takes care of converting the `default.vcl` file into a configmap. If the configmap is updated, the Varnish pods will be restarted to take on the new settings.

```yaml title=""varnish/kustomization.yaml""
resources:
- varnish-deployment.yaml
- varnish-service.yaml

configMapGenerator:
- name: vcl
  files:
  - default.vcl
```

```vcl title=""default.vcl""
vcl 4.1;
import dynamic;
backend default none;

sub vcl_init {
    new d = dynamic.director(port = ""80"", ttl = 60s);
}

## Redirect http to https
sub vcl_recv {
    if (req.http.X-Forwarded-Proto !~ ""(?i)https"") { 
        return (synth(10301, ""Moved Permanently"")); 
    }
}
sub vcl_synth {
    if (resp.status == 10301) {
        set resp.status = 301;
        set resp.http.Location = ""https://"" + req.http.host + req.url;
        return(deliver);
    }
}
## End redirect http to https

sub vcl_recv {
    set req.backend_hint = d.backend(""webapp-service""); # Replace this value with the name of the service linked to your app
    return (hash);
}

sub vcl_backend_response {
    set beresp.ttl = 600s;
    unset beresp.http.set-cookie;
    return (deliver);
}

sub vcl_deliver {
    if (obj.hits > 0) {
        set resp.http.Varnish-Cache = ""HIT"";
    } else {
        set resp.http.Varnish-Cache = ""MISS"";
    }
}

sub vcl_hash {
    hash_data(req.http.X-Forwarded-Proto);
}
```",504
Varnish Cache,Deploying the resources,"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Assuming you've configured the folder in accordance to the instructions above, you can deploy all the resources by running the following command from the project root folder:

```bash
kubectl apply -k .
```

You can review the resources after creation by running the following command:

```bash
kubectl get pods,deployment,service,configmap -o wide
```",167
Varnish Cache,Issuing Varnish Cache Bans,"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
[Banning](https://www.varnish-software.com/developers/tutorials/ban/) is a concept in Varnish that allows expression-based cache invalidation. We give two approaches, one that works for infrequent bans, and the other that works better when bans are the result of automation.",155
Varnish Cache,Low frequency bans,"Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Web App Caching with Varnish
Traffic flow diagram
Project structure
Root 'kustomization.yaml'
Folder 'varnish'
Deploying the resources
Issuing Varnish Cache Bans
Low frequency bans
Varnish Cache
Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters
Bans can be issued to all Varnish pods deployed in your project worldwide by using kubectl exec as shown below. Note that this method is not suitable for more than a few bans per minute.

```bash
BAN_EXPRESSION=""req.url ~ /path/to/ban""

kubectl get pod --selector app=varnish-cache -o name \
  | xargs -t -I{} kubectl exec {} -- varnishadm ban $BAN_EXPRESSION
```

## High frequency bans
For high frequency bans, our Varnish container has been programmed to be able to receive messages from the CNCF [NATS](https://nats.io/) cloud and edge native messaging system. You can provision your own instance of NATS. Or you can leverage a managed offering such as [Synadia](https://synadia.com/), a distributed instance of NATS that will work well with your distributed Section project.

![Varnish Cache with NATS](/img/docs/varn2.png)

The `NATS_URL` and `NATS_SUBJECT` environment variables within the `varnish-cache` deployment ([varnish-deployment.yaml](https://github.com/section/varnish-tutorial/blob/main/simple-example/varnish/varnish-deployment.yaml)) specify a NATS server, and a NATS subject respectively where a Golang-implemented process in the Pod will subscribe to receive Varnish Cache ban messages via NATS. The NATS cli is illustrated, but you can use any mechanism you like to get messages onto the message bus.

The messages in NATS are expected to be JSON-encoded and use the structure from the following example, where banExpression is any valid expression that would be accepted by the `varnishadm` ban command.

```json
{
  ""operation"": ""applyban"",
  ""parameters"": {
    ""banExpression"": ""req.url ~ /foo""
  }
}
```",483
CI/CD Pipeline Deployment,CI/CD Pipeline Deployment,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
If you've been manually deploying your app onto Section (e.g. via the Console UI or `kubectl`), there are many benefits to having an automated pipeline deployment that you should consider.

Reasons to have an automated pipeline:
- There's no need to share Auth tokens to access your Kubernetes cluster amongst your dev team
- You can run automated tests prior, mitigating risks of a bad deployment
- Builds always commence from a consistent starting condition, avoiding scenarios of ""it worked on my machine""",176
CI/CD Pipeline Deployment,Examples,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically

This tutorial provides you with examples for GitHub Actions workflow, and Bitbucket pipelines. If you require assistance with other CI/CD tools, reach out to us via the [support](https://console.section.io/support) channel and we will endeavour to assist.",126
CI/CD Pipeline Deployment,Prerequisites,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
You will need to obtain the following values before proceeding:
- `SECTION_K8S_API_URL`: This is the value of your project's [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) 
- `SECTION_API_TOKEN`: Create or use an existing [Section API token](/guides/iam/api-tokens/)

*Optional*:
- A container image repository",160
CI/CD Pipeline Deployment,Create your Kubernetes deployment YAMLs,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically

For simplicity, we will store our deployment YAMLs within a folder named `k8s` at the root of the repository. You may add additional files to be deployed if desired.

```sh
mkdir -p k8s
touch k8s/deploy.yaml
```

```yaml title=""./k8s/deploy.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cicd-demo
  labels:
    app: cicd-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cicd-demo
  template:
    metadata:
      labels:
        app: cicd-demo
    spec:
      containers:
      - name: cicd-demo
        image: ${IMAGE_NAME}:main
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "".1""
            memory: "".1Gi""
          limits:
            cpu: "".1""
            memory: "".1Gi""
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-upstream
  labels:
    app: ingress-upstream
spec:
  selector:
    app: cicd-demo
  ports:
    - name: 80-to-80
      protocol: TCP
      port: 80
      targetPort: 80

```

---",363
CI/CD Pipeline Deployment,GitHub Actions,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically

GitHub Actions ([docs](https://docs.github.com/en/actions/using-workflows/about-workflows)) are defined by creating workflow `.yaml` files within the `.github/workflows` directory in the root of your repository. The following is an example folder structure, with a `deploy-to-section.yaml` action defined:

``` title=""GitHub Actions folder structure""
.
├── .github
│   └── workflows
│       ├── < WORKFLOW_NAME >.yaml
│       └── deploy-to-section.yaml
├── k8s
│   └── deploy.yaml
├── Dockerfile
├── README.md
└── ...
```

### Add repository secrets

Before you add and commit the workflows YAML files, you will first need to insert the two values from earlier (`SECTION_K8S_API_URL` & `SECTION_API_TOKEN`) as repository secrets in the GitHub repo. 

This can be added via Settings > Secrets > Actions, use **`New repository secret`** to add the two secrets.

![GitHub Secrets](/img/docs/cicd-pipeline-gh-secrets.png)


### Create the workflow YAML

Create the GitHub Actions workflows `directory` and `YAML` if you haven't done so already

```sh
mkdir -p .github/workflows
touch .github/workflows/deploy-to-section.yaml
```

Within the `deploy-to-section.yaml` file, insert the following contents:

```yaml title=""deploy-to-section.yaml""
name: Deploy to Section

on:
  push:
    branches: [main]

env:
  REGISTRY: ghcr.io
  REPO_NAME: ${{ github.repository }} # Uses the GitHub repository name as image name by default. Override if desired.
  APP_NAME: cicd-demo # k8s deployment's app name (refer to k8s/deploy.yaml)

jobs:
  build-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
          
      - name: downcase REPO
        run: |
          echo ""LOWERCASE_REPO=${REPO_NAME,,}"" >>${GITHUB_ENV}
        
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.LOWERCASE_REPO }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

      - name: Deploy on Section
        env:
          SECTION_K8S_API_URL: ""${{ secrets.SECTION_K8S_API_URL }}""
          SECTION_API_TOKEN: ""${{ secrets.SECTION_API_TOKEN }}""
          CERT_PATH: ""/etc/ssl/certs/ca-certificates.crt""
          IMAGE_NAME: ${{ env.REGISTRY }}/${{ env.LOWERCASE_REPO }}
        run: | 
          #######################################
          # Configure kubectl to talk to Section
          #######################################

          kubectl config set-cluster section-cluster --server=$SECTION_K8S_API_URL --certificate-authority=$CERT_PATH
          kubectl config set-credentials section-user --token=$SECTION_API_TOKEN
          kubectl config set-context section --cluster=section-cluster --user=section-user

          kubectl config use-context section

          ########################
          # Deploy k8s YAML file
          ########################

          envsubst '$IMAGE_NAME' < ./k8s/deploy.yaml > ./k8s/deploy.yaml.temp
          mv ./k8s/deploy.yaml.temp ./k8s/deploy.yaml

          kubectl apply -f ./k8s/

          kubectl rollout restart deployment $APP_NAME

```

Once all files have been committed and pushed, you should see the GitHub Action run immediately.

---

## Bitbucket Pipelines

Bitbucket Pipelines ([docs](https://support.atlassian.com/bitbucket-cloud/docs/configure-bitbucket-pipelinesyml/)) are configured by creating a `bitbucket-pipelines.yml` file in the root of your repository. The following is an example folder structure:

``` title=""Bitbucket Pipelines folder structure""
.
├── k8s
│   └── deploy.yaml
├── bitbucket-pipelines.yml
├── Dockerfile
├── README.md
└── ...
```",1071
CI/CD Pipeline Deployment,Add Repository Variables,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically

You may refer to the [Bitbucket documentation](https://support.atlassian.com/bitbucket-cloud/docs/variables-and-secrets/) on steps to add Secrets and Variables to be used by this pipeline.

For the purposes of this tutorial, we assume that your image will be deployed on Docker Hub, and will require inserting user credentials.

Secrets/Variables needed:
- `DOCKERHUB_USERNAME`: Login credentials for Docker Hub.
- `DOCKERHUB_PASSWORD`: Login credentials for Docker Hub.
- `DOCKERHUB_NAMESPACE`: This may be the same value as the Username. If not, ensure that the user has write permissions to this namespace.
- `SECTION_K8S_API_URL`: This is the value of your project's [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) 
- `SECTION_API_TOKEN`: Create or use an existing [Section API token](/guides/iam/api-tokens/)
- `IMAGE_NAME`: The name for the built container image.",281
CI/CD Pipeline Deployment,Create the pipeline YAML,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically

Create the `bitbucket-pipelines.yml` file if you haven't done so already

```sh
touch bitbucket-pipelines.yml
```

Within that file, insert the following contents:
```yaml title=""bitbucket-pipelines.yml""
image: node:16.16.0
definitions:
  services:
    docker:
      memory: 2048
      
pipelines:            
  branches:
    main:
      - step:
          name: ""Build and push to Docker Hub""
          services:
            - docker
          caches:
            - docker
          script:
            - docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_PASSWORD
            - docker build -t $DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT .
            - docker push $DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT

      - step:
          name: Deploy to Section.io
          deployment: production
          script:
            # Install Kubectl
            - echo ""Installing Kubectl...""
            - curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl""
            - curl -LO ""https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256""
            - echo ""$(cat kubectl.sha256)  kubectl"" | sha256sum --check
            - install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

            # Configure kubectl to connect to Section's cluster
            - echo ""Configuring Kubectl...""
            - PATH_TO_CERT_AUTHORITY=""/etc/ssl/certs/ca-certificates.crt""
            - kubectl config set-cluster section-cluster --certificate-authority=$PATH_TO_CERT_AUTHORITY --server=$SECTION_K8S_API_URL
            - kubectl config set-credentials section-user --token=$SECTION_API_TOKEN
            - kubectl config set-context section --cluster=section-cluster --user=section-user --namespace=default
            - kubectl config use-context section

            # Deploying to Section
            - echo ""Deploying to Section...""
            - IMAGE_NAME=$DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT
            - envsubst '$IMAGE_NAME' < ./k8s/deploy.yaml > ./k8s/deploy.yaml.temp
            - mv ./k8s/deploy.yaml.temp ./k8s/deploy.yaml
            - kubectl apply -f ./k8s/
            - APP_NAME=""cicd-demo""  # k8s deployment's app name (refer to k8s/deploy.yaml above)
            - kubectl rollout restart deployment $APP_NAME

```

Once all files have been committed and pushed, you should see the Bitbucket pipeline run on any commits on the main branch.

---",685
CI/CD Pipeline Deployment,Need assistance?,"CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically
CI/CD Pipeline Deployment
Examples
Prerequisites
Create your Kubernetes deployment YAMLs
GitHub Actions
Add Repository Variables
Create the pipeline YAML
Need assistance?
CI/CD Pipeline Deployment
Learn how to create a pipeline that deploys your code to Section automatically

If you require assistance with other CI/CD tools, reach out to us via the [support](https://console.section.io/support) channel and we will endeavour to assist.",110
Migration from Fastly,Porting your Fastly VCL Configuration To Varnish On Section.io,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section",103
Migration from Fastly,Introduction,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section

This is a general guide to porting VCL (Varnish Configuration Language) from your Fastly CDN service to a Varnish container running on Section’s Kubernetes Edge Interface (KEI). 

In general, much of the VCL used on the Fastly platform can directly port over to Varnish (We recommend running Varnish 7.0.2 or later), but there are a few caveats and things to consider.",195
Migration from Fastly,Considerations,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section",103
Migration from Fastly,Infrastructure,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section",103
Migration from Fastly,Origin shielding,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section

*  Origin shielding is a Fastly infrastructure feature that sends edge requests to ‘edge POPs’, which then make the request to a ‘shield POP’, increasing cache hit ratio in certain situations. If you are running Varnish on a large number of POPs and would like to see if Section can help you build a similar system through our Professional Services program, please contact Support.",181
Migration from Fastly,Clustering,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section

* By default, Fastly uses an algorithm to route requests to the same URL to the same Varnish instance in their POPs. This is known as ‘clustering’ and is not a default Varnish behavior. If you have a large-scale CDN plan in mind, we recommend using a `consistenthash` container to provide similar functionality, increasing cache hit ratio for cacheable assets. Improvements in cache hit ratio via deploying `consistenthash` will be dependent on the number of Varnish containers and endpoints you will be running.",214
Migration from Fastly,Logging,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section

* Fastly logging format is defined in the VCL itself. On Section, you will define an external logging endpoint as we show in our [Log Streaming guide](https://sites.www.section.io/docs/guides/monitor/logs/log-streaming/). This has the advantage of being able to log all components to your logging system - so any A/B testing, IDS or bot detection components you choose to deploy can also send logs to your storage, enabling full visibility into the full application stack.",203
Migration from Fastly,Dictionaries and ACLs,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section
  
* Dictionaries are a proprietary Fastly feature, and there is not yet equivalent functionality in the open source Varnish. 
* Note that Access Control Lists (ACLs) _do_ work in OSS Varnish, so if you are using ACLs on your Fastly service to control access on an IP level, they will be usable.",175
Migration from Fastly,Code Considerations,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section",103
Migration from Fastly,Special variables,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section",103
Migration from Fastly,req.url.path,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section
* This variable contains the request URL without any query strings. This is a proprietary Fastly feature that has no out of the box equivalent in OSS Varnish. If you require this functionality, contact our Support Team to discuss your neesd and a possible implentation.",160
Migration from Fastly,Subroutines,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section",103
Migration from Fastly,`vcl_fetch`,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section

* This is Fastly’s name for the `vcl_backend_response` subroutine, which runs immediately after the content is fetched either from the origin or cache, and before it is delivered to the browser (via `vcl_deliver`).  If you would like to modify a response before delivering to the browser, place this in `vcl_backend_response`.",176
Migration from Fastly,`vcl_hit` and `vcl_miss`,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section

* These functions are available to your Varnish configuration, but are not present in the default config. Feel free to add them if they are needed.",135
Migration from Fastly,VCL example,"Migration from Fastly
Learn to migrate your VCL from Fastly Section
Porting your Fastly VCL Configuration To Varnish On Section.io
Introduction
Considerations
Infrastructure
Origin shielding
Clustering
Logging
Dictionaries and ACLs
Code Considerations
Special variables
req.url.path
Subroutines
`vcl_fetch`
`vcl_hit` and `vcl_miss`
VCL example
Migration from Fastly
Learn to migrate your VCL from Fastly Section

Fastly creates a new service with a basic configuration suitable for general caching needs.  In most cases, the default Varnish VCL below is a great starting point.

```
# This is an extension on the default VCL that Section has created to get
# you up and running with Varnish.
#
# Please note: There is an underlying default Varnish behavior that occurs after the VCL logic
# you see below. You can see the builtin code here
# https://github.com/varnishcache/varnish-cache/blob/5.2/bin/varnishd/builtin.vcl
#
# See the VCL chapters in the Users Guide at https://www.varnish-cache.org/docs/
# and http://varnish-cache.org/trac/wiki/VCLExamples for more examples.

# Marker to tell the VCL compiler that this VCL has been adapted to the
# new 4.0 format.
vcl 4.0;

# Tells Varnish the location of the upstream. Do not change .host and .port.
backend default {
    .host = “backend.example.com”;
    .port = ""80"";
    .first_byte_timeout = 125s;
    .between_bytes_timeout = 125s;
}

# The following VMODs are available for use if required:
#import std; # see https://www.varnish-cache.org/docs/5.2/reference/vmod_std.generated.html
#import header; # see https://github.com/varnish/varnish-modules


# Method: vcl_recv
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-recv
# Description: Happens before we check if we have this in cache already.
#
# Purpose: Typically you clean up the request here, removing cookies you don't need,
# rewriting the request, etc.
sub vcl_recv {
    
# Section default code
#
# Purpose: If the request method is not GET, HEAD or PURGE, return pass.
# Documentation: Reference documentation for vcl_recv.
if (req.method != ""GET"" && req.method != ""HEAD"" && req.method != ""PURGE"") {
    return (pass);
}

# Section default code
#
# Purpose: If the request contains auth header return pass.
# Documentation: Reference documentation for vcl_recv.
if (req.http.Authorization) {
    /* Not cacheable by default */
    return (pass);
    }

}


# Method: vcl_backend_fetch
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-backend-fetch
# Description: Called before sending the backend request.
#
# Purpose: Typically you alter the request for the backend here. Overriding to the
# required hostname, upstream Proto matching, etc
sub vcl_backend_fetch {
    # No default Section code for vcl_backend_fetch
}


# Method: vcl_backend_response
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-backend-response
# Description: Happens after reading the response headers from the backend.
#
# Purpose: Here you clean the response headers, removing Set-Cookie headers
# and other mistakes your backend may produce. This is also where you can manually
# set cache TTL periods.
sub vcl_backend_response {

unset beresp.http.Vary;

}


# Method: vcl_deliver
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-deliver
# Description: Happens when we have all the pieces we need, and are about to send the
# response to the client.
#
# Purpose: You can do accounting logic or modify the final object here.
sub vcl_deliver {
    # Section default code
    #
    # Purpose: We are setting 'HIT' or 'MISS' as a custom header for easy debugging.
    if (obj.hits > 0) {
       set resp.http.section-io-cache = ""Hit"";
    } else {
       set resp.http.section-io-cache = ""Miss"";
    }
    
set resp.http.hits = obj.hits;

}

sub vcl_synth {
  
}

# Method: vcl_hash
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-hash
# Description: This method is used to build up a key to look up the object in Varnish.
#
# Purpose: You can specify which headers you want to cache by.
sub vcl_hash {
    # Section default code
    #
    # Purpose: Split cache by HTTP and HTTPS protocol.
    hash_data(req.http.X-Forwarded-Proto);
}
```",1097
Cloudflare Workers on Section,Deploy a CF Worker-like App on Section,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section",91
Cloudflare Workers on Section,Why run a container instead of Functions-as-a-Service (FaaS)?,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
- Cloudflare Workers are resource bound, with 128 MB of memory allocated. 
- Cloudflare Workers are time capped. 10ms, 50ms, up to 30s depending on pricing tier.

If either of the above is causing a limitation for your workload, you should consider moving from a FaaS to a container hosted solution.

With containers, you have the ability to choose your own resource allocation, or define your own timeout duration for long running workloads (_such as reports generation_), leverage other containers from the community, and develop solutions using any language and technology of your choice.

In this tutorial we will show you how to convert your CF Worker to a container image, which you can then host on Section's multi-region, multi-cloud Kubernetes hosting solution.

This example app contains 2 function paths:
- A basic hello world, eg [Return small HTML page](https://developers.cloudflare.com/workers/examples/return-html/)
- A redirector, eg [Respond with another site](https://developers.cloudflare.com/workers/examples/respond-with-another-site/)",307
Cloudflare Workers on Section,Step by Step,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section",91
Cloudflare Workers on Section,Prerequisites,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- An ExpressJS app
- (optional) kubectl",123
Cloudflare Workers on Section,Steps,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
1. Folder structure
1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section",118
Cloudflare Workers on Section,Folder structure,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
You can refer to our example repository here - https://github.com/section/cfworker-tutorial

In our example, we have an existing Express setup, with a subfolder containing each function's code.

```
.
├── functions
│   ├── helloWorld.js
│   ├── respondWithAnotherSite.js
│   └── ...
├── app.js
├── Dockerfile
└── package.json
```

Within `app.js`, you would write the specific paths that correspond with the functions to be run. In our example, the ""`/`"" route returns a basic Hello World html page, and the ""`/section`"" route returns a redirected page.

```js title=""app.js""
const express = require('express');
const helloWorld = require('./functions/helloWorld');
const respondWithAnotherSite = require('./functions/respondWithAnotherSite');

const app = express()
const port = process.env.PORT || 3000

// Return small HTML page - https://developers.cloudflare.com/workers/examples/return-html/
app.get('/', async function(req, res) {
  res.send(await helloWorld())
})

// Respond with another site - https://developers.cloudflare.com/workers/examples/respond-with-another-site/
app.get('/section', async function(req, res) {
  res.send(await respondWithAnotherSite())
})


app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`)
})
```

To test that this works, you simply need to run `node app.js`

### Creating your Dockerfile

Create a `Dockerfile` in the root folder of your app. It should sit alongside package.json.

The [Dockerfile](https://github.com/section/cfworker-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as runner
WORKDIR /app
COPY package*.json ./
RUN npm clean-install
COPY . .
EXPOSE 3000
CMD [""node"", ""app.js""]

```

This script would use Node to run an Express app listening on port 3000 of the container for any incoming requests.",541
Cloudflare Workers on Section,Building the container image,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section

Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh",112
Cloudflare Workers on Section,Replace these example values,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
USER=section
IMAGENAME=cf-worker
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```",273
Cloudflare Workers on Section,Deploy to Section,"Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section
Deploy a CF Worker-like App on Section
Why run a container instead of Functions-as-a-Service (FaaS)?
Step by Step
Prerequisites
Steps
Folder structure
Building the container image
Replace these example values
Deploy to Section
Cloudflare Workers on Section
Learn how to move your Cloudflare workers into a container image on Section

Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before, and specify port 3000.",128
Mastodon,Mastodon on Section,"Mastodon
Learn to deploy a Mastodon server at the edge
Mastodon on Section
Prerequisites
Deploy It
See What You've Built
Mastodon
Learn to deploy a Mastodon server at the edge
Learn how to run a Mastodon server at the edge for low latency and high availability. Perform the steps below using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",167
Mastodon,Prerequisites,"Mastodon
Learn to deploy a Mastodon server at the edge
Mastodon on Section
Prerequisites
Deploy It
See What You've Built
Mastodon
Learn to deploy a Mastodon server at the edge
* You need a Postgres database (try using [PolyScale.ai](https://polyscale.ai) to optimize access to a fixed-location Postgres database of your choice). See our [tutorial](/tutorials/data/polyscale-caching.md) on how PolyScale works with Section.
* Redis database (try a managed Redis, such as [DigitalOcean's](https://www.digitalocean.com/products/managed-databases-redis), or from the several listed [here](https://geekflare.com/redis-hosting-platform/)).
* Email delivery service or SMTP server.
* S3 bucket (try a distributed managed object store like [Supabase](https://supabase.com), [Backblaze](https://www.backblaze.com/b2/docs/s3_compatible_api.html), [Synadia Jetstream Obj store](https://synadia.com/ngs), or [Wasabi](https://wasabi.com)).",239
Mastodon,Deploy It,"Mastodon
Learn to deploy a Mastodon server at the edge
Mastodon on Section
Prerequisites
Deploy It
See What You've Built
Mastodon
Learn to deploy a Mastodon server at the edge
Create a Section deployment for the Mastodon server with a `mastodon-deployment.yaml` file, substituting the [environment variables](https://docs.joinmastodon.org/admin/config/) accordingly. This will direct Section to distribute the [`linuxserver/mastodon`](https://hub.docker.com/r/linuxserver/mastodon) image.

```yaml title=""mastodon-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mastodon
  labels:
    app: mastodon
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mastodon
  template:
    metadata:
      labels:
        app: mastodon
    spec:
      containers:
      - name: mastodon
        image: linuxserver/mastodon:4.0.2
        imagePullPolicy: Always
        lifecycle:
          postStart:
            exec:
              command:
              - ""/bin/sh""
              - ""-c""
              - >
                sleep 5;
                sed -i -e ""s/\$scheme/'https'/"" /config/nginx/site-confs/default.conf
        resources:
          requests:
            memory: ""1000Mi""
            cpu: ""1000m""
          limits:
            memory: ""1000Mi""
            cpu: ""1000m""
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            port: 80
            httpHeaders:
            - name: ""Host""
              value: ""mastodon.example.com""
          failureThreshold: 15
          initialDelaySeconds: 60
          periodSeconds: 20
        env:
        - name: PUID
          value: ""1000""
        - name: PGID
          value: ""1000""
        - name: TZ
          value: ""America/New_York""
        - name: LOCAL_DOMAIN
          value: ""mastodon.example.com""
        - name: REDIS_HOST
          value: ""redis""
        - name: REDIS_PORT
          value: ""6379""
        - name: DB_HOST
          value: ""db""
        - name: DB_USER
          value: ""mastodon""
        - name: DB_NAME
          value: ""mastodon""
        - name: DB_PASS
          value: ""mastodon""
        - name: DB_PORT
          value: ""5432""
        - name: SECRET_KEY_BASE
          value: """"
        - name: OTP_SECRET
          value: """"
        - name: VAPID_PRIVATE_KEY
          value: """"
        - name: VAPID_PUBLIC_KEY
          value: """"
        - name: SMTP_SERVER
          value: ""mail.example.com""
        - name: SMTP_PORT
          value: ""25""
        - name: SMTP_LOGIN
          value: """"
        - name: SMTP_PASSWORD
          value: """"
        - name: SMTP_FROM_ADDRESS
          value: ""notifications@example.com""
        - name: ES_ENABLED
          value: ""false""
        - name: ES_HOST # optional
          value: ""es""
        - name: ES_PORT # optional
          value: ""9200""
        - name: ES_USER # optional
          value: ""elastic""
        - name: ES_PASS # optional
          value: ""elastic""
        - name: S3_ENABLED
          value: ""false""
        - name: S3_BUCKET # optional
          value: """"
        - name: AWS_ACCESS_KEY_ID # optional
          value: """"
        - name: AWS_SECRET_ACCESS_KEY # optional
          value: """"
        - name: S3_ALIAS_HOST # optional
          value: """"
        - name: WEB_DOMAIN # optional
          value: ""mastodon.example.com""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f mastodon-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::tip
For a production Mastodon server, use [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables) as the values for private environment variables.
:::

### Expose It
Expose it on the internet, mapping the container's port `80`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
        app: mastodon
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`. The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",1215
Mastodon,See What You've Built,"Mastodon
Learn to deploy a Mastodon server at the edge
Mastodon on Section
Prerequisites
Deploy It
See What You've Built
Mastodon
Learn to deploy a Mastodon server at the edge
See the Mastodon server you've built by visiting the `https://mastodon.example.com`, substituting `mastodon.example.com` according to your DNS and HTTPS configuration.",83
Quant WAF,QuantWAF on Section,"Quant WAF
Learn how to deploy a Quant web application firewall at the edge
QuantWAF on Section
Prerequisites
Create the `values.yml` file
See What You've Built
Quant WAF
Learn how to deploy a Quant web application firewall at the edge

Learn how to deploy a QuantWAF instance at the edge for low latency, high availability and to secure your web applications. QuantWAF has been packaged as a [helm chart](https://helm.sh/) so make sure to configure [`kubectl`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::note
Before starting, create a new Section Project, you can then optionally [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment).
:::",157
Quant WAF,Prerequisites,"Quant WAF
Learn how to deploy a Quant web application firewall at the edge
QuantWAF on Section
Prerequisites
Create the `values.yml` file
See What You've Built
Quant WAF
Learn how to deploy a Quant web application firewall at the edge
* You will need a [QuantWAF License](https://quantcdn.io/waf), please contact QuantCDN directly to get a license key.",88
Quant WAF,Create the `values.yml` file,"Quant WAF
Learn how to deploy a Quant web application firewall at the edge
QuantWAF on Section
Prerequisites
Create the `values.yml` file
See What You've Built
Quant WAF
Learn how to deploy a Quant web application firewall at the edge

The QuantWAF helm chart provides a list of configuration options to aid in the deployment of the WAF instance. At minimum; `nextHop.selector` and `quant` keys must be defined in your values.yaml file. Please view [QuantWAF documentation](https://quantcdn.io/waf) for a full list of supported values.

```yaml title=""values.yaml""
nextHop:
  selector:
    app: console-project
quant:
  license: <your quantwaf license key>
  endpoint: <your quant endpoint>
  key: <your quant key>
  organization: <your quant organization>
  project: <your quant project>
```

`NextHop.selector` is an object that matches selector values that have been applied to your applications deployment. The default section application deployment will use `app: console-project` as the selector, however this will need to be updated to match selection criteria that you have defined for your application.

:::note
The configuration values for the `quant` configuration object will be provided to you during onboarding.
:::

### Deploy It

Create a Section deployment for the QuantWAF instance with `helm` and the `values.yaml` file, ensuring that you have updated the values file accordingly. This will create all the required resources and configure the QuantWAF appropriately for your account.

```
helm install quant-waf quant-waf -f values.yaml
```

:::tip
You can review the resources that will be created with `helm install --dry-run` before you apply directly to your cluster
:::


### Expose It
Expose it on the internet, mapping the container's port `80`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-80
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app.kubernetes.io/name: quant-waf
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`. The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",664
Quant WAF,See What You've Built,"Quant WAF
Learn how to deploy a Quant web application firewall at the edge
QuantWAF on Section
Prerequisites
Create the `values.yml` file
See What You've Built
Quant WAF
Learn how to deploy a Quant web application firewall at the edge

See the QuantWAF instance you've deployed by visiting the `https://quantwaf.example.com?q=/etc/hosts`, substituting `quantwaf.example.com` according to your DNS and HTTPS configuration.",100
GraphQL with Hasura and Supabase,Distributed GraphQL with Hasura and Supabase,"GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Hasura container to multiple datacenters. We will configure it to use Supabase as the Postgres database backend.

The Hasura container we will use is [available on DockerHub](https://hub.docker.com/r/hasura/graphql-engine).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",227
GraphQL with Hasura and Supabase,Prerequisites,"GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
* You need an account on [Supabase](https://supabase.com).",105
GraphQL with Hasura and Supabase,Create a Database on Supabase,"GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Create a new Postgres instance for Hasura to connect to. For this tutorial we will be using Supabase as they provide free, managed Postgres instances. But any Postgres database will work.
1. Visit https://app.supabase.com/ and click ""New project"".
2. Select a name, password, and region for your database. Make sure to save the password, as you will need it later.
3. Click ""Create new project"". Creating the project can take a while, so be patient.
4. Once the project is created, navigate to the ""Database"" tab on the left.

You have just created an empty database on Supabase.  Hasura will populate it with metadata upon first connection.",236
GraphQL with Hasura and Supabase,Get Your Connection String,"GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-SUPABASE-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@db.abcxyzabcxyzabcxyzabcxyz.supabase.co:5432/postgres`.

1. Go to Settings, and then Database.
1. Scroll down to the ""Connection string"" section and copy the connection string from the ""URI"" tab.  (Do not use the connection string in ""Connection pooling"", as Hasura is doing connection pooling of its own.)
1. Insert the password you saved earlier into the string at `[YOUR-PASSWORD]`.",233
GraphQL with Hasura and Supabase,Create a Deployment for Hasura,"GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Next, create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Substitute `YOUR_CONNECTION_STRING` accordingly. And supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: YOUR_CONNECTION_STRING
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",667
GraphQL with Hasura and Supabase,Experiment with Hasura,"GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and Supabase
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Now, you can start using Hasura. While the main purpose of the Postgres database is to store Hasura metadata, note that you can use the Data section of the Hasura console to create tables of your own in the same Postgress database, and then use Hasura's querying ability to access that data.",151
GraphQL with Hasura and AWS RDS Aurora Postgres,Distributed GraphQL with Hasura and AWS RDS Aurora Postgres,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Hasura container to multiple datacenters. We will configure it to use AWS RDS Aurora Postgres as the Postgres database backend.

The Hasura container we will use is [available on DockerHub](https://hub.docker.com/r/hasura/graphql-engine).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",273
GraphQL with Hasura and AWS RDS Aurora Postgres,Prerequisites,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
* You need an account on [AWS](https://aws.amazon.com).",145
GraphQL with Hasura and AWS RDS Aurora Postgres,Create a New AWS RDS Aurora Postgress Database,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Create a new Postgres instance for Hasura to connect to. For this tutorial we will be using AWS RDS Auroroa Postgres. But any Postgres database will work. Following is guidance on how to complete the various sections of the AWS creation process.",183
GraphQL with Hasura and AWS RDS Aurora Postgres,Create database,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
1. In your AWS console, search for RDS.
1. Choose ""Create database"".
1. Choose ""Standard create"".",156
GraphQL with Hasura and AWS RDS Aurora Postgres,Engine options,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
1. Engine type is ""Amazon Aurora"".
1. Edition is ""Amazon Aurora PostgresSQL-Compatible Edition"".",152
GraphQL with Hasura and AWS RDS Aurora Postgres,Templates,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
1. Use the ""Dev/Test"" Template.",140
GraphQL with Hasura and AWS RDS Aurora Postgres,Settings,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
1. Accept the DB cluster identifier of ""database-1"", or whatever it suggests.
1. Login ID for the master username should remain as ""postgres"".
1. Choose a strong password.",169
GraphQL with Hasura and AWS RDS Aurora Postgres,Instance configuration,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
1. DB instance class: Serverless.
1. Capacity range: leave the defaults.",148
GraphQL with Hasura and AWS RDS Aurora Postgres,Availability & durability,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
  1. Multi-AZ deployment ""Don't create an Aurora Replica"".",146
GraphQL with Hasura and AWS RDS Aurora Postgres,Connectivity,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
1. Network type: IPv4.
1. VPC: Default.
1. Subject group: default.
1. Public access: Yes.
1. VPC security group: ""Create new"".
1. New VPC security group name: ""hasura"" (or any name of your choosing).
1. Availability Zone: No Preference.
1. Database port (use the default) 5432.",212
GraphQL with Hasura and AWS RDS Aurora Postgres,Finishing Up,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Ignore the remaining sections of the AWS creation process.

Choose ""Create database"", and wait several minutes.",150
GraphQL with Hasura and AWS RDS Aurora Postgres,Expose Your Database to the Internet,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
In general, Section deployments use IP addresses that change as the workload moves according to developer requirements. Because of this, it is not possible to allow/block database traffic by using IP address or range. Hence you should allow connections from all IP addresses. Typically this can be achieved by entering an IP address of (0.0.0.0).  While this might be a concern, relying solely on IP for security is generally ineffective and can lead to poor security practices. Be sure to use SSL when connecting to a production database.

When you created your database and made the choice to give public access, AWS gave access only to your own computer's IP address.  You will now need to adjust the VPC security group so that all IP addresses are allowed (0.0.0.0), as the Hasura container running in Section will be contacting the database from many different IP addresses.

1. Select the writer instance.
1. Select the Connectivity & security tab.
1. In the Security section, select the VPC security group, which we called ""hasura"" above.
1. In the Inbound rules tab, you will have one IP address allowed.
1. Choose ""Edit inbound rules""
1. Remove the single IP address and add a new one of ""0.0.0.0/0"" (Anywhere-IPv4).
1. Choose ""Save rules"".
1. Using a different computer (different IP address), test access by telneting to the endpoint, `telnet YOUR-ENDPOINT 5432`.  If it connects, then you're good.",451
GraphQL with Hasura and AWS RDS Aurora Postgres,Get Your Connection String,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-CLUSTER-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@database-1.cluster-ckremhbvmdf0.us-east-1.rds.amazonaws.com:5432/postgres`

1. Choose your cluster DB identifier, likely ""database-1"".
1. In the Connectivity & security tab, copy a writer instance endpoint name. Save this string for the Hasura deployment, which is next.

You have just created an empty Postgres database on AWS RDS Aurora.  Hasura will populate it with metadata upon first connection.",276
GraphQL with Hasura and AWS RDS Aurora Postgres,Create a Deployment for Hasura,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Next, create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Substitute `YOUR_CONNECTION_STRING` accordingly. And supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: YOUR_CONNECTION_STRING
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```
Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",710
GraphQL with Hasura and AWS RDS Aurora Postgres,Experiment with Hasura,"GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL with Hasura and AWS RDS Aurora Postgres
Prerequisites
Create a New AWS RDS Aurora Postgress Database
Create database
Engine options
Templates
Settings
Instance configuration
Availability & durability
Connectivity
Finishing Up
Expose Your Database to the Internet
Get Your Connection String
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura and AWS RDS Aurora Postgres
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Now, you can start using Hasura.  While the main purpose of the Postgres database is to store Hasura metadata, note that you can use the Data section of the Hasura console to create tables of your own in the same Postgress database, and then use Hasura's querying ability to access that data.",195
GraphQL with Apollo Router,Distributed GraphQL with Apollo Router,"GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
Distributed GraphQL with Apollo Router
Prerequisites
Get Setup with Apollo
Setup a ConfigMap for Router Configuration
Expose the Service on the Internet
GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance

The [Apollo Router](https://www.apollographql.com/docs/router/) is a high-performance GraphQL router that runs a [federated supergraph](https://www.apollographql.com/docs/federation/), which is a single graph composed of multiple underlying services. By distributing the router container across Section's network around the world your apps will be more responsive thanks to faster responses to their API calls. This tutorial shows you how to distribute the Apollo Router on Section.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",235
GraphQL with Apollo Router,Prerequisites,"GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
Distributed GraphQL with Apollo Router
Prerequisites
Get Setup with Apollo
Setup a ConfigMap for Router Configuration
Expose the Service on the Internet
GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
* You need an Apollo Federation 2 project in Apollo Studio. Apollo's [Federation 2 quickstart](https://www.apollographql.com/docs/federation/quickstart/setup) can guide you if you don't have one already.",127
GraphQL with Apollo Router,Get Setup with Apollo,"GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
Distributed GraphQL with Apollo Router
Prerequisites
Get Setup with Apollo
Setup a ConfigMap for Router Configuration
Expose the Service on the Internet
GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
If you already have an Apollo Federation 2 project, then skip to the next step. Otherwise set on up by following Apollo's [Federation 2 quickstart](https://www.apollographql.com/docs/federation/quickstart/setup). You'll be leveraging sample data provided by Apollo.",137
GraphQL with Apollo Router,Setup a ConfigMap for Router Configuration,"GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
Distributed GraphQL with Apollo Router
Prerequisites
Get Setup with Apollo
Setup a ConfigMap for Router Configuration
Expose the Service on the Internet
GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
Create a `config.yaml` as defined below. This ConfigMap configures the router to listen on port 80. Additional examples of what can be configured are [here](https://www.apollographql.com/docs/router/containerization/kubernetes/#kubernetes-configuration).

```yaml title=""config.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: apollo
  labels:
    name: apollo
data:
  configuration.yaml: |
    server:
      listen: 0.0.0.0:80
```

Apply this ConfigMap resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f configmap.yaml`.

## Create a Kubernetes Deployment for Apollo Router
Next, create a Section deployment for Apollo Router with a file `apollo-router-deployment.yaml`, substituting `YOUR_APOLLO_KEY` and `YOUR_APOLLO_GRAPH_REF` accordingly. `YOUR_APOLLO_KEY` is your personal API KEY for [Apollo Studio](https://studio.apollographql.com).  And `YOUR_APOLLO_GRAPH_REF` is a string along the lines of `My-Graph-lypak@current`, and is the reference for your federated graph.

This deployment direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""apollo-router-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apollo
  name: apollo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apollo
  template:
    metadata:
      labels:
        app: apollo
    spec:
      containers:
      - image: ghcr.io/apollographql/router:v1.0.0-alpha.3
        imagePullPolicy: Always
        name: apollo
        args:
          - --hot-reload
          - --config
          - /app/configuration.yaml
        env:
          - name: APOLLO_KEY
            value: YOUR_APOLLO_KEY
          - name: APOLLO_GRAPH_REF
            value: YOUR_APOLLO_GRAPH_REF
        volumeMounts:
          - name: router-configuration
            mountPath: /app/configuration.yaml
            subPath: configuration.yaml
            readOnly: true
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
      volumes:
        - name: router-configuration
          configMap:
            name: apollo
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f apollo-router-deployment.yaml`.",678
GraphQL with Apollo Router,Expose the Service on the Internet,"GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
Distributed GraphQL with Apollo Router
Prerequisites
Get Setup with Apollo
Setup a ConfigMap for Router Configuration
Expose the Service on the Internet
GraphQL with Apollo Router
Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance
We want to expose the Apollo Server on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: apollo
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network with `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Apollo pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 4000.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

## Browse Your GraphQL API
Visit https://YOUR_ENVIRONMENT_HOSTNAME in your browser to play in the Apollo Sandbox. You may have multiple pods running in multiple locations, but your chosen hostname will route to the one that is physically closest.

Enter the following query:
```GraphQL
query Query {
  locations {
    name
  }
}
```
You'll see the following result:
![Apollo Router Response](/img/docs/apollo-router-response.png)",478
GraphQL with Hasura Quickstart,Distributed GraphQL Quickstart with Hasura,"GraphQL with Hasura Quickstart
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL Quickstart with Hasura
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura Quickstart
Learn to deploy a multi-datacenter, multi-provider, Hasura service

This is a GraphQL quickstart. Your apps will run faster if the APIs they call are physically located close to your end users. You'll deploy the open-source Hasura [from DockerHub](https://hub.docker.com/r/hasura/graphql-engine), and to make things quick we'll leverage a pre-created data source that Section provides. (Our other GraphQL tutorials show you how to configure various backend databases of your own.)

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",211
GraphQL with Hasura Quickstart,Create a Deployment for Hasura,"GraphQL with Hasura Quickstart
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL Quickstart with Hasura
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura Quickstart
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Note that we are using a connection string that points to a read-only Section owned/operated database on AWS. Be sure to supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: postgresql://read_only_user:8BiusLd6Z89kjVgS@database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com:5432/postgres
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)",672
GraphQL with Hasura Quickstart,Experiment with Hasura,"GraphQL with Hasura Quickstart
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Distributed GraphQL Quickstart with Hasura
Create a Deployment for Hasura
Experiment with Hasura
GraphQL with Hasura Quickstart
Learn to deploy a multi-datacenter, multi-provider, Hasura service
Now, you can start using Hasura.  The AWS RDS Aurora database contains a small amount of read-only ""pets"" data that you can use to experiment with Hasura's GraphQL querying ability.

![Pets query](/img/docs/pets.png)",117
GraphQL with Postgraphile and Supabase,Distributed GraphQL with Postgraphile and Supabase,"GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Distributed GraphQL with Postgraphile and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Postgraphile
Experiment with Postgraphile
GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Postgraphile container to multiple datacenters. We will configure it to use Supabase as the Postgres database backend.

The Postgraphile container we will use is [available on DockerHub](https://hub.docker.com/r/graphile/postgraphile).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",236
GraphQL with Postgraphile and Supabase,Prerequisites,"GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Distributed GraphQL with Postgraphile and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Postgraphile
Experiment with Postgraphile
GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
* You need an account on [Supabase](https://supabase.com).",112
GraphQL with Postgraphile and Supabase,Create a Database on Supabase,"GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Distributed GraphQL with Postgraphile and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Postgraphile
Experiment with Postgraphile
GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Create a new Postgres instance for Postgraphile to connect to. For this tutorial we will be using Supabase as they provide free, managed Postgres instances. But any Postgres database will work.
1. Visit https://app.supabase.com/ and click ""New project"".
2. Select a name, password, and region for your database. Make sure to save the password, as you will need it later.
3. Click ""Create new project"". Creating the project can take a while, so be patient.
4. Once the project is created, navigate to the ""Database"" tab on the left.

You have just created an empty database on Supabase.  Postgraphile will populate it with metadata upon first connection.",245
GraphQL with Postgraphile and Supabase,Get Your Connection String,"GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Distributed GraphQL with Postgraphile and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Postgraphile
Experiment with Postgraphile
GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-SUPABASE-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@db.abcxyzabcxyzabcxyzabcxyz.supabase.co:5432/postgres`.

1. Go to Settings, and then Database.
1. Scroll down to the ""Connection string"" section and copy the connection string from the ""URI"" tab.  (Do not use the connection string in ""Connection pooling"", as Postgraphile is doing connection pooling of its own.)
1. Insert the password you saved earlier into the string at `[YOUR-PASSWORD]`.",241
GraphQL with Postgraphile and Supabase,Create a Deployment for Postgraphile,"GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Distributed GraphQL with Postgraphile and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Postgraphile
Experiment with Postgraphile
GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Next, create the deployment for Postgraphile as `postgraphile-deployment.yaml`.  This will direct Section to run the Postgraphile open source container. Substitute `YOUR_CONNECTION_STRING` accordingly.

```yaml title=""postgraphile-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: postgraphile
  name: postgraphile
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgraphile
  template:
    metadata:
      labels:
        app: postgraphile
    spec:
      containers:
      - image: graphile/postgraphile:latest
        imagePullPolicy: Always
        name: postgraphile
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: DATABASE_URL
          value: YOUR_CONNECTION_STRING
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f postgraphile-deployment.yaml`.

## Expose the Postgraphile Console on the Internet

We want to expose the Postgraphile console on the Internet. Create `ingress-upstream.yaml` as defined below.  Note that it directs port 80 traffic to port 5000 on the Postgraphile container.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 5000
  selector:
    app: postgraphile
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",644
GraphQL with Postgraphile and Supabase,Experiment with Postgraphile,"GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Distributed GraphQL with Postgraphile and Supabase
Prerequisites
Create a Database on Supabase
Get Your Connection String
Create a Deployment for Postgraphile
Experiment with Postgraphile
GraphQL with Postgraphile and Supabase
Learn to deploy a multi-datacenter, multi-provider, Postgraphile service
Now, you can start using the GraphiQL interface of Postgraphile by visiting the `https://YOUR.DOMAIN.COM/graphiql`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",139
GraphQL with Apollo Server,Distributed GraphQL with Apollo Server,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will show you how to distribute an Apollo Server in multiple locations around the world by using the Section.

You'll first build a small Apollo Server container, push it to Docker Hub, and then deploy it to Section. This tutorial was inspired by an [example](https://www.preciouschicken.com/blog/posts/apollo-server-docker-container/) from PreciousChicken.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",306
GraphQL with Apollo Server,Prerequisites,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.",206
GraphQL with Apollo Server,Get Setup with Apollo,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Install the relevant packages.

```bash
mkdir apollo
cd apollo
npm init -y
npm install apollo-server graphql
```

## Create Your Container Image
The container image is Apollo code plus a small amount of static data, built upon node. For a real Apollo Server you'd have it pointing to a database or REST API. But static data is all we want for the minute. Let's start by saving the following code into `index.js`.

```javascript title=""index.js""
const { ApolloServer, gql } = require('apollo-server');

const data = {
  ""beasts"": [
    {
	    ""id"": ""md"",
	    ""legs"": 6,
	    ""binomial"": ""Musca domestica"",
	    ""commonName"": ""housefly"",
    },
    {
	    ""id"": ""nr"",
	    ""legs"": 8,
	    ""binomial"": ""Neriene radiata"",
	    ""commonName"": ""filmy dome spider"",
    },
    {
	    ""id"": ""cc"",
	    ""legs"": 2,
	    ""binomial"": ""Corvus corone"",
	    ""commonName"": ""carrion crow"",
    },
    {
	    ""id"": ""fc"",
	    ""legs"": 4,
	    ""binomial"": ""Felis catus"",
	    ""commonName"": ""cat"",
    }
  ]
};

const typeDefs = gql`
	type Beast {
		id: ID
		legs: Int
		binomial: String
		commonName: String
	}

	type Query {
		beasts: [Beast]
	}
`;

const resolvers = {
	Query: {
		// Returns array of all beasts.
		beasts: () => data.beasts
	}
};

const server = new ApolloServer({ typeDefs, resolvers });

// The `listen` method launches a web server.
server.listen(4000).then(({ url }) => {
    console.log(`🚀  Server ready at ${url}`);
});
```

Now let's define the Dockerfile.

```dockerfile title=""Dockerfile""",583
GraphQL with Apollo Server,Uses the node base image with the latest LTS version,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
FROM node:14.16.0",157
GraphQL with Apollo Server,Informs Docker that the container listens on the,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance",148
GraphQL with Apollo Server,specified network ports at runtime,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
EXPOSE 4000",154
GraphQL with Apollo Server,Copies index.js and the two package files from the local,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance",148
GraphQL with Apollo Server,directory to a new app directory on the container,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
COPY index.js package.json package-lock.json  app/",160
GraphQL with Apollo Server,Changes working directory to the new directory just created,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
WORKDIR /app",153
GraphQL with Apollo Server,Installs npm dependencies on container,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
RUN npm ci",152
GraphQL with Apollo Server,Command container will actually run when called,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
CMD [""node"", ""index.js""]
```

## Build and Publish the Image
Build the Docker image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.

```bash
docker build -t my-apollo-server-image .
docker tag my-apollo-server-image YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
docker push YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
```",241
GraphQL with Apollo Server,Create a Kubernetes Deployment for Apollo Server,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Next, create a Section deployment for Apollo Server with a file `apollo-deployment.yaml`, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""apollo-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apollo
  name: apollo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apollo
  template:
    metadata:
      labels:
        app: apollo
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
        imagePullPolicy: Always
        name: apollo
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f apollo-deployment.yaml`.

## Expose the Service on the Internet
We want to expose the Apollo Server on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 4000
  selector:
    app: apollo
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network with `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Apollo pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 4000.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",708
GraphQL with Apollo Server,Browse Your GraphQL API,"GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Distributed GraphQL with Apollo Server
Prerequisites
Get Setup with Apollo
Uses the node base image with the latest LTS version
Informs Docker that the container listens on the
specified network ports at runtime
Copies index.js and the two package files from the local
directory to a new app directory on the container
Changes working directory to the new directory just created
Installs npm dependencies on container
Command container will actually run when called
Create a Kubernetes Deployment for Apollo Server
Browse Your GraphQL API
GraphQL with Apollo Server
Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance
Visit https://YOUR_ENVIRONMENT_HOSTNAME in your browser to play in the Apollo Sandbox. You may have multiple pods running in multiple locations, but your chosen hostname will route to the one that is physically closest.

Enter the following query:
```GraphQL
{
  beasts {
    commonName
    legs
  }
}
```
You'll see the following result:
![Apollo Sandbox Beasts](/img/docs/apollo-sandbox-beasts.png)",239
Go,Go App on Section,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
Learn how to run a ""Hello World"" <a href=""https://go.dev/"">Go app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",156
Go,What You'll Build,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://aged-rain-2087.section.app/"">https://aged-rain-2087.section.app/</a></strong> to see what you'll be building.</p>",127
Go,Option 1 - Copy Our GitHub Repo,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
![workflow status](https://github.com/section/go-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/go-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.py` and watch your changes go live.",279
Go,Option 2 - Step by Step,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge

Following are step-by-step instructions to deploy a Go ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",106
Go,Prerequisites,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",99
Go,Create the Go App,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
Create a new directory for your app.

```bash
mkdir my-go-app
cd my-go-app
```

Create `helloworld.go` with the following code.

```go title=""helloworld.go""
package main

import (
    ""fmt""
    ""net/http""
)

func main() {
    http.HandleFunc(""/"", HelloServer)
    http.ListenAndServe("":8080"", nil)
}

func HelloServer(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, ""Hello World from Go on Section!"")
}
```",180
Go,Dockerize It,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM golang:1.19
WORKDIR /usr/src/my-go-app
RUN go mod init my-go-app
RUN go mod download && go mod verify
COPY . .
RUN go build -v -o /usr/local/bin/my-go-app ./...
CMD [""my-go-app""]
EXPOSE 8080
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",321
Go,Deploy It,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
Next, use the Create Project command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod` with port 8080.

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",281
Go,See What You've Built,"Go
Learn to deploy a Go app at the edge
Go App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Go App
Dockerize It
Deploy It
See What You've Built
Go
Learn to deploy a Go app at the edge
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",110
Nim,Nim App on Section,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
Learn how to run a ""Hello World"" <a href=""https://nim-lang.org/"">Nim app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",161
Nim,What You'll Build,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://fragrant-smoke-2968.section.app/"">https://fragrant-smoke-2968.section.app/</a></strong> to see what you'll be building.</p>",132
Nim,Option 1 - Copy Our GitHub Repo,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
![workflow status](https://github.com/section/nim-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/nim-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.nim` and watch your changes go live.",285
Nim,Option 2 - Step by Step,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge

Following are step-by-step instructions to deploy a Nim ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",109
Nim,Prerequisites,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",102
Nim,Create the Nim App,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
Create a new directory for your app.

```bash
mkdir my-nim-app
cd my-nim-app
```

Create `helloworld.nim` with the following code.

```nim title=""helloworld.nim""
import std/asynchttpserver
import std/asyncdispatch

proc main {.async.} =
  var server = newAsyncHttpServer()
  proc cb(req: Request) {.async.} =
    echo (req.reqMethod, req.url, req.headers)
    let headers = {""Content-type"": ""text/plain; charset=utf-8""}
    await req.respond(Http200, ""Hello World from Nim on Section!"", headers.newHttpHeaders())

  server.listen(Port(8080))
  let port = server.getPort
  echo ""Web server is now running on port "" & $port.uint16
  while true:
    if server.shouldAcceptRequest():
      await server.acceptRequest(cb)
    else:
      # too many concurrent connections, `maxFDs` exceeded
      # wait 500ms for FDs to be closed
      await sleepAsync(500)

waitFor main()
```",305
Nim,Dockerize It,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM nimlang/nim
COPY . ./my-nim-app
WORKDIR /my-nim-app
EXPOSE 8080
CMD [ ""nim"", ""c"", ""--run"", ""./helloworld.nim"" ]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",308
Nim,Deploy It,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`Web server is now running on port 8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",284
Nim,See What You've Built,"Nim
Learn to deploy a Nim app at the edge
Nim App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Nim App
Dockerize It
Deploy It
See What You've Built
Nim
Learn to deploy a Nim app at the edge
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",113
ASP.NET Core,ASP.NET Core on Section,"ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
ASP.NET Core on Section
What You'll Build
Step by Step
Prerequisites
Create the ASP.NET Core App
Push It
See What You've Built
ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
Learn how to run a <a href=""https://github.com/dotnet/dotnet-docker/tree/main/samples/aspnetapp"">sample</a> <a href=""https://learn.microsoft.com/en-us/aspnet/core/?view=aspnetcore-7.0"">ASP.NET Core app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",193
ASP.NET Core,What You'll Build,"ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
ASP.NET Core on Section
What You'll Build
Step by Step
Prerequisites
Create the ASP.NET Core App
Push It
See What You've Built
ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://spring-sea-8563.section.app/"">https://spring-sea-8563.section.app/</a></strong> to see what you'll be building.</p>",120
ASP.NET Core,Step by Step,"ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
ASP.NET Core on Section
What You'll Build
Step by Step
Prerequisites
Create the ASP.NET Core App
Push It
See What You've Built
ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge

Following are step-by-step instructions to deploy a ASP.NET Core <a href=""https://github.com/dotnet/dotnet-docker/tree/main/samples/aspnetapp"">sample</a> application to the edge on Section. We'll Dockerize it, and deploy it on Section.",126
ASP.NET Core,Prerequisites,"ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
ASP.NET Core on Section
What You'll Build
Step by Step
Prerequisites
Create the ASP.NET Core App
Push It
See What You've Built
ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",92
ASP.NET Core,Create the ASP.NET Core App,"ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
ASP.NET Core on Section
What You'll Build
Step by Step
Prerequisites
Create the ASP.NET Core App
Push It
See What You've Built
ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
Create a new directory for your app.

```bash
mkdir my-aspnetcore-app
cd my-aspnetcore-app
```

Download or clone the Microsoft ASP.NET Core <a href=""https://github.com/dotnet/dotnet-docker"">dotnet/dotnet-docker</a> repository from Github.

Then place the contents of the `aspnetapp` directory in your `my-aspnetcore-app` directory.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
# https://hub.docker.com/_/microsoft-dotnet
FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build
WORKDIR /source

# copy csproj and restore as distinct layers
COPY aspnetapp/*.csproj .
RUN dotnet restore --use-current-runtime  

# copy everything else and build app
COPY aspnetapp/. .
RUN dotnet publish -c Release -o /app --use-current-runtime --self-contained false --no-restore

# final stage/image
FROM mcr.microsoft.com/dotnet/aspnet:7.0
WORKDIR /app
COPY --from=build /app .
EXPOSE 80
ENTRYPOINT [""dotnet"", ""aspnetapp.dll""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8000:80 ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
curl http://localhost:8000
```",432
ASP.NET Core,Push It,"ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
ASP.NET Core on Section
What You'll Build
Step by Step
Prerequisites
Create the ASP.NET Core App
Push It
See What You've Built
ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-aspnetcore-app:prod` with port 8000.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8000:
```bash
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://[::]:80
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Production
info: Microsoft.Hosting.Lifetime[0]
      Content root path: /app
```

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",415
ASP.NET Core,See What You've Built,"ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
ASP.NET Core on Section
What You'll Build
Step by Step
Prerequisites
Create the ASP.NET Core App
Push It
See What You've Built
ASP.NET Core
Learn to deploy a ASP.NET Core app at the edge
See the sample app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",100
Ruby on Rails,Ruby on Rails on Section,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Learn how to run a <a href=""https://rubyonrails.org/"">Ruby on Rails app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",201
Ruby on Rails,What You'll Build,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://dry-dew-3573.section.app/"">https://dry-dew-3573.section.app/</a></strong> to see what you'll be building.</p>",172
Ruby on Rails,Step by Step,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge

Following are step-by-step instructions to deploy a Ruby on Rails application to the edge on Section. We'll Dockerize it, and deploy it on Section.",149
Ruby on Rails,Prerequisites,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image, and [Ruby](https://www.ruby-lang.org/en/documentation/installation/), [Rails](https://guides.rubyonrails.org/getting_started.html#creating-a-new-rails-project-installing-rails-installing-rails), and [SQLite](https://www.sqlite.org/download.html) installed for testing locally.",210
Ruby on Rails,Create the Ruby on Rails App,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Create a new directory for your app.

```bash
mkdir my-rubyonrails-app
cd my-rubyonrails-app
```

Initialize the Ruby on Rails application with the following commands:

```bash
rails new hello_world
cd hello_world
```

Run the Ruby on Rails application locally with the following command:

```bash",186
Ruby on Rails,On Windows,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
ruby bin/rails server",123
Ruby on Rails,On Linux/Mac,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
bin/rails server
```

Now navigate to `http://localhost:3000` in your browser to see the Ruby on Rails app running.

:::note
After deploying on Section and using your own domain name(s) you will need to edit the `config.hosts` setting in `my_rubyonrails_app/hello_world/config/environments/development.rb` to be your domain name(s). For example, if your domain name is `example.com` (or including subdomains such as `www.example.com`), you would change it to the following line:
```ruby title=""my_rubyonrails_app/hello_world/config/environments/development.rb""
  # Allow hosts
  config.hosts << /[a-z0-9-.]+\.example\.com/
```
:::",282
Ruby on Rails,Dockerize It,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your `hello_world` directory with the following content.

```dockerfile title=""Dockerfile""",160
Ruby on Rails,Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby),"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
FROM ruby:latest

WORKDIR /hello_world
COPY . /hello_world",134
Ruby on Rails,Run bundle install to install the Ruby dependencies.,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
RUN bundle install",121
Ruby on Rails,Install Yarn.,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
RUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -
RUN echo ""deb https://dl.yarnpkg.com/debian/ stable main"" | tee /etc/apt/sources.list.d/yarn.list
RUN apt-get update && apt-get install -y yarn",181
Ruby on Rails,Run yarn install to install JavaScript dependencies.,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
RUN yarn install --check-files",124
Ruby on Rails,Export port 3000,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
EXPOSE 3000

CMD [""rails"", ""server"", ""-b"", ""0.0.0.0""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
curl http://localhost:3000
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",297
Ruby on Rails,Deploy It,"Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Ruby on Rails on Section
What You'll Build
Step by Step
Prerequisites
Create the Ruby on Rails App
On Windows
On Linux/Mac
Dockerize It
Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
Run bundle install to install the Ruby dependencies.
Install Yarn.
Run yarn install to install JavaScript dependencies.
Export port 3000
Deploy It
Ruby on Rails
Learn to deploy a Ruby on Rails app at the edge
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod` with port 3000.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000:
```cmd
=> Booting Puma
=> Rails 7.0.4 application starting in development
=> Run `bin/rails server --help` for more startup options
Puma starting in single mode...
* Puma version: 5.6.5 (ruby 3.1.3-p185) (""Birdie's Version"")
*  Min threads: 5
*  Max threads: 5
*  Environment: development
*          PID: 1
* Listening on http://0.0.0.0:3000
```

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the sample app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",476
Next.js,Next.js App on Section,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge
Learn how to run a default <a href=""https://nextjs.org/"">Next.js app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",162
Next.js,What You'll Build,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://crimson-resonance-8094.section.app/"">https://crimson-resonance-8094.section.app/</a></strong> to see what you'll be building.</p>

---",139
Next.js,Option 1 - Copy Our GitHub Repo,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge
![workflow status](https://github.com/section/nextjs-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/nextjs-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `pages/index.tsx` and watch your changes go live.

---",291
Next.js,Option 2 - Step by Step,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge",79
Next.js,Prerequisites,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge
- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- (optional) An existing Next.js app
- (optional) kubectl",115
Next.js,Steps,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge
1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section",101
Next.js,Creating your Dockerfile,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge
We're assuming you are either using the Section tutorial example, you have your own Next.js app that compiles successfully, or a newly created Next.js app eg:
```sh
npx create-next-app@latest
```

Create a `Dockerfile` in the root folder of your app. It should sit alongside `package.json`, eg the following

```
.
├── pages
│   ├── index.tsx
│   └── ...
├── public
│   └── ...
├── styles
│   └── ...
├── Dockerfile
├── next.config.js
└── package.json
```

The [Dockerfile](https://github.com/section/nextjs-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as dependencies
WORKDIR /app
COPY package.json yarn.lock* package-lock.json* ./
RUN yarn install --frozen-lockfile

FROM node:alpine as builder
WORKDIR /app
COPY . .
COPY --from=dependencies /app/node_modules ./node_modules
RUN yarn build

FROM node:alpine as runner
WORKDIR /app
ENV NODE_ENV production
# If you are using a custom next.config.js file, uncomment this line.
# COPY --from=builder /app/next.config.js ./
COPY --from=builder /app/public ./public
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json

EXPOSE 3000
CMD [""yarn"", ""start""]

```

This script would create a clean build of your Next.js app, and host it on port 3000 when the container is run.",464
Next.js,Building the container image,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge

Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh",100
Next.js,Replace these example values,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge
USER=section
IMAGENAME=next
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```",260
Next.js,Deploy to Section,"Next.js
Learn to deploy a Next.js app at the edge
Next.js App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
Next.js
Learn to deploy a Next.js app at the edge

Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before (eg: `ghcr.io/section/nextdemo:0.0.1`), and specify port 3000.

That's it! Navigate to your project URL and you'll see your Next.js page live, similar to our <a href=""https://crimson-resonance-8094.section.app/"">example</a>.

![Next.js demo splash page](/img/docs/nextjs-tutorial.png)",195
Rust,Rust on Section,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
Learn how to run a default <a href=""https://www.rust-lang.org"">Rust app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",170
Rust,What You'll Build,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://white-flower-3215.section.app"">https://white-flower-3215.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",200
Rust,Option 1 - Copy Our GitHub Repo,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
![workflow status](https://github.com/section/rust-tutorial/actions/workflows/workflows.yaml/badge.svg)

1. Make a new repo from our template: in your browser visit https://github.com/section/rust-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the message in `src/main.rs` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",329
Rust,Option 2 - Step by Step,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge

Following are step-by-step instructions to deploy a Rust app to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",121
Rust,Prerequisites,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
* You need [Docker](https://docs.docker.com/get-docker) and [Rust](https://www.rust-lang.org/tools/install) installed so that you can build a Docker image.",127
Rust,Create the Rust App,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
Create a Rust app via the Rust `cargo` command:

```bash
cargo new rust-tutorial
```

After the Rust app has been created, we need to set up the web server. We'll be using the [Actix Web](https://actix.rs) framework to handle this. Add this dependency to your Rust app in the `Cargo.toml` file:

```rust title=""Cargo.toml""
[package]
name = ""rust-tutorial""
version = ""0.1.0""
edition = ""2021""

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

// highlight-start
[dependencies]
actix-web = ""4""
// highlight-end
```

Next, update the `src/main.rs` file to the following:

```rust title=""src/main.rs""
use actix_web::{get, App, HttpResponse, HttpServer, Responder};",277
Rust,"[get(""/"")]","Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
async fn hello() -> impl Responder {
    HttpResponse::Ok().body(""Hello World from Rust on Section!"")
}",110
Rust,[actix_web::main],"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
async fn main() -> std::io::Result<()> {
    HttpServer::new(|| {
        App::new()
            .service(hello)
    })
    .bind(""0.0.0.0:8080"")?
    .run()
    .await
}
```

Lastly, we'll want to build and run the Rust app by running the following:

```bash
cd rust-tutorial

cargo build --release

cargo run --release
```

Test it by running `curl http://localhost:8080` in your terminal or by visiting `http://localhost:8080` in your browser. You should get a ""Hello World from Rust on Section!"" message.",227
Rust,Dockerize It,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
Let's build the container image that we'll deploy to Section. First, make a `Dockerfile` in your directory with the following content:

```dockerfile title=""Dockerfile""
FROM rust:latest as build

RUN cargo new --bin rust-tutorial
WORKDIR /rust-tutorial

COPY ./Cargo.lock ./Cargo.lock
COPY ./Cargo.toml ./Cargo.toml

RUN cargo build --release
RUN rm src/*.rs

COPY ./src ./src

RUN rm ./target/release/deps/rust_tutorial*
RUN cargo build --release

FROM debian:buster-slim

COPY --from=build /rust-tutorial/target/release/rust-tutorial .

CMD [""./rust-tutorial""]
```

Create a `.dockerignore` file from the `.gitignore` file:

```bash
cp .gitignore .dockerignore
```

Build and tag the Docker image:

```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`.",365
Rust,Deploy It,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
Next, create a Section deployment for the Rust app with a `rust-deployment.yaml` file, substituting `YOUR_GITHUB_USERNAME` and the environment variables accordingly. This will direct Section to distribute the container you've pushed to GitHub Packages.

```yaml title=""rust-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rust
  labels:
    app: rust
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rust
  template:
    metadata:
      labels:
        app: rust
    spec:
      containers:
      - name: rust
        image: ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "".1""
            memory: "".1Gi""
          limits:
            cpu: "".1""
            memory: "".1Gi""
        ports:
        - containerPort: 8080
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f rust-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

### Expose It
Expose it on the internet, mapping the container's port `8080`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  name: ingress-upstream
  labels:
    app: ingress-upstream
spec:
  selector:
    app: rust
  ports:
    - name: 80-to-8080
      protocol: TCP
      port: 80
      targetPort: 8080
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",607
Rust,See What You've Built,"Rust
Learn to deploy a Rust app at the edge
Rust on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Rust App
[get(""/"")]
[actix_web::main]
Dockerize It
Deploy It
See What You've Built
Rust
Learn to deploy a Rust app at the edge
See the Rust app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",120
Deno,Deno on Section,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
Learn how to run a ""Hello World"" <a href=""https://deno.land/"">Deno app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",199
Deno,What You'll Build,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://old-glade-8722.section.app/"">https://old-glade-8722.section.app/</a></strong> to see what you'll be building.</p>",168
Deno,Step by Step,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge

Following are step-by-step instructions to deploy a Deno ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",154
Deno,Prerequisites,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
* You need [Docker](https://docs.docker.com/get-docker) and [Deno](https://deno.land/manual@v1.28.3/getting_started/installation) installed so that you can build a docker image.",164
Deno,Create the Deno App,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
Create a new directory for your app.

```bash
mkdir my-deno-app
cd my-deno-app
```

Create `main.ts` with the following code.

```typescript title=""main.ts""
import { serve } from ""https://deno.land/std@0.167.0/http/server.ts"";
serve((_req) => new Response(""Hello World from Deno on Section!""), { port: 1993 });
```

Test it using `run --allow-net main.ts` and visit it using `curl http://localhost:1993`. You'll get the ""Hello World from Deno on Section!"" response.",243
Deno,Dockerize It,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM denoland/deno:latest",159
Deno,The port that your application listens to.,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
EXPOSE 1993

WORKDIR /my-deno-app",127
Deno,Prefer not to run as root.,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
USER deno",117
Deno,These steps will be re-run upon each file change in your working directory:,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
ADD . .",117
Deno,Compile the main app so that it doesn't need to be compiled each startup/entry.,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
RUN deno cache main.ts

CMD [""run"", ""--allow-net"", ""main.ts""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
```

Launch it locally to test it.

```bash
docker run -p 1993:1993 ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
curl http://localhost:1993
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",281
Deno,Deploy It,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 1993 (`Listening on http://localhost:1993/`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",321
Deno,See What You've Built,"Deno
Learn to deploy a Deno app at the edge
Deno on Section
What You'll Build
Step by Step
Prerequisites
Create the Deno App
Dockerize It
The port that your application listens to.
Prefer not to run as root.
These steps will be re-run upon each file change in your working directory:
Compile the main app so that it doesn't need to be compiled each startup/entry.
Deploy It
See What You've Built
Deno
Learn to deploy a Deno app at the edge
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",151
Laravel,Laravel on Section,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
Learn how to run a default <a href=""https://laravel.com/"">Laravel app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",157
Laravel,What You'll Build,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://snowy-thunder-5268.section.app"">https://snowy-thunder-5268.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",191
Laravel,Option 1 - Copy Our GitHub Repo,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
![workflow status](https://github.com/section/laravel-tutorial/actions/workflows/workflows.yaml/badge.svg)

1. Make a new repo from our template: in your browser visit https://github.com/section/laravel-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the HTML text in `resources/views/welcome.blade.php` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",322
Laravel,Option 2 - Step by Step,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge

Following are step-by-step instructions to deploy a default Laravel app to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",111
Laravel,Prerequisites,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
* You need [Docker](https://docs.docker.com/get-docker), PHP, and [Composer](https://getcomposer.org) installed so that you can build a Docker image.",113
Laravel,Create the Laravel App,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
Create a Laravel app via the Composer `create-project` command:

```bash
composer create-project laravel/laravel laravel
```

After the Laravel app has been created, test it locally using the Laravel's Artisan CLI `serve` command:

```bash
cd laravel

php artisan serve
```

Access it by running `curl http://localhost:8000` in your terminal or by visiting `http://localhost:8000` in your browser. You should get the default Laravel welcome page.",179
Laravel,Dockerize It,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
Let's build the container image that we'll deploy to Section. First, make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM lorisleiva/laravel-docker:8.1

COPY . .

RUN composer install

CMD php artisan serve --host=0.0.0.0
```

Create a `.dockerignore` file from the `.gitignore` file:

```bash
cp .gitignore .dockerignore
```

Build and tag the Docker image.

```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`.",278
Laravel,Deploy It,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
Next, create a Section deployment for the Laravel app with a `laravel-deployment.yaml` file, substituting `YOUR_GITHUB_USERNAME` and the environment variables accordingly. This will direct Section to distribute the container you've pushed to GitHub Packages.

```yaml title=""laravel-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: laravel
  labels:
    app: laravel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: laravel
  template:
    metadata:
      labels:
        app: laravel
    spec:
      containers:
      - name: laravel
        image: ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
        imagePullPolicy: Always
        resources:
          requests:
            memory: ""200Mi""
            cpu: ""200m""
          limits:
            memory: ""200Mi""
            cpu: ""200m""
        ports:
        - containerPort: 80
        env:
        - name: APP_NAME
          value: ""Laravel""
        - name: APP_ENV
          value: ""production""
        - name: APP_KEY
          value: ""YOUR_APP_KEY""
        - name: APP_DEBUG
          value: ""false""
        - name: APP_URL
          value: ""https://YOUR.DOMAIN.COM""

        - name: LOG_CHANNEL
          value: ""stack""
        - name: LOG_DEPRECATIONS_CHANNEL
          value: ""null""
        - name: LOG_LEVEL
          value: ""debug""

        - name: BROADCAST_DRIVER
          value: ""log""
        - name: CACHE_DRIVER
          value: ""file""
        - name: FILESYSTEM_DISK
          value: ""local""
        - name: QUEUE_CONNECTION
          value: ""sync""
        - name: SESSION_DRIVER
          value: ""cookie""
        - name: SESSION_LIFETIME
          value: ""120""

        - name: MAIL_MAILER
          value: ""smtp""
        - name: MAIL_HOST
          value: ""mailhog""
        - name: MAIL_PORT
          value: ""1025""
        - name: MAIL_USERNAME
          value: ""null""
        - name: MAIL_PASSWORD
          value: ""null""
        - name: MAIL_ENCRYPTION
          value: ""null""
        - name: MAIL_FROM_ADDRESS
          value: ""hello@example.com""
        - name: MAIL_FROM_NAME
          value: ""Laravel""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f laravel-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::tip
For a production Laravel app, use [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables) as the values for private environment variables.
:::

### Expose It
Expose it on the internet, mapping the container's port `8000`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 8000
    selector:
        app: laravel
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",984
Laravel,See What You've Built,"Laravel
Learn to deploy a Laravel app at the edge
Laravel on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Laravel App
Dockerize It
Deploy It
See What You've Built
Laravel
Learn to deploy a Laravel app at the edge
See the Laravel app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",109
Node Express,Node Express on Section,"Node Express
Learn to deploy a Node Express app at the edge
Node Express on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Node.js App
Push It
Node Express
Learn to deploy a Node Express app at the edge
Learn how to run a ""Hello World"" <a href=""https://expressjs.com/"">Node Express app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",152
Node Express,What You'll Build,"Node Express
Learn to deploy a Node Express app at the edge
Node Express on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Node.js App
Push It
Node Express
Learn to deploy a Node Express app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://floral-smoke-4415.section.app"">https://floral-smoke-4415.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",183
Node Express,Option 1 - Copy Our GitHub Repo,"Node Express
Learn to deploy a Node Express app at the edge
Node Express on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Node.js App
Push It
Node Express
Learn to deploy a Node Express app at the edge
![workflow status](https://github.com/section/node-express/actions/workflows/workflows.yaml/badge.svg)


1. Make a new repo from our template: in your browser visit https://github.com/section/node-express and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the HTML text in `views/pages/index.ejs` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",311
Node Express,Option 2 - Step by Step,"Node Express
Learn to deploy a Node Express app at the edge
Node Express on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Node.js App
Push It
Node Express
Learn to deploy a Node Express app at the edge

Following are step-by-step instructions to deploy a Node.js Express ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",108
Node Express,Prerequisites,"Node Express
Learn to deploy a Node Express app at the edge
Node Express on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Node.js App
Push It
Node Express
Learn to deploy a Node Express app at the edge
* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.",107
Node Express,Create the Node.js App,"Node Express
Learn to deploy a Node Express app at the edge
Node Express on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Node.js App
Push It
Node Express
Learn to deploy a Node Express app at the edge
Create a new directory for your app.

```bash
mkdir node-express
cd node-express
```

Initialize the `package.json` file, which handles metadata for Node.js. Answer `yes` in response.

```bash
npm init
... 
Is this OK? (yes)
```

Now install Express with `npm install express --save`.

And then we add our application code. Create `app.js` with the following code.

```javascript title=""app.js""
const express = require('express')
const app = express()
const port = process.env.PORT || 3000

app.get('/', (req, res) => {
  res.send( ""<h1>Hello World from Node Express on Section!</h1>"" );
})

app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`)
})
```

Test it using `node app.js` and visit it using `curl http://localhost:3000`. You'll get the ""Hello World from Node Express on Section!"" response.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM node:lts as runner
WORKDIR /node-express
ENV NODE_ENV production
ARG COMMIT_ID
ENV COMMIT_ID=${COMMIT_ID}
COPY . .
RUN npm ci --only=production
EXPOSE 3000
CMD [""node"", ""app.js""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
curl http://localhost:3000
```",450
Node Express,Push It,"Node Express
Learn to deploy a Node Express app at the edge
Node Express on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Node.js App
Push It
Node Express
Learn to deploy a Node Express app at the edge
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](/get-started/create-project) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod` with port 80.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Node Express pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000 (`App listening at http://localhost:3000`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",395
Python Django,Python Django App on Section,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
Learn how to run a <a href=""https://www.djangoproject.com/"">Python Django</a> app at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",160
Python Django,What You'll Build,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://muddy-butterfly-8163.section.app/"">https://muddy-butterfly-8163.section.app/</a></strong> to see what you'll be building.</p>",137
Python Django,Option 1 - Copy Our GitHub Repo,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
[![workflow status](https://github.com/section/python-django-tutorial/actions/workflows/workflows.yaml/badge.svg)](https://github.com/section/python-django-tutorial/actions)

Make a new repo from our template: in your browser visit https://github.com/section/python-django-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to the files within `./my_django_app/` and watch your changes go live.",309
Python Django,Option 2 - Step by Step,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge

Following are step-by-step instructions to deploy a Python Django application to the edge on Section. We'll Dockerize it, and deploy it on Section.",109
Python Django,Prerequisites,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image, [Python](https://www.python.org/) installed so you can test it locally (which comes with `pip` the Python package manager since Python 3.4).",139
Python Django,Create the Python Django App,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
Create a new directory for your app.

```bash
mkdir my-django-app
cd my-django-app
```

Initialize the Django application with the following commands:

```bash
pip install django
django-admin startproject my_django_app
```

Run the Django application locally with the following commands:

```bash
python manage.py runserver 8080
```

Now navigate to `http://localhost:8080` in your browser to see the Django app running.

:::note
After deploying on Section and using your own domain name(s) you will need to edit the `ALLOWED_HOSTS` setting in `my_django_app/settings.py` to include your domain name(s). For example, if your domain name is `example.com`, you would add the following line to `my_django_app/settings.py`:
```python
ALLOWED_HOSTS = ['example.com']
```
:::",265
Python Django,Dockerize It,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM python:3.7-alpine
EXPOSE 8080
WORKDIR /my-django-app 
RUN pip install django
COPY . /my-django-app
ENTRYPOINT [""python3""] 
CMD [""manage.py"", ""runserver"", ""0.0.0.0:8080""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",329
Python Django,Deploy It,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",252
Python Django,See What You've Built,"Python Django
Learn to deploy a Python Django app at the edge
Python Django App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Django App
Dockerize It
Deploy It
See What You've Built
Python Django
Learn to deploy a Python Django app at the edge
See the Python Django app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",114
React,React App on Section,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge
Learn how to run a <a href=""https://reactjs.org/"">React app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",155
React,What You'll Build,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://icy-tire-4479.section.app/"">https://icy-tire-4479.section.app/</a></strong> to see what you'll be building.</p>

---",130
React,Option 1 - Copy Our GitHub Repo,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge
![workflow status](https://github.com/section/react-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/react-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `src/app.js` and watch your changes go live.

---",281
React,Option 2 - Step by Step,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge",74
React,Prerequisites,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge
- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- (optional) An existing React app
- (optional) kubectl",109
React,Steps,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge
1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section",96
React,Creating your Dockerfile,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge
We're assuming you are either using the Section tutorial example, you have your own React app that compiles successfully, or a newly created React app eg:
```sh
npx create-react-app $NAME
```

Create a `Dockerfile` in the root folder of your app. It should sit alongside package.json, eg the following

```
.
├── public
│   ├── favicon.ico
│   ├── index.html
│   └── ...
├── src
│   ├── App.js
│   ├── index.js
│   └── ...
├── Dockerfile
├── package-lock.json
└── package.json
```

The [Dockerfile](https://github.com/section/react-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as build
WORKDIR /app
ENV PATH /app/node_modules/.bin:$PATH
COPY package.json ./
COPY package-lock.json ./
RUN npm clean-install
RUN npm install react-scripts -g
COPY . ./
RUN npm run build

# production environment
FROM nginx:alpine
COPY --from=build /app/build /usr/share/nginx/html
EXPOSE 80
CMD [""nginx"", ""-g"", ""daemon off;""]

```

This script would create a clean build of your React app, and host it on port 80 of a lightweight nginx image when it's run.",381
React,Building the container image,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge

Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh",95
React,Replace these example values,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge
USER=section
IMAGENAME=react
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```",255
React,Deploy to Section,"React
Learn to deploy a React app at the edge
React App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Steps
Creating your Dockerfile
Building the container image
Replace these example values
Deploy to Section
React
Learn to deploy a React app at the edge

Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before, and specify port 80.

That's it! Navigate to your project URL and you'll see your React page live, similar to our <a href=""https://icy-tire-4479.section.app/"">example</a>.

![React demo splash page](/img/docs/react-tutorial.png)",164
Dart,Dart App on Section,"Dart
Learn to deploy a Dart app at the edge
Dart App on Section
What You'll Build
Step by Step
Prerequisites
Create the Dart App
Push It
Dart
Learn to deploy a Dart app at the edge
Learn how to run a ""Hello World"" <a href=""https://dart.dev/"">Dart app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",136
Dart,What You'll Build,"Dart
Learn to deploy a Dart app at the edge
Dart App on Section
What You'll Build
Step by Step
Prerequisites
Create the Dart App
Push It
Dart
Learn to deploy a Dart app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://snowy-morning-9265.section.app/"">https://snowy-morning-9265.section.app/</a></strong> to see what you'll be building.</p>",108
Dart,Step by Step,"Dart
Learn to deploy a Dart app at the edge
Dart App on Section
What You'll Build
Step by Step
Prerequisites
Create the Dart App
Push It
Dart
Learn to deploy a Dart app at the edge

Following are step-by-step instructions to deploy a Dart ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",85
Dart,Prerequisites,"Dart
Learn to deploy a Dart app at the edge
Dart App on Section
What You'll Build
Step by Step
Prerequisites
Create the Dart App
Push It
Dart
Learn to deploy a Dart app at the edge
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",78
Dart,Create the Dart App,"Dart
Learn to deploy a Dart app at the edge
Dart App on Section
What You'll Build
Step by Step
Prerequisites
Create the Dart App
Push It
Dart
Learn to deploy a Dart app at the edge
Create a new directory for your app.

```bash
mkdir my-dart-app
cd my-dart-app
```

Create directory `bin` with a new file `server.dart` with the following code.

```dart title=""server.dart""
import 'dart:convert';
import 'dart:io';

import 'package:shelf/shelf.dart';
import 'package:shelf/shelf_io.dart' as shelf_io;
import 'package:shelf_router/shelf_router.dart' as shelf_router;
import 'package:shelf_static/shelf_static.dart' as shelf_static;

Future<void> main() async {
  // If the ""PORT"" environment variable is set, listen to it. Otherwise, 8080.
  // https://cloud.google.com/run/docs/reference/container-contract#port
  final port = int.parse(Platform.environment['PORT'] ?? '8080');

  // See https://pub.dev/documentation/shelf/latest/shelf/Cascade-class.html
  final cascade = Cascade()
      // First, serve files from the 'public' directory
      .add(_staticHandler)
      // If a corresponding file is not found, send requests to a `Router`
      .add(_router);

  // See https://pub.dev/documentation/shelf/latest/shelf_io/serve.html
  final server = await shelf_io.serve(
    // See https://pub.dev/documentation/shelf/latest/shelf/logRequests.html
    logRequests()
        // See https://pub.dev/documentation/shelf/latest/shelf/MiddlewareExtensions/addHandler.html
        .addHandler(cascade.handler),
    InternetAddress.anyIPv4, // Allows external connections
    port,
  );

  print('Serving at http://${server.address.host}:${server.port}');
}

// Serve files from the file system.
final _staticHandler =
    shelf_static.createStaticHandler('public', defaultDocument: 'index.html');

// Router instance to handler requests.
final _router = shelf_router.Router()
  ..get('/helloworld', _helloWorldHandler);

Response _helloWorldHandler(Request request) => Response.ok('Hello World!');
```

Create a directory called `public` in the root directory, and create a new file inside the `public` directory called `index.html` with the following content.

```html title=""index.html""
<html5>
    <head>
        <title>Dart Example on Section</title>
    </head>
    <body>
        <h1>Hello World from Dart on Section!</h1>
    </body>
</html5>
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your root directory with the following content.

```dockerfile title=""Dockerfile""
# Official Dart image: https://hub.docker.com/_/dart
# Specify the Dart SDK base image version using dart:<version> (ex: dart:2.12)
FROM dart:stable AS build

# Resolve app dependencies.
WORKDIR /app
COPY pubspec.* ./
RUN dart pub get

# Copy app source code and AOT compile it.
COPY . .
# Ensure packages are still up-to-date if anything has changed
RUN dart pub get --offline
RUN dart compile exe bin/server.dart -o bin/server

# Build minimal serving image from AOT-compiled `/server` and required system
# libraries and configuration files stored in `/runtime/` from the build stage.
FROM scratch
COPY --from=build /runtime/ /
COPY --from=build /app/bin/server /app/bin/

# Include files in the /public directory to enable static asset handling
COPY --from=build /app/public/ /public

# Start server.
EXPOSE 8080
CMD [""/app/bin/server""]
```

Now create a file called `pubspec.yaml` in the root directory, this is where we will define our dependencies.

```yaml title=""pubspec.yaml""
name: helloworld
publish_to: none

environment:
  sdk: "">=2.12.0 <3.0.0""

dependencies:
  shelf: ^1.2.0
  shelf_router: ^1.0.0
  shelf_static: ^1.0.0

dev_dependencies:
  http: ^0.13.0
  lints: ^2.0.0
  test: ^1.15.0
  test_process: ^2.0.0
```

Finally, create a file called `.dockerignore` in the root directory, this is where we will define files that we don't want to include in our container image.

```text title="".dockerignore""
.dockerignore
Dockerfile
build/
.dart_tool/
.git/
.github/
.gitignore
.packages
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
curl http://localhost:8080
```",1106
Dart,Push It,"Dart
Learn to deploy a Dart app at the edge
Dart App on Section
What You'll Build
Step by Step
Prerequisites
Create the Dart App
Push It
Dart
Learn to deploy a Dart app at the edge
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`Serving at http://0.0.0.0:8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",380
KoaJS,KoaJS on Section,"KoaJS
Learn to deploy a KoaJS app at the edge
KoaJS on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the KoaJS App
Push It
KoaJS
Learn to deploy a KoaJS app at the edge
Learn how to run a ""Hello World"" <a href=""https://koajs.com/"">KoaJS app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",159
KoaJS,What You'll Build,"KoaJS
Learn to deploy a KoaJS app at the edge
KoaJS on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the KoaJS App
Push It
KoaJS
Learn to deploy a KoaJS app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://little-snowflake-1628.section.app"">https://little-snowflake-1628.section.app</a></strong> to see what you'll be building.</p>",128
KoaJS,Option 1 - Copy Our GitHub Repo,"KoaJS
Learn to deploy a KoaJS app at the edge
KoaJS on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the KoaJS App
Push It
KoaJS
Learn to deploy a KoaJS app at the edge
1. Make a new repo from our template: in your browser visit <a href=""https://github.com/section/koajs-tutorial"">https://github.com/section/koajs-tutorial</a> and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the value of `ctx.body` on line 5 of `app.js` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",320
KoaJS,Option 2 - Step by Step,"KoaJS
Learn to deploy a KoaJS app at the edge
KoaJS on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the KoaJS App
Push It
KoaJS
Learn to deploy a KoaJS app at the edge

Following are step-by-step instructions to deploy a KoaJS ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",114
KoaJS,Prerequisites,"KoaJS
Learn to deploy a KoaJS app at the edge
KoaJS on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the KoaJS App
Push It
KoaJS
Learn to deploy a KoaJS app at the edge
* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.",113
KoaJS,Create the KoaJS App,"KoaJS
Learn to deploy a KoaJS app at the edge
KoaJS on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the KoaJS App
Push It
KoaJS
Learn to deploy a KoaJS app at the edge
Create a new directory for your app.

```bash
mkdir node-koajs
cd node-koajs
```

Install the KoaJS application.

```bash
npm i koa
```

And then we add our application code. Create `app.js` with the following code.

```javascript title=""app.js""
const Koa = require('koa');
const app = new Koa();

app.use(async ctx => {
  ctx.body = 'Hello World from KoaJS on Section!';
});

app.listen(3000);
```

Test it using `node app.js` and visit it using `curl http://localhost:3000`. You'll get the ""Hello World from KoaJS on Section!"" response.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM node:lts as runner
WORKDIR /node-koajs
ENV NODE_ENV production
ARG COMMIT_ID
ENV COMMIT_ID=${COMMIT_ID}
COPY . .
RUN npm i koajs
EXPOSE 3000
CMD [""node"", ""app.js""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
curl http://localhost:3000
```",395
KoaJS,Push It,"KoaJS
Learn to deploy a KoaJS app at the edge
KoaJS on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the KoaJS App
Push It
KoaJS
Learn to deploy a KoaJS app at the edge
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-koajs-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000 (`App listening at http://localhost:3000`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",396
Gatsby,Gatsby.js on Section,"Gatsby
Learn to deploy a Gatsby app
Gatsby.js on Section
Step by Step
Prerequisites
Create the Gatsby.js App
Gatsby
Learn to deploy a Gatsby app
Learn how to deploy a <a href=""https://www.gatsbyjs.com/"">Gatsby.js app</a> on Section. At the end of this tutorial, you'll have a Gatsby.js app deployed with the following:

* Publicly accessible URL for your app
* Automatic SSL
* Automatic HTTP/2
* Multi-Region hosting
* Multi-Cloud hosting
* Layer 3 and 4 DDoS protection",131
Gatsby,Step by Step,"Gatsby
Learn to deploy a Gatsby app
Gatsby.js on Section
Step by Step
Prerequisites
Create the Gatsby.js App
Gatsby
Learn to deploy a Gatsby app
Follow the step-by-step instructions to deploy a Gatsby.js ""Hello World"" application on Section. We'll Dockerize it, push it to Dockerhub, and deploy it on Section.",80
Gatsby,Prerequisites,"Gatsby
Learn to deploy a Gatsby app
Gatsby.js on Section
Step by Step
Prerequisites
Create the Gatsby.js App
Gatsby
Learn to deploy a Gatsby app
You'll need the following to complete this tutorial:

* A Docker Hub account
* A Section account
* Node.js installed on your local machine
  * You can install it from <a href=""https://nodejs.org/en/download/"">here</a>
* Gatsby.js installed on your local machine
  * You can install it from <a href=""https://www.gatsbyjs.com/docs/tutorial/part-0/"">here</a>
* Docker installed on your local machine",140
Gatsby,Create the Gatsby.js App,"Gatsby
Learn to deploy a Gatsby app
Gatsby.js on Section
Step by Step
Prerequisites
Create the Gatsby.js App
Gatsby
Learn to deploy a Gatsby app
Create a new directory for your app.

```bash
mkdir gatsby-app
cd gatsby-app
```

Initialise a Gatsby.js project by using the Gatsby CLI.

```bash
gatsby new .
```

Make a simple change to the HTML in `src/pages/index.js` and test it locally.

```bash
gatsby develop
```

### Create a Dockerfile
Create a file called `Dockerfile` in the root of your project with the following contents:

```dockerfile title=""Dockerfile""
FROM node:18

# set the working directory
WORKDIR /app
# copy the repository files to it
COPY . /app

RUN npm install
RUN npm install -g gatsby-cli

RUN gatsby build
EXPOSE 80

CMD gatsby serve --port 80 --host 0.0.0.0
```

Then build the image with your docker daemon.

```bash
docker build -t gatsby-app:v1 .
```

### Push to Docker Hub
Authenticate your local docker with Docker Hub.

```bash
docker login
```

Ensure you have a repository on Docker Hub. If you don't, create one. E.g. `myusername/gatsby-app`

Tag the image and push it to Docker Hub. Replace `myusername` with your Docker Hub username/organization. *Note: We are assuming the repository you've created is public.*

```bash
docker tag gatsby-app:v1 myusername/gatsby-app:latest
docker push myusername/gatsby-app:latest
```

### Deploy to Section
Login to Section and [create a new Section project](https://www.section.io/docs/get-started/create-project/).

Use the following values for the project settings:

* **Plan**: Free
* **Image**: myusername/gatsby-app:latest
* **Port**: 80

Click **Create Project**.

### Test the Deployment
Once the deployment is complete, you can test it by visiting the `section.app` URL provided in the project details. Your traffic will be routed to the closest Point of Presence based on your geographical location.",477
Python Flask,Python Flask App on Section,"Python Flask
Learn to deploy a Python Flask app at the edge
Python Flask App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Flask App
Push It
Python Flask
Learn to deploy a Python Flask app at the edge
Learn how to run a ""Hello World"" <a href=""https://flask.palletsprojects.com/en/2.2.x/"">Python Flask app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",163
Python Flask,What You'll Build,"Python Flask
Learn to deploy a Python Flask app at the edge
Python Flask App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Flask App
Push It
Python Flask
Learn to deploy a Python Flask app at the edge
<p><mark>Short on time?</mark> Visit <strong><a href=""https://twilight-fire-9701.section.app/"">https://twilight-fire-9701.section.app/</a></strong> to see what you'll be building.</p>",122
Python Flask,Option 1 - Copy Our GitHub Repo,"Python Flask
Learn to deploy a Python Flask app at the edge
Python Flask App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Flask App
Push It
Python Flask
Learn to deploy a Python Flask app at the edge
![workflow status](https://github.com/section/python-flask-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/python-flask-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.py` and watch your changes go live.",278
Python Flask,Option 2 - Step by Step,"Python Flask
Learn to deploy a Python Flask app at the edge
Python Flask App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Flask App
Push It
Python Flask
Learn to deploy a Python Flask app at the edge

Following are step-by-step instructions to deploy a Python Flask ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",102
Python Flask,Prerequisites,"Python Flask
Learn to deploy a Python Flask app at the edge
Python Flask App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Flask App
Push It
Python Flask
Learn to deploy a Python Flask app at the edge
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",94
Python Flask,Create the Python Flask App,"Python Flask
Learn to deploy a Python Flask app at the edge
Python Flask App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Flask App
Push It
Python Flask
Learn to deploy a Python Flask app at the edge
Create a new directory for your app.

```bash
mkdir my-flask-app
cd my-flask-app
```

Create `helloworld.py` with the following code.

```python title=""helloworld.py""
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello World from Python Flask on Section!'

if __name__ == ""__main__"":
    app.run(debug=True, host='0.0.0.0', port=8080)
```

Create `requirements.txt` with the following content to specify the version of Flask you are using.

```text title=""requirements.txt""
Flask==2.2.2
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM python:3-alpine
WORKDIR /my-flask-app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8080
CMD [""python"", ""helloworld.py""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod
curl http://localhost:8080
```",374
Python Flask,Push It,"Python Flask
Learn to deploy a Python Flask app at the edge
Python Flask App on Section
What You'll Build
Option 1 - Copy Our GitHub Repo
Option 2 - Step by Step
Prerequisites
Create the Python Flask App
Push It
Python Flask
Learn to deploy a Python Flask app at the edge
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`App listening at http://localhost:8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",389
PyTorch,Distributed Machine Learning Predictions Using PyTorch,"PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Distributed Machine Learning Predictions Using PyTorch
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
Start Making Predictions at the Edge
PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch

Achieve faster ML model serving for your users at the edge by running a distributed ML model server. This tutorial will use  Section to deploy [PyTorch TorchServe](https://github.com/pytorch/serve) with an example pretrained model.

The PyTorch container we will use is [available on DockerHub](https://hub.docker.com/r/pytorch/torchserve). This tutorial was inspired by a [GCP example](https://cloud.google.com/ai-platform/prediction/docs/getting-started-pytorch-container).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",264
PyTorch,Prerequisites,"PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Distributed Machine Learning Predictions Using PyTorch
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
Start Making Predictions at the Edge
PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",137
PyTorch,Pull Down the Pretrained Model,"PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Distributed Machine Learning Predictions Using PyTorch
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
Start Making Predictions at the Edge
PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Pull down the PyTorch example models from GitHub so that we can build one of them into the container image. We'll be using one that recognizes digits from PNG images.

```bash
mkdir my-pytorch-example
cd my-pytorch-example
git clone https://github.com/pytorch/serve.git \
  --branch=v0.3.0 \
  --depth=1
```

## Create a Dockerfile for Your Container Image
The container image you'll build relies upon [PyTorch Serve](https://hub.docker.com/r/pytorch/torchserve) on Docker Hub. We'll be using the MNIST model, which will allow us to do image classification of digits from PNG images.

```dockerfile title=""Dockerfile""
FROM pytorch/torchserve:0.3.0-cpu

COPY serve/examples/image_classifier/mnist/mnist.py \
    serve/examples/image_classifier/mnist/mnist_cnn.pt \
    serve/examples/image_classifier/mnist/mnist_handler.py \
    /home/model-server/

USER root
RUN printf ""\nservice_envelope=json"" >> /home/model-server/config.properties
USER model-server

RUN torch-model-archiver \
  --model-name=mnist \
  --version=1.0 \
  --model-file=/home/model-server/mnist.py \
  --serialized-file=/home/model-server/mnist_cnn.pt \
  --handler=/home/model-server/mnist_handler.py \
  --export-path=/home/model-server/model-store

CMD [""torchserve"", \
     ""--start"", \
     ""--ts-config=/home/model-server/config.properties"", \
     ""--models"", \
     ""mnist=mnist.mar""]
```",439
PyTorch,Build and Publish the Image,"PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Distributed Machine Learning Predictions Using PyTorch
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
Start Making Predictions at the Edge
PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Build the docker image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.
```bash
docker build -t my-pytorch-image .
docker tag my-pytorch-image YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
docker push YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
```

## Create a Kubernetes Deployment for PyTorch
Next, create the deployment for PyTorch as pytorch-deployment.yaml substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""pytorch-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pytorch
  name: pytorch
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pytorch
  template:
    metadata:
      labels:
        app: pytorch
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
        imagePullPolicy: Always
        name: pytorch
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f pytorch-upstream.yaml`.",410
PyTorch,Expose the Service on the Internet,"PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Distributed Machine Learning Predictions Using PyTorch
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
Start Making Predictions at the Edge
PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
We want to expose the PyTorch service on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: pytorch
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

## Create a File with an Image
You'll send an image of a '3' to the prediction engine to see if it can figure it out. Place the following JSON into a file called `png-image-of-a-3.json`.

```json title=""png-image-of-a-3.json""
{
  ""instances"": [
    {
      ""data"": {
        ""b64"": ""iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAv0lEQVR4nGNgGKSA03faPyDwxibHu/7vvwfnzz/5tsgRU3LW33uukgwMCi1PdmBKOr7dAmEsuiiIKSssDpX8q4fbYYv/4ZZk3YTNWCg48HcGTrnOf39dcUgpzPv97+/b56LY5PKBIfTi+bt//7ptMSV7Py6NYWCQirn17zymJK8R1PRVd4RxuoqhG6erCEmevoBbbsqvUkxBXWMQabzk+wksOhZ9vHDh4oWPf1d6YZFUuff377+/9zp5cNtIHQAAtP5OgKw1m4AAAAAASUVORK5CYII=""
      }
    }
  ]
}
```

The image looks like this:

![Image of 3](/img/docs/3.png)",641
PyTorch,Start Making Predictions at the Edge,"PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Distributed Machine Learning Predictions Using PyTorch
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
Start Making Predictions at the Edge
PyTorch
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch
Exercise the ML model server substituting `YOUR_ENVIRONMENT_HOSTNAME` accordingly.
```bash
curl -X POST \
  -H ""Content-Type: application/json; charset=utf-8"" \
  -d @png-image-of-a-3.json \
  YOUR_ENVIRONMENT_HOSTNAME/predictions/mnist
```

The result you'll get:
```
{ ""predictions"": [3] }
```",177
TensorFlow,Distributed Machine Learning Predictions Using Tensorflow,"TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
Distributed Machine Learning Predictions Using Tensorflow
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow

Achieve faster ML model serving for your users at the edge by running a distributed ML model server. This tutorial will use Section to deploy [TensorFlow Serving](https://github.com/tensorflow/serving) with an example pretrained model.

The TensorFlow container we will use is [available on DockerHub](https://hub.docker.com/r/tensorflow/serving).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",215
TensorFlow,Prerequisites,"TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
Distributed Machine Learning Predictions Using Tensorflow
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",124
TensorFlow,Pull Down the Pretrained Model,"TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
Distributed Machine Learning Predictions Using Tensorflow
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
Pull down the TensorFlow example models from GitHub so that we can build one of them into the container image.

```bash
mkdir my-tensorflow-example
cd my-tensorflow-example
git clone https://github.com/tensorflow/serving
```

## Create a Dockerfile for Your Container Image
The container image you'll build relies upon [TensorFlow Serving](https://hub.docker.com/r/tensorflow/serving) on Docker Hub. We'll just use one of those models you downloaded, called `saved_model_half_plus_two_cpu`.  It halves a value, then adds 2.
```dockerfile title=""Dockerfile""
FROM tensorflow/serving

ADD serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu /models/half_plus_two

ENTRYPOINT [""/usr/bin/tf_serving_entrypoint.sh""]
```",261
TensorFlow,Build and Publish the Image,"TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
Distributed Machine Learning Predictions Using Tensorflow
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
Build the image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.
```bash
docker build -t my-tensorflow-image .
docker tag my-tensorflow-image YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
docker push YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
```

## Create a Kubernetes Deployment for TensorFlow
Next, create the deployment for TensorFlow as tensorflow-deployment.yaml substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""tensorflow-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tensorflow
  name: tensorflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tensorflow
  template:
    metadata:
      labels:
        app: tensorflow
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
        imagePullPolicy: Always
        name: tensorflow
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: MODEL_NAME
          value: half_plus_two
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f tensorflow-upstream.yaml`.",404
TensorFlow,Expose the Service on the Internet,"TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
Distributed Machine Learning Predictions Using Tensorflow
Prerequisites
Pull Down the Pretrained Model
Build and Publish the Image
Expose the Service on the Internet
TensorFlow
Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow
We want to expose the TensorFlow service on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8501
  selector:
    app: tensorflow
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

## Start Making Predictions at the Edge
Exercise the ML prediction service substituting `YOUR_ENVIRONMENT_HOSTNAME` accordingly.
```bash
curl -d '{""instances"": [1.0, 2.0, 5.0]}' \
  -X POST http://YOUR_ENVIRONMENT_HOSTNAME/v1/models/half_plus_two:predict
```

The result you'll get:
```
{
    ""predictions"": [2.5, 3.0, 4.5]
}
```",408
Database Caching with PolyScale,Reduce Database Latency with Global Caching,"Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Reduce Database Latency with Global Caching
Prerequisites
The Basic Idea
Deployment
Move the Workload Around
Extra Credit: Metrics in Grafana
Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
You've decided to distribute your app closer to your users for increased performance. But if you also leverage a central relational database, how do you prevent that from being a performance bottleneck? In this tutorial we use [PolyScale.ai](https://polyscale.ai/) to address this problem. PolyScale provides a global database cache as a service that automatically handles cache lifetimes and invalidation. It is wire-protocol compatible with multiple databases (MySQL, Postgres, etc.). This enables you to scale your single, fixed-location database without requiring changes in your code, operating read-replicas, or incurring the cost of nodes for a distributed database.

What we've built is a simple performance testing app that executes a query:
- from every Section location where your project is deployed,
- into the nearest [PolyScale.ai](https://polyscale.ai) cache location,
- and then in-turn into your origin database for any cache misses.

The query executes every 60 seconds and emits a log entry with latency measurements. For those who want to go further, we provide an Extra Credit section that shows you how to scrape a metrics endpoint and send data to Grafana Cloud for charting.

There is no need to build the Docker image, we provide one for you on https://ghcr.io/section/polyscale-metrics. The deployment yaml refers to that one, although we've provided a Dockerfile if you'd like to customize it.

You just need to substitute your secrets and connection strings and deploy the yamls to your Section Project.

Checkout our [Mastodon hosting tutorial](/tutorials/mastodon) for an example use case!

![Section and Polyscale Deployment](/img/docs/ps4.png)",421
Database Caching with PolyScale,Prerequisites,"Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Reduce Database Latency with Global Caching
Prerequisites
The Basic Idea
Deployment
Move the Workload Around
Extra Credit: Metrics in Grafana
Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
* You'll need a small example SQL database from PolyScale's [supported types](https://docs.polyscale.ai/supported-platforms/). The actual contents do not matter. A simple Supabase that you create yourself will suffice, or you can use a sample read-only database URL that we provide so that you don't have to create your own.
* Create an account and global cache for your database at [PolyScale.ai](https://app.polyscale.ai/signup).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",231
Database Caching with PolyScale,The Basic Idea,"Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Reduce Database Latency with Global Caching
Prerequisites
The Basic Idea
Deployment
Move the Workload Around
Extra Credit: Metrics in Grafana
Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Clone our repo to get started: [https://github.com/section/polyscale-metrics](https://github.com/section/polyscale-metrics).

The meat of the app is this small snippet located in `dbQuery()` in `polyscale_metrics.go`:

```go title=""polyscale-metrics.go""
...
	// Time the query
	start := time.Now()
	rows, err := conn.Query(context.Background(), query)
	duration := time.Since(start)
```
The above query is called twice, once against the cache and once against the origin. It repeats this process forever with a sleep in between, logging the results to stdout. That's all there is to it.

## Setup Your Databases
Here is a snipped of the deployment yaml showing the container environment variables that you'll need pertaining to your database.
```yaml title=""polyscale-metrics-deployment.yaml""
...
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName        
          - name: CACHE_DATABASE_URL
            value: YOUR_CACHE_DATABASE_URL
          - name: ORIGIN_DATABASE_URL
            value: YOUR_ORIGIN_DATABASE_URL
          - name: QUERY
            value: YOUR_QUERY
```

If you don't want to create your own database, use ours:

```
ORIGIN_DATABASE_URL is postgresql://read_only_user:8BiusLd6Z89kjVgS@database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com:5432/postgres
CACHE_DATABASE_URL is whatever PolyScale gives you
QUERY is ""SELECT * from pets.pet_names limit 1;""
```

And then from that database create a PolyScale cache by clicking on the New Cache button in the top-right corner. If you use our database, tt is a Postgres database, host is `database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com`, and port is 5432.",489
Database Caching with PolyScale,Deployment,"Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Reduce Database Latency with Global Caching
Prerequisites
The Basic Idea
Deployment
Move the Workload Around
Extra Credit: Metrics in Grafana
Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Replace the three strings accordingly and apply your modified deployment resource to your Section Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f polyscale-metrics-deployment.yaml`.

See the pods running with `kubectl get pods -o wide`. 

```
$ kubectl get pods -o wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES
polyscale-metrics-86449c4f7d-qtrwd   1/1     Running   0          12m   10.244.70.138   sof-zzbd7   <none>           <none>
polyscale-metrics-86449c4f7d-sgqs9   1/1     Running   0          12m   10.244.48.80    rio-kz6s3   <none>           <none>
```

And pick one of the pod names to see its logs using `kubectl logs POD`.

```
$ kubectl logs polyscale-metrics-86449c4f7d-sgqs9
Node lmn-rio-k1-shared-ingress12 listening to /metrics on port :2112 interval 300 s query select * from foodsales limit 1;
nodename lmn-rio-k1-shared-ingress12 cache 618ms origin 634ms 
nodename lmn-rio-k1-shared-ingress12 cache 9ms origin 316ms 
nodename lmn-rio-k1-shared-ingress12 cache 9ms origin 316ms 
```

Explore the other pods as well to see the speed of the cache from other locations.

That's it!",446
Database Caching with PolyScale,Move the Workload Around,"Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Reduce Database Latency with Global Caching
Prerequisites
The Basic Idea
Deployment
Move the Workload Around
Extra Credit: Metrics in Grafana
Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Use the [location optimizer](/guides/projects/set-edge-locations.md) to cause your project to move to other locations so that you can see how Section and PolyScale work together.",112
Database Caching with PolyScale,Extra Credit: Metrics in Grafana,"Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Reduce Database Latency with Global Caching
Prerequisites
The Basic Idea
Deployment
Move the Workload Around
Extra Credit: Metrics in Grafana
Database Caching with PolyScale
Learn how to reduce your database latencies using serverless global caching
Our GoLang app exposes a metrics endpoint so that you can scrape p50, p90, and p95 query latencies into a 3rd party metrics system such as Grafana Cloud. Read more in our [tutorial](/guides/monitor/exporting-telemetry/grafana-section/) about scraping metrics into Grafana.

Replace items in the `remote_write` section of the ConfigMap:

```yaml title=""grafana-app-agent-configmap.yaml""
...
    remote_write:
    - url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
      basic_auth:
        username: GRAFANA_METRICS_INSTANCE_ID
        password: GRAFANA_API_KEY
```

And apply with `kubectl apply -f grafana-app-scrape-configmap.yaml`.

In the Deployment resource no substitutions required. So just apply with `kubectl apply -f grafana-app-agent-deployment.yaml`.

And finally, in order for the Grafana agent to contact the polyscale-metrics-service pod, you'll need a Kubernetes Service resource. We've provided that for you, so just apply with `kubectl apply -f polyscale-metrics-service.yaml`. This exposes `http://polyscale-metrics-service:80` within your Section project so that the Grafana agent can access the metrics endpoint.

Then go to the Explore menu in Grafana Cloud to start charting your metrics!",351
Distributed Data on Section,Native Data on Section,"Distributed Data on Section
Learn how to support all your data needs on Section
Native Data on Section
Ephemeral Volumes
Persistent Volumes
Streaming Database Backups
Re-populating a Database as Section Relocates It
Synchronized Databases
Serverless Data Integrations
Distributed Data on Section
Learn how to support all your data needs on Section",76
Distributed Data on Section,Ephemeral Volumes,"Distributed Data on Section
Learn how to support all your data needs on Section
Native Data on Section
Ephemeral Volumes
Persistent Volumes
Streaming Database Backups
Re-populating a Database as Section Relocates It
Synchronized Databases
Serverless Data Integrations
Distributed Data on Section
Learn how to support all your data needs on Section
Whenever your container opens a file for writing, you are using Section's ephemeral volumes. This storage goes away when the pod terminates.",104
Distributed Data on Section,Persistent Volumes,"Distributed Data on Section
Learn how to support all your data needs on Section
Native Data on Section
Ephemeral Volumes
Persistent Volumes
Streaming Database Backups
Re-populating a Database as Section Relocates It
Synchronized Databases
Serverless Data Integrations
Distributed Data on Section
Learn how to support all your data needs on Section
By using a [Persistent Volume Claim](/explanations/persistent-volumes.md) you can share a filesystem across multiple pods. Using this technology you can share virtually any data technology between pods, such as relational databases, document stores, object stores, key-value stores, and message queues. Sharing across pods can occur in the following instances:
* horizontal scaling of a pod, so that the multiple replicas have access to shared data
* different pods of a microservice application, giving those pods a common source of truth for whatever data they might need
* data that needs to survive a pod that crashes and restarts

Check out our tutorial to see how this works for a [Postgres deployment](/tutorials/data/postgres-on-pvc).",228
Distributed Data on Section,Streaming Database Backups,"Distributed Data on Section
Learn how to support all your data needs on Section
Native Data on Section
Ephemeral Volumes
Persistent Volumes
Streaming Database Backups
Re-populating a Database as Section Relocates It
Synchronized Databases
Serverless Data Integrations
Distributed Data on Section
Learn how to support all your data needs on Section
Depending upon your circumstances, when you house your database at an edge location then you may wish to stream a backup to a more permanent location such as AWS S3, Azure Blob Storage, Wasabi, Exoscale, etc. [Litestream.io](https://litestream.io/) supports this idea for SQLite.",142
Distributed Data on Section,Re-populating a Database as Section Relocates It,"Distributed Data on Section
Learn how to support all your data needs on Section
Native Data on Section
Ephemeral Volumes
Persistent Volumes
Streaming Database Backups
Re-populating a Database as Section Relocates It
Synchronized Databases
Serverless Data Integrations
Distributed Data on Section
Learn how to support all your data needs on Section
Streaming backup technology, such as Litestream.io mentioned previously, can also be used to repopulate a database as Section moves your project from one region to another.",111
Distributed Data on Section,Synchronized Databases,"Distributed Data on Section
Learn how to support all your data needs on Section
Native Data on Section
Ephemeral Volumes
Persistent Volumes
Streaming Database Backups
Re-populating a Database as Section Relocates It
Synchronized Databases
Serverless Data Integrations
Distributed Data on Section
Learn how to support all your data needs on Section
Strategies for sharing data between locations include configuring replication to occur between persistent volumes residing in different locations. We have a guide that explains how you can set this up (coming). You might choose to use a static location configuration for your project and then replicate data between them in order to provide a globally-distributed persistent data.",141
Distributed Data on Section,Serverless Data Integrations,"Distributed Data on Section
Learn how to support all your data needs on Section
Native Data on Section
Ephemeral Volumes
Persistent Volumes
Streaming Database Backups
Re-populating a Database as Section Relocates It
Synchronized Databases
Serverless Data Integrations
Distributed Data on Section
Learn how to support all your data needs on Section
Below are a few managed serverless data offerings that we are aware of at Section. We've written tutorials for a few. Almost all offer some kind of multi-region replication, which is useful for disaster recovery, high-availability, and low-latency reads for better performance for geographically distributed end users. Those that support multi-region writes are listed separately. The data landscape has [lots of options](https://db-engines.com/) to choose from, many of them [free for limited usage](https://free-for.dev/#/?id=dbaas).

<div class=""table--white-space-normal"">

| Data Type | Single-Region Write | Multi-Region Write |
|-----|--------------------|---------|
|Relational (SQL)|[Supabase](https://supabase.com), [Neon](https://neon.tech), [Aiven for PostgreSQL](https://aiven.io/postgresql), [bit.io](https://bit.io/) (Postgres compatible)|[Polyscale.ai](/tutorials/data/polyscale-caching.md), [CockroachDB Serverless](https://cockroachlabs.cloud) (multi-region write is in preview)|
|Document (NoSQL)|[MongoDB Atlas](https://www.mongodb.com/atlas)|[FaunaDB](https://fauna.com/), [DataStax Astra DB](https://www.datastax.com/products/datastax-astra) (Cassandra-based), [HarperDB](https://harperdb.io/), [Couchbase Capella](https://www.couchbase.com/products/capella)|
|Key-value|[ZippyDB](https://www.zippydb.com/), [Aiven Redis](https://aiven.io/redis), [ScaleGrid Redis](https://scalegrid.io/redis)|[Upstash Redis](https://upstash.com/)|
|Object storage|[Wasabi](https://wasabi.com), [Backblaze](https://backblaze.com)|[Synadia Jetstream Object Store](https://synadia.com/ngs) (NATS.io-based)|
|Message Queue|[Upstash Kafka](https://upstash.com/kafka)|[Synadia](https://synadia.com/ngs) (NATS.io-based)|
||                    ||

</div>",587
Persistent Volumes,Create and Mount a Persistent Volume Claim,"Persistent Volumes
Learn how to create and mount a persistent volume claim for use by your pods
Create and Mount a Persistent Volume Claim
Make the Claim
Test It
Persistent Volumes
Learn how to create and mount a persistent volume claim for use by your pods

Cloud applications often require a place to persist data. Containers inside pods have ephemeral filesystems that are lost when a pod restarts or terminates. [Persistent volumes](/explanations/persistent-volumes.md) solve this problem by allowing data to persist beyond a pod's lifetime. This tutorial explains how to create a Persistent Volume Claim, mount it in a pod, and then demonstrate that it works across two replicas of a pod.",142
Persistent Volumes,Make the Claim,"Persistent Volumes
Learn how to create and mount a persistent volume claim for use by your pods
Create and Mount a Persistent Volume Claim
Make the Claim
Test It
Persistent Volumes
Learn how to create and mount a persistent volume claim for use by your pods
First, here is the yaml the makes a claim to a volume. It will be dynamically created once it is mounted.
* The [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) supported by Section is `nfs-client`.
* The size of my volume below is set to 30Mi.
* The [access mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) for a volume intended to be shared between many pods should be set to `ReadWriteMany`.
* The [volume mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode) is set to `Filesystem`, which mounts it as a directory in the pod's filesystem.

```yaml title=""pvc-claim.yaml""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nginx
  namespace: default
spec:
  storageClassName: nfs-client
  resources:
    requests:
      storage: 30Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany # access by many pods
```

Apply and check the PersistentVolumeClaim resource.

```
$ kubectl apply -f pvc-claim.yaml
persistentvolumeclaim/pvc-nginx created
$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-nginx   Pending                                      nfs-client     14m
$ 
```

## Mount It
Next, we'll mount the claim in a simple NGINX deployment. This will cause the volume to be created dynamically at this time. Things to note:
* We've specifically asked for 2 replicas so that we can demonstrate access to the volume from each replica.
* We've asked for the volume to be mounted in our pod's filesystem at `/tmp/nginx-data`.

```yaml title=""pvc-mount.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - name: nginx-data
            mountPath: /tmp/nginx-data/
        resources:
          requests:
            cpu: 150m
            memory: 100Mi
          limits:
            cpu: 150m
            memory: 100Mi
      volumes:
        - name: nginx-data
          persistentVolumeClaim:
            claimName: pvc-nginx
```

Apply and check the deployment resource.

```
$ kubectl apply -f pvc-mount.yaml 
deployment.apps/nginx created
$ kubectl get pods -o wide
NAME                     READY   STATUS              RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES
nginx-65bbb4d46c-2rwl8   0/1     ContainerCreating   0          72s   <none>          nyc-adljs   <none>           <none>
nginx-65bbb4d46c-4hp4s   1/1     Running             0          72s   10.244.9.148    sfo-gwuie   <none>           <none>
nginx-65bbb4d46c-dng7q   1/1     Running             0          72s   10.245.160.39   nyc-adljs   <none>           <none>
nginx-65bbb4d46c-mg9pr   1/1     Running             0          71s   10.245.182.89   sfo-gwuie   <none>           <none>
$ 
```",878
Persistent Volumes,Test It,"Persistent Volumes
Learn how to create and mount a persistent volume claim for use by your pods
Create and Mount a Persistent Volume Claim
Make the Claim
Test It
Persistent Volumes
Learn how to create and mount a persistent volume claim for use by your pods
Now, let's exec into a pod running in the NYC node. We'll place a file there, and then demonstrate that it is also available in the other pod in the NYC node.

```
$ kubectl exec -it nginx-65bbb4d46c-dng7q -- sh
# cd /tmp/nginx-data
# echo hi there > file.txt
# ls -l
total 4
-rw-r--r-- 1 nobody nogroup 9 Jan  4 22:09 file.txt
# exit
$
```

Let's now exec into the other pod running in the NYC node and check for that same file.

```
$ kubectl exec -it nginx-65bbb4d46c-2rwl8 -- sh
# cd /tmp/nginx-data
# ls -l
total 4
-rw-r--r-- 1 nobody nogroup 9 Jan  4 22:09 file.txt
# cat file.txt
hi there
# exit
$
```

Success!

Finally, in order to shut everything down, we first need to delete the dployment, so that no pods have the volume mounted. Then we need to delete the PVC itself.

```
$ kubectl delete deploy nginx
deployment.apps ""nginx"" deleted
$ kubectl delete pvc
persistentvolumeclaime/pvc-nginx deleted
$
```

Alternately, you can delete those resources from the [Kubernetes dashboard](/guides/kubernetes-ui/dashboard.md) as well.

And that's it!

Next, try installing [Postgres on your PVC](/tutorials/data/postgres-on-pvc.md).",395
Postgres on a PVC,Create a Postgres Database using a PVC,"Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume
Create a Postgres Database using a PVC
Run in a Single Location
Create the Deployment
Test It
Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume

Cloud applications often require a place to persist relational data. Containers inside pods have ephemeral filesystems that are lost when a pod restarts or terminates. [Persistent volumes](/explanations/persistent-volumes.md) solve this problem by allowing data to persist beyond a pod's lifetime. This tutorial explains how to create a Postgres database on a persistent volume, and then demonstrates that the data survives the restart of the Postgres pod.

Note that the PVC data won't move if Section [moves your project](/explanations/aee) to a new location. See [here](/tutorials/data/distributed-data/#re-populating-a-database-as-section-relocates-it) for strategies to handle this. Or upgrade to our [Pro Plan](/explanations/billing/#standard-and-pro-plans) to allow you to specify a [static](/explanations/aee/#static) location configuration.

You'll create the following resources:
* a `location-optimizer` ConfigMap to specify a single location
* a persistent volume claim for the database, `postgres-pvc`
* a Postgres deployment, `postgres-deployment`, using the official [Postgres image](https://hub.docker.com/_/postgres) on Docker Hub
* a Service, `postgres-service`, that points to the Postgres pod",329
Postgres on a PVC,Run in a Single Location,"Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume
Create a Postgres Database using a PVC
Run in a Single Location
Create the Deployment
Test It
Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume
You'll likely want your database to [run in a single location](/guides/projects/set-edge-locations/), so specify that your project should have `maximumLocations` of 1 in your `location-optimizer` ConfigMap resource:

```yaml title=""postgres-single-location.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```

## Create the PVC

Next, create the PersistentVolumeClaim to hold the database.

```yaml title=""postgres-pvc-claim.yaml""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  storageClassName: nfs-client
  resources:
    requests:
      storage: 50Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
```

Apply and check the PersistentVolumeClaim resource.

```
$ kubectl apply -f postgres-pvc-claim.yaml
persistentvolumeclaim/postgres-pvc created
$ kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
postgres-pvc  Pending                                      nfs-client     14m
$ 
```",353
Postgres on a PVC,Create the Deployment,"Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume
Create a Postgres Database using a PVC
Run in a Single Location
Create the Deployment
Test It
Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume

Notes about the deployment:
* Write-replicas, which you are creating below, must be no more than 1. If you need horizontal scale then you can make a separate deployment of read replicas.
* The PVC is mounted at `/var/lib/postgresql/data`, and then we place the database files in the `k8s` folder underneath. The `k8s` subfolder and `runAsUser` are specified so that the Postgres container works properly with the security of the NFS volume supporting the PVC.
* In production you should use a Secret resource instead of supplying a password directly.

```yaml title=""postgres-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1 # must be no more than 1
  selector:
    matchLabels:
      app: postgres-deployment
  template:
    metadata:
      labels:
        app: postgres-deployment
    spec:
      securityContext:
        runAsUser: 999
      containers:
        - name: postgres
          image: postgres
          imagePullPolicy: Always
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_PASSWORD
              value: UseASecretInstead
            - name: PGDATA
              value: /var/lib/postgresql/data/k8s
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "".5Gi""
              cpu: ""500m""
            limits:
              memory: "".5Gi""
              cpu: ""500m""
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: postgres-pvc
```

Apply and check the Deployment resource.

```
$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE    IP              NODE        NOMINATED NODE   READINESS GATES
postgres-deployment-74fffd4c8b-98sf9   1/1     Running   0          6m7s   10.244.73.223   sfo-gwuie   <none>           <none>
postgres-deployment-74fffd4c8b-d6d5c   1/1     Running   0          6m7s   10.246.184.74   nyc-adljs   <none>           <none>
$ 
```

## Create the Service

```yaml title=""postgres-service.yaml""
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  ports:
    - port: 5432
  selector:
    app: postgres-deployment
```

Apply and check the Service resource.

```
$ kubectl apply -f postgres-service.yaml
service/postgres-service created
$ kubectl get service
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
ingress-upstream   ClusterIP   10.43.209.247   <none>        80/TCP     25h
kubernetes         ClusterIP   10.43.0.1       <none>        443/TCP    25h
postgres-service   ClusterIP   10.43.227.23    <none>        5432/TCP   6s
$ 
```

The Service gives you a hostname `postgres-service` that points to your deployment, one that you will use with the `-h` argument of `psql` in the next section. This hostname also insulates you from the fact that when the pod restarts it might come back with a different IP address.",844
Postgres on a PVC,Test It,"Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume
Create a Postgres Database using a PVC
Run in a Single Location
Create the Deployment
Test It
Postgres on a PVC
Learn how to create a PostgreSQL database on a persistent volume
We'll exec into the SFO pod, run `psql`, make a table, and do a query.

```
$ kubectl exec -it postgres-deployment-74fffd4c8b-98sf9 -- sh
postgres-deployment-74fffd4c8b-98sf9$ psql -h postgres-service
Password for user postgres: XXXXXXXXXXXXX
psql (15.1 (Debian 15.1-1.pgdg110+1))
Type ""help"" for help.

postgres=# \dt
Did not find any relations.
postgres=# CREATE TABLE PETS(
ID SERIAL PRIMARY KEY NOT NULL,
NAME TEXT NOT NULL UNIQUE,
ANIMAL TEXT NOT NULL
);
CREATE TABLE

postgres=# INSERT INTO PETS (NAME,ANIMAL) VALUES('JED','CAT');
INSERT 0 1

postgres=# INSERT INTO PETS (NAME,ANIMAL) VALUES('BUCKLEY','DOG');
INSERT 0 1

postgres=# SELECT * FROM PETS;
 id |  name   | animal 
----+---------+--------
  1 | JED     | CAT
  2 | BUCKLEY | DOG
(2 rows)

postgres=# quit
postgres-deployment-74fffd4c8b-98sf9$ exit
```

Now we'll delete the deployment but not the PVC, recreate the deployment, and then validate that the table is still present in the SFO location. This demonstrates that the PVC retained the data.

```
$ kubectl delete deploy postgres-deployment
deployment.apps ""postgres-deployment"" deleted

$ kubectl get pods -o wide
No resources found in default namespace.

$ kubectl apply -f postgres-deployment.yaml 
deployment.apps/postgres-deployment created

$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES
postgres-deployment-74fffd4c8b-2fxsp   1/1     Running   0          18s   10.246.104.111   nyc-adljs   <none>           <none>
postgres-deployment-74fffd4c8b-8chhc   1/1     Running   0          18s   10.244.73.226    sfo-gwuie   <none>           <none>

$ kubectl exec -it postgres-deployment-74fffd4c8b-8chhc -- sh
postgres-deployment-74fffd4c8b-8chhc$ psql -h postgres-service
Password for user postgres: 
psql (15.1 (Debian 15.1-1.pgdg110+1))
Type ""help"" for help.

postgres=# SELECT * FROM PETS;
 id |  name   | animal 
----+---------+--------
  1 | JED     | CAT
  2 | BUCKLEY | DOG
(2 rows)

postgres=# quit
postgres-deployment-74fffd4c8b-8chhc$ exit
$ 
```

And there you have it! Now you can access your Postgres database from within your Section project at host postgres-service, port 5432.",746
Solver Service Parameters,Solver Service Parameters A reference of available location options for Section's AEE,"Solver Service Parameters
A reference of available location options for Section's AEE
The following table provides all available location options you can use to define the location strategy when configuring your environment.

| Parameter | Value | Value Name |
|---|---|---|
| locationcode | ams | Amsterdam, Netherlands |
| locationcode | atl | Atlanta GA, USA |
| locationcode | bah | Bahrain | 
| locationcode | ord | Chicago IL, USA |
| locationcode | cmh | Columbus OH, USA |
| locationcode | dfw | Dallas TX, USA |
| locationcode | den | Denver CO, USA |
| locationcode | dus | Dusseldorf, Germany |
| locationcode | hkg | Hong Kong | 
| locationcode | lon | London, UK |
| locationcode | mad | Madrid, Spain |
| locationcode | mel | Melbourne VIC, AU |
| locationcode | mia | Miami FL, USA |
| locationcode | ewr | Newark NJ, USA |
| locationcode | nyc | New York NY, USA |
| locationcode | cdg | Paris, France |
| locationcode | per | Perth WA, AU |
| locationcode | pdx | Portland OR, USA |
| locationcode | sfo | San Francisco CA, USA |
| locationcode | sjc | San Jose CA, USA |
| locationcode | gru | Sao Paolo, Brazil |
| locationcode | sea | Seattle WA, USA |
| locationcode | sin | Singapore | 
| locationcode | arn | Stockholm, Sweden |
| locationcode | syd | Sydney NSW, AU |
| locationcode | hnd | Tokyo, Japan |
| locationcode | iad | Washington DC, USA |
| locationcode | bna | Buenos Aires, Argentina |
| locationcode | rio | Rio de Janeiro, Brazil |
| region | asia | Asia |
| region | europe | Europe |
| region | middleeast | Middle East |
| region | northamerica | North America |
| region | oceania | Oceania |
| region | southamerica | South America |",435
kubectl Supported Commands,kubectl Supported Commands A reference of available kubectl commands when working with Section,"kubectl Supported Commands
A reference of available kubectl commands when working with Section
The following are the available kubectl commands when working with Section.

<div class=""table--white-space-normal"">

| kubectl commands | Objects it applies to                             | Description |
|---|---------------------------------------------------|---|
| [get](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get) | pod, service, deployment, hpa.v2beta2.autoscaling | Prints a table of the most important information about the specified resources. |
| [create](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#create) | pod, service, deployment, hpa.v2beta2.autoscaling | Create a resource from a file or from stdin. |
| [edit](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#edit) | pod, service, deployment, hpa.v2beta2.autoscaling | Edit a resource from the default editor. |
| [delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete) | pod, service, deployment, hpa.v2beta2.autoscaling | Delete resources by file names, stdin, resources and names, or by resources and label selector. |
| [apply](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#apply) | pod, service, deployment, hpa.v2beta2.autoscaling                        | Apply a configuration to a resource by file name or stdin. |
| [describe](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe) | node, pod, service, deployment                    | Show details of a specific resource or group of resources. |
| [logs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs) | pod                                               | Print the logs for a container in a pod or specified resource. |
| [exec](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#exec) | pod                                               | Execute a command in a container. |
| [attach](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#attach) | pod                                               | Attach to a process that is already running inside an existing container. |

</div>",472
Section Metrics Reference,Section Metrics Reference A reference of metrics and labels generated by the Section Platform,"Section Metrics Reference
A reference of metrics and labels generated by the Section Platform
The following metrics are available from the Section Platform in OpenMetrics format for export to third part systems such as Datadog and Grafana Cloud, etc. See our [guide](/guides/monitor/exporting-telemetry/) for examples on how to set this up.

<div class=""table--white-space-normal"">

| Metric | Definition |
|---|---|
| section_container_cpu_usage_seconds_total | From container_cpu_usage_seconds_total. This is the actual CPU usage of a pod, in vCPU (Kubernetes ""cores""), averaged over the last 1 minute. It is a sum of all replicas of a pod in a given popname (see ""popname"" in the labels below). |
| section_container_memory_usage_bytes | From container_memory_usage_bytes. This is the actual memory usage, in GiB, averaged over the last 1 minute. |
| section_http_bytes_total_sum_rate (section_http_bytes_total:sum_rate) | This is an HTTP byte transmission rate, request bytes per second, calculated over a 2 minute span. It is called a ""sum_rate"" because it is doing its calculation over all the replicas of a given container that are running for that environment. In other words, it is a sum of all bytes in all replicas. |
| section_http_request_count_total_sum_rate (section_http_request_count_total:sum_rate) | This is an HTTP request count rate, requests per second, calculated over a 5 minute span. It is called a ""sum_rate"" because it is doing its calculation over all the replicas of a given container that are running for that environment. In other words, it is a sum of all requests in all replicas. |
| section_kube_pod_container_resource_requests | Comes from kube_pod_container_resource_requests, this gives either memory or cpu requests, depending upon the value of the labels ""resource"" or ""unit"", below. |
| section_kube_pod_status_phase | From kube_pod_status_phase |
| section_kube_pod_status_ready | From kube_pod_status_ready |

</div>

<div class=""table--white-space-normal"">

| Label | Definition |
|---|---|
| popname | This is a location in the form xxx-yyyyyyyy, where xxx is a three letter airport code (with a few exceptions), along with a yyyyyyyy hash. |
| endpoint | Deprecated |
| geo_hash | The [GeoHash](https://chrishewett.com/blog/geohash-explorer/) from where the end user is making their request |
| host | Deprecated |
| instance | Deprecated |
| job | Deprecated |
| prometheus | Deprecated |
| prometheus_replica | Deprecated |
| section_io_account_id | Section Account ID |
| section_io_application_id | Section Application ID |
| section_io_environment_id | Section Environment ID |
| section_io_module_name | The Section module for our Better CDN product |
| status_bucket | HTTP status codes from Section to the end user for our Better CDN product |
| traffic_monitor_region | The region that is handling the traffic |
| upstream_label | Deprecated |
| varnish_handling | Deprecated |
| resource | This is ""memory"" or ""cpu"" |
| unit | This is ""byte"" for resource=""memory"", and ""core"" for resource=""cpu"" |

</div>",685
HTTP Ingress,HTTP Ingress Overview,"HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress Overview
Activating HTTP Ingress
How It Works
TLS metadata
IP geolocation
Client IP Detection
Request correlation
HTTP Ingress
Description for Section HTTP Ingress
Section HTTP Ingress is implemented by an enhanced Kubernetes Ingress Controller that is built upon the open source [Nginx Ingress](https://github.com/kubernetes/ingress-nginx). Section's enhancements give application developers special capabilities such as IP blocking, handling of SSL certificates, maintenance pages, and HTTP request metadata enhancement with geo IP properties of the end user.


Section HTTP Ingress has has the following responsibilities:
* Performing the TLS handshake for HTTPS connections
* Request enrichment
* Routing HTTP requests to your applications and services
* Implementing the HTTP/2 protocol
* Request correlation

This ingress controller supports TCP port 80 for HTTP and 443 for HTTPS.

The ingress will route traffic to your applications and services based on the host header over HTTP/1.1 regardless of the incoming protocol.",213
HTTP Ingress,Activating HTTP Ingress,"HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress Overview
Activating HTTP Ingress
How It Works
TLS metadata
IP geolocation
Client IP Detection
Request correlation
HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress is enabled by declaring in your environment a single Kubernetes service that you name `ingress-upstream`. Specify the type of Service as ClusterIP. Note that Service resources of type NodePort and LoadBalancer services are not accepted. Note that the standard Kubernetes Ingress resource is not leveraged nor recognized by the Platform.

Check our [guide](/guides/http-extensions/http-ingress/) on this topic.",136
HTTP Ingress,How It Works,"HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress Overview
Activating HTTP Ingress
How It Works
TLS metadata
IP geolocation
Client IP Detection
Request correlation
HTTP Ingress
Description for Section HTTP Ingress
Use of Section HTTP Ingress causes a deployment of small, special ingress pods that will be charged to your account. There are 2 pods in the deployment with horizontal pod autoscaling turned on. This deployment is [charged to your account](https://section.io/pricing/) like any other application containers. You will not be able to see the containers using kubectl.",125
HTTP Ingress,TLS metadata,"HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress Overview
Activating HTTP Ingress
How It Works
TLS metadata
IP geolocation
Client IP Detection
Request correlation
HTTP Ingress
Description for Section HTTP Ingress

Section HTTP Ingress performs the TLS handshake with the client on behalf of your application. Some details of this TLS handshake are then relayed to your application by adding HTTP headers to the request.

An `x-forwarded-proto` header with a value of either `http` or `https` is added to indicate if the client used HTTP (i.e. no TLS) or HTTPS to connect. For HTTPS requests, the following headers will also be present.

* `section-io-tls-protocol` - the TLS protocol version negotiated with the client. Examples include `TLSv1.2` and `TLSv1.3`.
* `section-io-tls-cipher` - the TLS cipher suite negotiated with the client, in OpenSSL naming convention. Examples include `ECDHE-RSA-AES128-GCM-SHA256` and `TLS_AES_256_GCM_SHA384`. For translations to other cipher suite naming conventions see [ciphersuite.info](https://ciphersuite.info/) or [testssl.sh](https://testssl.sh/openssl-iana.mapping.html).",271
HTTP Ingress,IP geolocation,"HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress Overview
Activating HTTP Ingress
How It Works
TLS metadata
IP geolocation
Client IP Detection
Request correlation
HTTP Ingress
Description for Section HTTP Ingress
For each incoming request, the ingress controller will attempt to resolve the client's connecting IP address to a geographic location. The results of the lookup are then exposed as HTTP request headers. These headers are:
* `section-io-geo-country-code` - the [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) country code e.g. `US` for the United States.
* `section-io-geo-country-name` - the country name e.g. `United States`.
* `section-io-geo-asn` - ASN number.
* `section-io-geo-region-code` - examples include `CA` and `VA` for US IPs or `NSW` for Australian IPs.
* `section-io-geo-region-name` - examples include `California` and `Virginia` for US IPs or `New South Wales` for Australian IPs.
* `section-io-geo-city` - examples include `New York`, `Mountain View`, `Sydney`, and `Drummoyne`.
* `section-io-geo-latlon` - the approximate latitude and longitude in the format `-33.8696,151.2099`.
* `section-io-geo-postal-code` - examples include `2047` for Drummoyne, NSW, Australia and `80302` for Boulder, CO, USA.
* `section-io-geo-dma-code` - The Nielsen Designated Market Area ID as used by DoubleClick. Only for US IPs.

The ingress controller leverages the open source [MaxMind GeoIP2](https://dev.maxmind.com/geoip) library for geolocation data.",396
HTTP Ingress,Client IP Detection,"HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress Overview
Activating HTTP Ingress
How It Works
TLS metadata
IP geolocation
Client IP Detection
Request correlation
HTTP Ingress
Description for Section HTTP Ingress
Section adds the `True-Client-IP` HTTP header which is equivalent to the `remote_addr` value. This request header can be used in a number of ways:
* Fraud detection
* Logging client usage
* Rate limiting

Section also sets the [`X-Forwarded-For`](https://en.wikipedia.org/wiki/X-Forwarded-For) header if you need to use that, however this header will often be a list of IP addresses depending on how the request has been proxied prior to arriving at Section and is not always reliable.",163
HTTP Ingress,Request correlation,"HTTP Ingress
Description for Section HTTP Ingress
HTTP Ingress Overview
Activating HTTP Ingress
How It Works
TLS metadata
IP geolocation
Client IP Detection
Request correlation
HTTP Ingress
Description for Section HTTP Ingress
When the ingress controller handles each incoming request, a unique identifier is generated and is added to the request via a `section-io-id` HTTP request header. You can use this value to trace the request through your application stack.

The format of the `section-io-id` identifier is subject to change without notice so it should be treated as an opaque string and no meaning should be inferred from its value.",131
HTTP Egress,HTTP Egress Description for Section HTTP Egress,"HTTP Egress
Description for Section HTTP Egress
The Section HTTP Egress is a Kubernetes deployment that you can add to your KEI environment. It is particularly useful if you are using Section to build a CDN-like application, but may have other applications as well. It provides:

**Accurate DNS resolution for origin services**:

**HTTP keepalive management**: When a client browser visits your website, an HTTP connection opens and closes for every request. Therefore, every page that is downloaded requires a new connection. Websites are typically comprised of hundreds of files, such as images, stylesheets, JavaScript, etc., so the impact can be heavy, thereby causing slow page load times. By enabling the keep-alive header, your website can serve all the resources over a single connection.

Read our [guide](/guides/http-extensions/http-egress/) to learn how to set it up.",181
Sumo Logic,Sumo Logic Sumo Logic Integration with Section for metrics and logs,"Sumo Logic
Sumo Logic Integration with Section for metrics and logs
Learn how Section integrates with Sumo Logic.

* [Logging](/guides/monitor/logs/log-streaming/)",38
Datadog,Datadog Datadog Integration with Section for metrics and logs,"Datadog
Datadog Integration with Section for metrics and logs
Learn how Section integrates with Datadog.

* [Metrics](/guides/monitor/exporting-telemetry/datadog-section/)
* [Logging](/guides/monitor/logs/datadog-logs/)",58
Splunk,Splunk Splunk Integration with Section for metrics and logs,"Splunk
Splunk Integration with Section for metrics and logs
Learn how Section integrates with Splunk.

* [Logging](/guides/monitor/logs/log-streaming/)",35
Google Cloud,Google Cloud Google Cloud Integration with Section for metrics and logs,"Google Cloud
Google Cloud Integration with Section for metrics and logs
Learn how Section integrates with Google Cloud.

* [Logging](/guides/monitor/logs/log-streaming/)",35
S3,S3 S3 Integration with Section for metrics and logs,"S3
S3 Integration with Section for metrics and logs
Learn how Section integrates with S3.

* [Logging](/guides/monitor/logs/log-streaming/)",35
Terraform,Terraform Terraform Integration with Section,"Terraform
Terraform Integration with Section
The [Terraform Kubernetes Provider](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs) can be used to spin up supported [Kubernetes resources](/explanations/kubernetes/#what-kubernetes-resources-are-supported-by-section) in Section. Learn more about [Terraform](https://developer.hashicorp.com/terraform) and how it facilitates [infrastructure as code (IaC)](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/infrastructure-as-code).",118
PolyScale,PolyScale PolyScale integration with Section for Global Database Edge Cache,"PolyScale
PolyScale integration with Section for Global Database Edge Cache
Learn how Section integrates with PolyScale.

* [Reduce Database Latency with Global Caching](/tutorials/data/polyscale-caching.md)
* [Mastodon Hosting at the Edge](/tutorials/mastodon.md)",61
Dynatrace,Dynatrace Dynatrace Integration with Section for metrics and logs,"Dynatrace
Dynatrace Integration with Section for metrics and logs
Learn how Section integrates with Dynatrace.

* [Logging](/guides/monitor/logs/log-streaming/)",38
New Relic,New Relic New Relic Integration with Section for metrics and logs,"New Relic
New Relic Integration with Section for metrics and logs
Learn how Section integrates with New Relic.

* [Metrics](/guides/monitor/exporting-telemetry/newrelic-section/)
* [Logging](/guides/monitor/logs/newrelic-logs/)",58
Lens,Lens Lens Integration with Section for Kubernetes Integrated Development Environment,"Lens
Lens Integration with Section for Kubernetes Integrated Development Environment
Learn how Section integrates with Lens.

* [Lens IDE](/docs/guides/kubernetes-ui/other-ui/lens-integration/)",39
Logtail,Logtail Logtail Integration with Section for metrics and logs,"Logtail
Logtail Integration with Section for metrics and logs
Learn how Section integrates with Logtail.

* [Logging](/guides/monitor/logs/log-streaming/)",35
Elasticsearch,Elasticsearch Elasticsearch Integration with Section for metrics and logs,"Elasticsearch
Elasticsearch Integration with Section for metrics and logs
Learn how Section integrates with Elasticsearch.

* [Logging](/guides/monitor/logs/log-streaming/)",36
Grafana Cloud,Grafana Cloud Grafana Cloud Integration with Section for metrics and logs,"Grafana Cloud
Grafana Cloud Integration with Section for metrics and logs
Learn how Section integrates with Grafana Cloud.

* [Metrics](/guides/monitor/exporting-telemetry/grafana-section/)
* [Logging](/guides/monitor/logs/grafana-loki/)",60
1. Deploy A Project,Create a Project,"1. Deploy A Project
Create your First Section Project and Deploy it to the Section Platform
Create a Project
1. Deploy A Project
Create your First Section Project and Deploy it to the Section Platform

To begin navigate to the [Section Console](https://console.section.io/) and select **CREATE PROJECT**

![create-project](/img/docs/getting-started-create-project-select.png)

Provide the following information:
* Image Name - The name of your container image
* Port - The port you want to expose from your container
* Plan - The [Section Pricing Plan](https://section.io/pricing/) you want to use for this project

If you do not have a container you want to use for your first project, Section provides an example container, prefilled in the form, for you to test out.

When you are ready, select **Deploy** to launch your project.

![deploy-project](/img/docs/getting-started-deploy-project.png)

**That's It!** You now have a Project running on the Section platform. 

![project-card](/img/docs/getting-started-project-card.png)

Section automatically generates the following:

* Placement locations - We will configure your Section Project with a default [Location Optization](/explanations/aee/#locationoptimizer) strategy
* Global Routing - Your global HTTP traffic will be routed to the placement locations using our [Adaptive Edge Engine](/explanations/aee/)
* Project URL - Navigate to your application in the browser 
* DNS and SSL - Your Project URL will automatically have DNS routed and an SSL certificate provisioned 
* Kubernetes API - Run commands and make updates to your project through the Kubernetes API
* Kubernetes Dashboard - See the status of your deployments, optimize your locations and define resource allocations using the native Kubernetes dashboard


Next we will walk through how to check to see when your project URL has completed [Global DNS Propogation](/get-started/replace-demo-app/) so that you can navigate to your application in a browser.",412
5. Assign Domains,Add a Domain to Project,"5. Assign Domains
Assign your own custom domain to your Section Project.
Add a Domain to Project
5. Assign Domains
Assign your own custom domain to your Section Project.

To begin navigate to the [Section Console](https://console.section.io/) and for the Section Project you want to add a **Custom Domain** to and go into Project Settings by selecting the gear icon.

![project-settings](/img/docs/getting-started-project-settings.png)

Next, select **Domains** in the navigation menu. 

Add your new domain(s) by selecting **Add Domains** and putting in the hostname of your new domain, then hit **Save**.

Section will generate a new CNAME record for your project. Copy the CNAME record and configure your DNS to point to the Section provided CNAME record.

![custom-domain](/img/docs/getting-started-custom-domain.png)

You are now up and running with your own **Custom Domain**. You can verify that the domain's DNS is engaged by using the **Verify** option on the Domains settings page. 

You may now delete the section provided URL if you no longer have use for it. 

Next we will go over [Next Steps](/get-started/next-steps/) you can take with your **Section Project** such as additional Guides, Tutorials and ways to Monitor your project.",279
3. Configure Your Project,Launch the Kubernetes Dashboard,"3. Configure Your Project
Use the Kubernetes Dashboard to Manage the your containers running on the Section Platform
Launch the Kubernetes Dashboard
Edit Deployment
3. Configure Your Project
Use the Kubernetes Dashboard to Manage the your containers running on the Section Platform

From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page. 

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.",113
3. Configure Your Project,Edit Deployment,"3. Configure Your Project
Use the Kubernetes Dashboard to Manage the your containers running on the Section Platform
Launch the Kubernetes Dashboard
Edit Deployment
3. Configure Your Project
Use the Kubernetes Dashboard to Manage the your containers running on the Section Platform

On the Kubernetes Dashboard you should see your deployment. In this example and if you chose to deploy our example container the deployment is named `section-project-deployment` and is running the image `sectionio/my-first-distributed-app:latest`. If you wanted to change out this image for another you will need to edit the deployment. 

![edit-deployment](/img/docs/getting-started-edit-deployment.png)

Edit the deployment YAML Section generated when you deployed your first project. To change the container image you will need to update the container spec with the `image` and `containerPort`.

![update-image-name](/img/docs/getting-started-update-image-name.png)

Select **Update** and your changes will be applied to your deployment. You can then use the Kubernetes dashboard to monitor the changes happening to your workload in real time. 

Your application will be running on a select set of Section's available global edge locations. In the next step we will walk through how to [Update App Locations](/get-started/demo-app-locations/) so you can optimize your deployment for performance and make full use of Section's dynamic and distributed platform.",282
2. Check DNS Propagation,Copy Project URL,"2. Check DNS Propagation
Check to see the Section has propagated your DNS globally for your project
Copy Project URL
Check DNS
Visit Your Application
2. Check DNS Propagation
Check to see the Section has propagated your DNS globally for your project

To begin navigate to the [Section Console](https://console.section.io/) and copy your **PROJECT URL** from the project you just created.

![project-url](/img/docs/getting-started-project-url.png)",97
2. Check DNS Propagation,Check DNS,"2. Check DNS Propagation
Check to see the Section has propagated your DNS globally for your project
Copy Project URL
Check DNS
Visit Your Application
2. Check DNS Propagation
Check to see the Section has propagated your DNS globally for your project

Next, bring up a [DNS Checker](https://dnschecker.org/) tool and paste the **PROJECT URL** and hit search. As your DNS propagates you will see more and more global locations resolve. Once you see all green checks your DNS has propagated and your **PROJECT URL** is live. 

![dns-checker](/img/docs/getting-started-dns-checker.png)",133
2. Check DNS Propagation,Visit Your Application,"2. Check DNS Propagation
Check to see the Section has propagated your DNS globally for your project
Copy Project URL
Check DNS
Visit Your Application
2. Check DNS Propagation
Check to see the Section has propagated your DNS globally for your project

You can now navigate to your application in the browser using the provided **PROJECT URL**. 


Now that you have an application running on Section, we will walk you through how to [Configure Your Project](/get-started/replace-demo-app/) deployment so that you can run your own application and learn to make changes.",118
6. Next Steps,Tutorials,"6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project
Tutorials
Section Platform
Upgrade your Plan
Monitoring
6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project

Section offers a wide range of **Tutorials** so you can further your learning around what is possible with Section.

We have tutorials that walk through a number of different **Frameworks** such as:
* [Node Express](/tutorials/frameworks/)
* [React](/tutorials/frameworks/)
* [NextJS](/tutorials/frameworks/)",125
6. Next Steps,Section Platform,"6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project
Tutorials
Section Platform
Upgrade your Plan
Monitoring
6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project

The Section Platform offers many additional features that may be applicaple to your **Section Project**. To read more more see our [Explanations](/docs/explanations/) docs for more information on things such as:

* [Adaptive Edge Engine](/explanations/aee/)
* [Anycast Network](/explanations/anycast/)
* [Horizontal Pod Autoscaler](/explanations/horizontal-pod-autoscaler/)
* [Composable Edge Cloud](explanations/cec/)",161
6. Next Steps,Upgrade your Plan,"6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project
Tutorials
Section Platform
Upgrade your Plan
Monitoring
6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project
To access more features, locations and higher resource limits, Upgrade your Project Plan to Standard or Pro Plan. 
* [Review Plans and Pricing](https://section.io/pricing/)
* [Upgrade your Plan](/docs/guides/projects/upgrade-plan/)",106
6. Next Steps,Monitoring,"6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project
Tutorials
Section Platform
Upgrade your Plan
Monitoring
6. Next Steps
Learn more about our available Guides, Tutorials and options for Monitoring your Project
Section offers a variety of ways to monitor your Section Project. 

* [Kubernetes Dashboard](/guides/monitor/monitoring-using-k8s-dashboard/): You can make use of the **Kubernetes Dashboard** to get real time information using standard Kubernetes outputs
* [Export Telemetry](/guides/monitor/exporting-telemetry/): Export metrics to tools such as Datadog and Grafana Cloud.
* [Project Logs](/guides/monitor/logs/): Ship your Project's logs to external tools such as Datadog and Grafana Loki.",171
4. Configure Locations,Launch the Kubernetes Dashboard,"4. Configure Locations
Use the Kubernetes Dashboard to Manage the Locations for your Project
Launch the Kubernetes Dashboard
Create New Resource
4. Configure Locations
Use the Kubernetes Dashboard to Manage the Locations for your Project

From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page.

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.",105
4. Configure Locations,Create New Resource,"4. Configure Locations
Use the Kubernetes Dashboard to Manage the Locations for your Project
Launch the Kubernetes Dashboard
Create New Resource
4. Configure Locations
Use the Kubernetes Dashboard to Manage the Locations for your Project

To update your Project location optimization strategy you must supply a **ConfigMap** named `location-optimizer`. For an explanation on the configuration of the ConfigMap see our [AEE Explanation](/explanations/aee/).

To add the ConfigMap to your deployment from the Kubernetes Dashboard you will add a new **resources** by selecting the plus in the upper right hand corner of the dashboard.

Select **Create from input** and paste in your `location-optimizer` **ConfigMap**

![location-optimizer](/img/docs/getting-started-location-optimizer.png)


Select **Upload** and Section will begin to apply your location optimization strategy to your deployment. You can monitor this on the overview page on the Kubernetes dashboard.

***NOTE:*** Projects using the free plan have limited access to location optimization strategies. For more see our [Pricing Information](https://section.io/pricing/)

Next we will walk you through how to assign your own [Custom Domain](/get-started/assign-domains/) to a project.",252
