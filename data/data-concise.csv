title,description,content,tokens
Kubernetes and Section ,Kubernetes API and Section,"For more information about Kuberbetes please refer to the [Kubernetes Documentation](https://kubernetes.io/docs/home).

Using the Kubernetes API with Section allows you to implement important Kubernetes Resources that are supported by Section.",44
Kubernetes and Section ,What can you use the Kubernetes API to do with Section?,"Once you've created a containerized application, you can use the Kubernetes API to do the following on the Section platform:

* Deploy your container to multiple locations
* Configure service discovery, so that your users will be routed to the best container instance
* Define more complex applications, such as composite applications that consist of more than one container
* Define the resource allocations for your system
* Define scaling factors, such as the number of containers per location, and what signals should be used to scale in and out
* Maintain your application code, configuration and deployment manifests in your own code management systems and image registries
* Control how the Adaptive Edge Engine schedules containers, performs health management, and routes traffic",141
Kubernetes and Section ,How can you use the Kubernetes API with Section?,"Because Section is compatible with the Kubernetes API, you can use standard Kubernetes tools to interact with the system.

The most common tool is `kubectl`, which is the tool that this documentation is centered around.

When you use `kubectl` you will create a configuration context, which will connect your tool to Section. From there you can continue to use `kubectl` as you normally would with a single cluster.

As Section is Kubenetes API compatible, you'll find that many existing tools work without complication. For example, you could use `helm` to manage your system.

Check out [Getting started with Kubernetes API](/docs/guides/kubernetes-ui/kubernetes-api/basics/) for a step-by-step guide.",142
Kubernetes and Section ,What Kubernetes resources are supported by Section?,"In your manifests you can use the following Kubernetes Resources
* Deployment
* ConfigMap
* Secret
* Service (ClusterIP and ExternalName, but not types NodePort nor LoadBalancer)
* HorizontalPodAutoscaler ([example](/explanations/horizontal-pod-autoscaler/))

Section will create and manage the following resources for you, i.e. you cannot create them yourself:
* Namespace
* NetworkPolicy
* ReplicaSet
* Pod

For specification of location strategies, Section recognizes a particular ConfigMap resource with a specific name of `location-optimizer`.

For engaging Section's [HTTP Ingress](/guides/http-extensions/http-ingress/), Section recognizes a particular Service resource with a specific name of `ingress-upstream`.",155
Kubernetes and Section ,Container Resources,"When you define your Deployment objects, you can specify the CPU and RAM requests or limits for each container instance.

Use the standard Kubernetes methods for specifying your container's requirements.

Some notes:
* Please refer to the [plans and pricing](https://section.io/pricing/) to understand how your requests can impact your billing.
* Section may alter your YAML to ensure that request = limit.
  * If request and limit are not equal, Section will use the higher of the two values.
  * If only one of request or limit is specified, that value will be used for both request and limit.
* You cannot request ephemeral storage directly. Section will automatically apply the ephemeral storage limits when the deployment is created.",144
Kubernetes and Section ,What storage systems are available for Section workloads?,"Section supports ephemeral storage.

Section will automatically apply ephemeral storage limits to your containers based on the container size you've selected.

Your application can use the ephemeral storage in its local filesystem to perform disk IO activities.

""Ephemeral"" means that there is no long-term guarantee about durability. You should take this into consideration in your application design.",70
Kubernetes and Section ,How does Section interact with Kubernetes,"
Our platform is designed to provide you with a simple on ramp to a federated, dynamic, global, cluster of Kubernetes clusters.  

Working with the Section platform will feel just as though you are working with a single Kubernetes Cluster and that cluster is provided for you ""**As a Service**"".

Because we expose Kubernetes API exactly as per the [Kubernetes Docs](https://kubernetes.io/docs/home), working with Section is a Cloud Native Dream.

You can use standard Kubernetes tooling to deploy applications to Section's Distributed Platform.  We provide you with a [Kubernetes Dashboard](/docs/guides/kubernetes-ui/dashboard/) for every Project or you can use the Kubernetes API endpoint provided for each Project.


## Kubernetes API and Section

For more information about Kuberbetes please refer to the [Kubernetes Documentation](https://kubernetes.io/docs/home).

Using the Kubernetes API with Section allows you to implement important Kubernetes Resources that are supported by Section.

### What can you use the Kubernetes API to do with Section?

Once you've created a containerized application, you can use the Kubernetes API to do the following on the Section platform:

* Deploy your container to multiple locations
* Configure service discovery, so that your users will be routed to the best container instance
* Define more complex applications, such as composite applications that consist of more than one container
* Define the resource allocations for your system
* Define scaling factors, such as the number of containers per location, and what signals should be used to scale in and out
* Maintain your application code, configuration and deployment manifests in your own code management systems and image registries
* Control how the Adaptive Edge Engine schedules containers, performs health management, and routes traffic

### How can you use the Kubernetes API with Section?

Because Section is compatible with the Kubernetes API, you can use standard Kubernetes tools to interact with the system.

The most common tool is `kubectl`, which is the tool that this documentation is centered around.

When you use `kubectl` you will create a configuration context, which will connect your tool to Section. From there you can continue to use `kubectl` as you normally would with a single cluster.

As Section is Kubenetes API compatible, you'll find that many existing tools work without complication. For example, you could use `helm` to manage your system.

Check out [Getting started with Kubernetes API](/docs/guides/kubernetes-ui/kubernetes-api/basics/) for a step-by-step guide.

## What Kubernetes resources are supported by Section?

In your manifests you can use the following Kubernetes Resources
* Deployment
* ConfigMap
* Secret
* Service (ClusterIP and ExternalName, but not types NodePort nor LoadBalancer)
* HorizontalPodAutoscaler ([example](/explanations/horizontal-pod-autoscaler/))

Section will create and manage the following resources for you, i.e. you cannot create them yourself:
* Namespace
* NetworkPolicy
* ReplicaSet
* Pod

For specification of location strategies, Section recognizes a particular ConfigMap resource with a specific name of `location-optimizer`.

For engaging Section's [HTTP Ingress](/guides/http-extensions/http-ingress/), Section recognizes a particular Service resource with a specific name of `ingress-upstream`.

## Container Resources

When you define your Deployment objects, you can specify the CPU and RAM requests or limits for each container instance.

Use the standard Kubernetes methods for specifying your container's requirements.

Some notes:
* Please refer to the [plans and pricing](https://section.io/pricing/) to understand how your requests can impact your billing.
* Section may alter your YAML to ensure that request = limit.
  * If request and limit are not equal, Section will use the higher of the two values.
  * If only one of request or limit is specified, that value will be used for both request and limit.
* You cannot request ephemeral storage directly. Section will automatically apply the ephemeral storage limits when the deployment is created.

## What storage systems are available for Section workloads?

Section supports ephemeral storage.

Section will automatically apply ephemeral storage limits to your containers based on the container size you've selected.

Your application can use the ephemeral storage in its local filesystem to perform disk IO activities.

""Ephemeral"" means that there is no long-term guarantee about durability. You should take this into consideration in your application design.
",893
Section API,Section API Learn how to interact with the Section API,"Section’s API allows you to interact with all Section objects and resources in the manner you choose.

To get started using Section’s api please start by getting an [API Token](/guides/iam/api-tokens/) for your user. Then you'll be able to give commands such as this one, which returns all of the accounts to which the user belongs.

```bash
curl \
  --header ""Accept: application/json"" \
  --header ""section-token: SECTION_API_TOKEN"" \
  -X GET ""https://aperture.section.io/api/v1/account""
```

For the full documentation of available calls please visit our [interactive API documentation](https://aperture.section.io/api/ui/).",143
Section API,Learn how to interact with the Section API,"
Section’s API allows you to interact with all Section objects and resources in the manner you choose.

To get started using Section’s api please start by getting an [API Token](/guides/iam/api-tokens/) for your user. Then you'll be able to give commands such as this one, which returns all of the accounts to which the user belongs.

```bash
curl \
  --header ""Accept: application/json"" \
  --header ""section-token: SECTION_API_TOKEN"" \
  -X GET ""https://aperture.section.io/api/v1/account""
```

For the full documentation of available calls please visit our [interactive API documentation](https://aperture.section.io/api/ui/).
",144
Horizontal Pod Autoscaler,Horizontal Pod Autoscaler (HPA),"HPA - the Horizontal Pod Autoscaler is a Kubernetes extension that automatically adjusts the number of replicas of a deployment in response to the resource demand of a workload.

HPA is a feature available on Section Enterprise Accounts.

For more information on Kubernetes and HPA, see [Kubernetes docs](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/).",76
Horizontal Pod Autoscaler,How to use the Horizontal Pod Autoscaler resource with Section?,"After you have [created a Project](/get-started/create-project) in Section, you can use the Horizontal Pod Autoscaler to automatically scale the number of replicas of the deployment.

- Create a yaml file, such as hpa.yaml with the following content:
```yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 50

```

- Apply it to your application:
```bash
kubectl apply -f hpa.yaml
```

- See your HPA object running on Section:
```bash
kubectl get hpa.v2beta2.autoscaling
```

See other supported [kubectl commands](/reference/kubectl-commands) you can use with the HPA resource.

## What parts of the Horizontal Pod Autoscaler spec are supported by Section?

- Section, supports the `autoscaling/v2beta2` version of the Horizontal Pod Autoscaler API object.
- The following fields (including subfields) of the Horizontal Pod Autoscaler spec are supported:
  - `scaleTargetRef`
  - `minReplicas`
  - `maxReplicas`
  - `metrics`
      - `type: Resource`
- When using a `Resource` metric, scaling is only supported based on the `cpu` and `memory` resources.
- The `maxReplicas` field can have the highest value of `20`.


## Adaptive Edge Engine(AEE) and Horizontal Pod Autoscaler (HPA)

The [AEE](/explanations/aee) and the HPA work together to provide a scalable container deployment that scales across
the globe and within a particular edge location.

While AEE deploys the deployment to new edge locations depending on the traffic requirements in a particular region, the HPA is used to scale the number of replicas of the deployment in a particular edge location based on the resource (CPU and/or memory) demand.",499
Horizontal Pod Autoscaler,What is Horizontal Pod Autoscaler?,"
# Horizontal Pod Autoscaler (HPA)

HPA - the Horizontal Pod Autoscaler is a Kubernetes extension that automatically adjusts the number of replicas of a deployment in response to the resource demand of a workload.

HPA is a feature available on Section Enterprise Accounts.

For more information on Kubernetes and HPA, see [Kubernetes docs](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/).

## How to use the Horizontal Pod Autoscaler resource with Section?

After you have [created a Project](/get-started/create-project) in Section, you can use the Horizontal Pod Autoscaler to automatically scale the number of replicas of the deployment.

- Create a yaml file, such as hpa.yaml with the following content:
```yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 50

```

- Apply it to your application:
```bash
kubectl apply -f hpa.yaml
```

- See your HPA object running on Section:
```bash
kubectl get hpa.v2beta2.autoscaling
```

See other supported [kubectl commands](/reference/kubectl-commands) you can use with the HPA resource.

## What parts of the Horizontal Pod Autoscaler spec are supported by Section?

- Section, supports the `autoscaling/v2beta2` version of the Horizontal Pod Autoscaler API object.
- The following fields (including subfields) of the Horizontal Pod Autoscaler spec are supported:
  - `scaleTargetRef`
  - `minReplicas`
  - `maxReplicas`
  - `metrics`
      - `type: Resource`
- When using a `Resource` metric, scaling is only supported based on the `cpu` and `memory` resources.
- The `maxReplicas` field can have the highest value of `20`.


## Adaptive Edge Engine(AEE) and Horizontal Pod Autoscaler (HPA)

The [AEE](/explanations/aee) and the HPA work together to provide a scalable container deployment that scales across
the globe and within a particular edge location.

While AEE deploys the deployment to new edge locations depending on the traffic requirements in a particular region, the HPA is used to scale the number of replicas of the deployment in a particular edge location based on the resource (CPU and/or memory) demand.

",598
Section Billing,Free Plans,"On sign up for a Section account and [creation of your first Free Project](/get-started/create-project/), you will receive a $0 invoice for your Project.   

You may have one Free Project per Account.  Additional Standard or Pro Projects can be added you account at any time.

Free projects are hibernated after a period of time, requiring regular interaction in the Section Console in order to remain active.",85
Section Billing,Standard and Pro Plans,"Pricing and Limits for Standard and Pro Plans can be found at our [Pricing Overview](https://section.io/pricing/).

On creation of a Standard or Pro plan Project, you will be billed for the plan amount on the date of Project creation (or upgrade to the new plan) and you will receive and invoice by email to your User email address.

Thereafter, you will be billed monthly from that date at the Plan rate.",89
Section Billing,Enterprise Plans,"Please [contact us](https://www.section.io/contact-us/) for help with Enterprise Plan options where you need addtiional resources, functionality or wish to combine a larger number of Standard or Pro Projects into a single Enterprise Plan billing structure.",48
Section Billing,Credit Card Billing,"On creation of (or upgrade to) a Standard or Pro Plan you will be asked to enter (or select) a Credit Card for that Project.

![Add Payment](/img/docs/add-payment.png)

You may keep several payment methods on your Account and different payment methods can be used for different Projects.",61
Section Billing,Section Billing Processes,"
## Plans

### Free Plans

On sign up for a Section account and [creation of your first Free Project](/get-started/create-project/), you will receive a $0 invoice for your Project.   

You may have one Free Project per Account.  Additional Standard or Pro Projects can be added you account at any time.

Free projects are hibernated after a period of time, requiring regular interaction in the Section Console in order to remain active.

### Standard and Pro Plans

Pricing and Limits for Standard and Pro Plans can be found at our [Pricing Overview](https://section.io/pricing/).

On creation of a Standard or Pro plan Project, you will be billed for the plan amount on the date of Project creation (or upgrade to the new plan) and you will receive and invoice by email to your User email address.

Thereafter, you will be billed monthly from that date at the Plan rate.

### Enterprise Plans

Please [contact us](https://www.section.io/contact-us/) for help with Enterprise Plan options where you need addtiional resources, functionality or wish to combine a larger number of Standard or Pro Projects into a single Enterprise Plan billing structure. 

## Credit Card Billing

On creation of (or upgrade to) a Standard or Pro Plan you will be asked to enter (or select) a Credit Card for that Project.

![Add Payment](/img/docs/add-payment.png)

You may keep several payment methods on your Account and different payment methods can be used for different Projects.


",307
Section Console Reference Application,Overview,"This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.

Use this codebase to build a more tailored portal experience for your users to interact with your product while abstracting the Section administrative interface.",47
Section Console Reference Application,License to Use,"Section grants permission to users of this Console Reference Application code to use, modify and build upon the code ongoing as needed, free of charge, for the purpose of building a tailored portal experience. Section grants these rights to current paying customers. This portal code is provided for reference purposes only and provided as-is with no warranties.

This is provided on the condition that the code is not:
1. Open sourced.
2. Made available to other parties outside of the conditions highlighted herein.
3. Used for purposes outside the scope of building a tailored portal for access to the Section platform.
4. Separately resold - either as a one-off or subscription service (royalties or license fees may apply without prior approval from Section).",146
Section Console Reference Application,Tools and technologies,"This project is built upon Laravel, a PHP application framework. Services required to run this application (Nginx webserver, php-fpm, MySQL) are provided as docker images (see `dockerfiles/*` and `docker-compose.yml` for more detail).

Tooling required to develop on this codebase (e.g Composer, NodeJS) are included in the PHP image, and wrapper commands exist for ease of use (e.g ```ahoy composer``` and ```ahoy npm```).

The following tools are required in your local development environment:
-   [Ahoy CLI](https://github.com/ocean/ahoy/releases/tag/2.1.0)
-   [Docker and Docker Compose](https://www.docker.com/)",152
Section Console Reference Application,Initial configuration,"The majority of configuration required happens in the `.env` file. This file injects environment variables to the running application.

To begin, copy the `.env.example` file to `.env` and edit in your favorite text editor.",46
Section Console Reference Application,Basic configuration,"Start by editing the following values:

-   `APP_NAME`: The application name
-   `APP_COMPANY_NAME`: Your company name
-   `APP_COMPANY_URL`: Your company URL
-   `MAIL_FROM_ADDRESS`: The from email address associated with outgoing mail
-   `MAIL_FROM_NAME`: The from name associated with outgoing mail

These values are used throughout the portal, in email notifications, and on invoices (if Subscriptions are enabled).

Connect to your Section account by modifying the following values:
-   `SECTION_ACCOUNT`: Your Section account ID
-   `SECTION_APPLICATION_STACK`: The name of the stack to provision on create
-   `SECTION_CONFIG_FILE`: Path to custom JSON file in the repository
-   `SECTION_AUTH`: Basic authentication credentials for Aperture, create with ```echo -n user:password | base64```",173
Section Console Reference Application,reCAPTCHA support,"By default Google reCAPTCHA is enabled on the user registration form. This prevents spam registrations and is generally recommended.

1. Create a new v2 ""I'm not a robot"" tickbox from the [reCAPTCHA dashboard](https://www.google.com/u/1/recaptcha/admin).
2. Add ""localhost"" as a domain for local development, plus any other domains the application will run on.
3. Add the site key to the NOCAPTCHA_SITEKEY value in .env.
4. Add the secret key to the NOCAPTCHA_SECRET value in .env.

If you wish to disable reCAPTCHA set the NOCAPTCHA_ENABLED value to false.",135
Section Console Reference Application,Building the project,"Once all required software packages and basic configuration values are present we can build the project. To do this run the following command:

    ahoy build

This will build docker images and run local containers. It will also import database schema and import seed data. You should see the following info on success:

      --- App Info ---

      Site:  http://localhost:8002

      Mailhog:  http://localhost:8026

      Maria port: 3307

Visit the project on [http://localhost:8002](http://localhost:8002/) to get started.",120
Section Console Reference Application,Configuring look & feel,"The starter kit provides some simple variables to alter color scheme and logo. Edit the `resources/sass/_variables.scss` file to update path to logo file (SVG recommended) and colors.

Once changes have been made you will need to recompile the frontend assets. Do this with:

    ahoy npm run production

If you wish to build non-minified versions for debugging build with:

    ahoy npm run development

You should commit the resulting built artefacts to the repository.",97
Section Console Reference Application,Mailhog & Email configuration,"Mailhog is an application that traps outgoing emails to make local development easier. This application runs Mailhog and is configured to trap email by default.

To view the Mailhog interface run ```ahoy info``` for the local service URL. Any verification, password reset, notification emails will be sent here.",60
Section Console Reference Application,Configuring email,"Many email providers are supported, including Mailgun, Postmark, AWS SES, SMTP, sendmail. Read the [Laravel docs](https://laravel.com/docs/8.x/mail) for detail on configuring the mail driver in the .env file.

To set the email sender address and name, use the following settings:

    EMAIL_ADDRESS_FROM=info@example.com

    EMAIL_ADDRESS_NAME=Info",81
Section Console Reference Application,Email verification,"To require users to verify their email address for their account to have full access set the following in the .env file:

    EMAIL_VERIFY_REQUIRED=true",29
Section Console Reference Application,Billing and subscriptions,"A simple subscriptions, billing and invoices solution is included in the starter kit. This allows for simple SaaS-like products to be built on the Section platform with minimal effort.

By default, subscription services are enabled. Subscription services are broken into two main categories: individuals and organizations. Organizations allow for teams to be created and users to be invited to access an account with varying levels of permission.

Out of the box, subscription features place limits on the numbers of projects and team members that can be created based on a number of customizable tiers defined in code.

To disable the need for a subscription to enable all features, set the following in the .env file. Alternatively, a specific individual/organization can be assigned the 'unlimited' subscription.

    SUBSCRIPTIONS_ENABLE=false",153
Section Console Reference Application,Production deployments,"To run in a production environment the following changes should be made in the .env file.

    APP_DEBUG=false

    APP_URL=https://www.production-domain.com

    PROD_MODE=true

Ensure the following SECTION_AUTH env variable contains valid authentication with access to your production applications and environments.

If Subscriptions/Billing is active ensure the following values are provided:

    STRIPE_KEY_PROD 

    STRIPE_SECRET_PROD

Database values (`DB_*`) should be updated to point to a production database service. While it is possible to use the provided MySQL container it is not recommended, as it represents a SPOF (single point of failure). A cloud service like AWS, Azure, Google Cloud.

Email should be configured as per the ""Configuring email"" section above.",158
Section Console Reference Application,"Help, Support and Professional Services","This reference portal application is provided as-is with no Section specific support provided. If you would like help building on the scope of this codebase outside the reference application itself, we can facilitate a relationship with our partner [QuantCDN](https://www.quantcdn.io/) who are experts in building portal applications on top of this reference application. They are able to work with you in a Professional Services capacity to customize this console experience.

Contact Section at se@section.io and we will be happy to set up that connection.",105
Section Console Reference Application,This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.,"
## Overview
This codebase provides a starter kit that provides out-of-the-box integrations with the Section platform.

Use this codebase to build a more tailored portal experience for your users to interact with your product while abstracting the Section administrative interface.

### License to Use
Section grants permission to users of this Console Reference Application code to use, modify and build upon the code ongoing as needed, free of charge, for the purpose of building a tailored portal experience. Section grants these rights to current paying customers. This portal code is provided for reference purposes only and provided as-is with no warranties.

This is provided on the condition that the code is not:
1. Open sourced.
2. Made available to other parties outside of the conditions highlighted herein.
3. Used for purposes outside the scope of building a tailored portal for access to the Section platform.
4. Separately resold - either as a one-off or subscription service (royalties or license fees may apply without prior approval from Section).

### Tools and technologies
This project is built upon Laravel, a PHP application framework. Services required to run this application (Nginx webserver, php-fpm, MySQL) are provided as docker images (see `dockerfiles/*` and `docker-compose.yml` for more detail).

Tooling required to develop on this codebase (e.g Composer, NodeJS) are included in the PHP image, and wrapper commands exist for ease of use (e.g ```ahoy composer``` and ```ahoy npm```).

The following tools are required in your local development environment:
-   [Ahoy CLI](https://github.com/ocean/ahoy/releases/tag/2.1.0)
-   [Docker and Docker Compose](https://www.docker.com/)

## Getting Started
### Initial configuration
The majority of configuration required happens in the `.env` file. This file injects environment variables to the running application.

To begin, copy the `.env.example` file to `.env` and edit in your favorite text editor.

#### Basic configuration
Start by editing the following values:

-   `APP_NAME`: The application name
-   `APP_COMPANY_NAME`: Your company name
-   `APP_COMPANY_URL`: Your company URL
-   `MAIL_FROM_ADDRESS`: The from email address associated with outgoing mail
-   `MAIL_FROM_NAME`: The from name associated with outgoing mail

These values are used throughout the portal, in email notifications, and on invoices (if Subscriptions are enabled).

Connect to your Section account by modifying the following values:
-   `SECTION_ACCOUNT`: Your Section account ID
-   `SECTION_APPLICATION_STACK`: The name of the stack to provision on create
-   `SECTION_CONFIG_FILE`: Path to custom JSON file in the repository
-   `SECTION_AUTH`: Basic authentication credentials for Aperture, create with ```echo -n user:password | base64```

#### reCAPTCHA support
By default Google reCAPTCHA is enabled on the user registration form. This prevents spam registrations and is generally recommended.

1. Create a new v2 ""I'm not a robot"" tickbox from the [reCAPTCHA dashboard](https://www.google.com/u/1/recaptcha/admin).
2. Add ""localhost"" as a domain for local development, plus any other domains the application will run on.
3. Add the site key to the NOCAPTCHA_SITEKEY value in .env.
4. Add the secret key to the NOCAPTCHA_SECRET value in .env.

If you wish to disable reCAPTCHA set the NOCAPTCHA_ENABLED value to false.

### Building the project
Once all required software packages and basic configuration values are present we can build the project. To do this run the following command:

    ahoy build

This will build docker images and run local containers. It will also import database schema and import seed data. You should see the following info on success:

      --- App Info ---

      Site:  http://localhost:8002

      Mailhog:  http://localhost:8026

      Maria port: 3307

Visit the project on [http://localhost:8002](http://localhost:8002/) to get started.

### Configuring look & feel
The starter kit provides some simple variables to alter color scheme and logo. Edit the `resources/sass/_variables.scss` file to update path to logo file (SVG recommended) and colors.

Once changes have been made you will need to recompile the frontend assets. Do this with:

    ahoy npm run production

If you wish to build non-minified versions for debugging build with:

    ahoy npm run development

You should commit the resulting built artefacts to the repository.

### Mailhog & Email configuration
Mailhog is an application that traps outgoing emails to make local development easier. This application runs Mailhog and is configured to trap email by default.

To view the Mailhog interface run ```ahoy info``` for the local service URL. Any verification, password reset, notification emails will be sent here.

#### Configuring email
Many email providers are supported, including Mailgun, Postmark, AWS SES, SMTP, sendmail. Read the [Laravel docs](https://laravel.com/docs/8.x/mail) for detail on configuring the mail driver in the .env file.

To set the email sender address and name, use the following settings:

    EMAIL_ADDRESS_FROM=info@example.com

    EMAIL_ADDRESS_NAME=Info

#### Email verification
To require users to verify their email address for their account to have full access set the following in the .env file:

    EMAIL_VERIFY_REQUIRED=true

### Billing and subscriptions
A simple subscriptions, billing and invoices solution is included in the starter kit. This allows for simple SaaS-like products to be built on the Section platform with minimal effort.

By default, subscription services are enabled. Subscription services are broken into two main categories: individuals and organizations. Organizations allow for teams to be created and users to be invited to access an account with varying levels of permission.

Out of the box, subscription features place limits on the numbers of projects and team members that can be created based on a number of customizable tiers defined in code.

To disable the need for a subscription to enable all features, set the following in the .env file. Alternatively, a specific individual/organization can be assigned the 'unlimited' subscription.

    SUBSCRIPTIONS_ENABLE=false

## Production deployments
To run in a production environment the following changes should be made in the .env file.

    APP_DEBUG=false

    APP_URL=https://www.production-domain.com

    PROD_MODE=true

Ensure the following SECTION_AUTH env variable contains valid authentication with access to your production applications and environments.

If Subscriptions/Billing is active ensure the following values are provided:

    STRIPE_KEY_PROD 

    STRIPE_SECRET_PROD

Database values (`DB_*`) should be updated to point to a production database service. While it is possible to use the provided MySQL container it is not recommended, as it represents a SPOF (single point of failure). A cloud service like AWS, Azure, Google Cloud.

Email should be configured as per the ""Configuring email"" section above.

## Help, Support and Professional Services
This reference portal application is provided as-is with no Section specific support provided. If you would like help building on the scope of this codebase outside the reference application itself, we can facilitate a relationship with our partner [QuantCDN](https://www.quantcdn.io/) who are experts in building portal applications on top of this reference application. They are able to work with you in a Professional Services capacity to customize this console experience.

Contact Section at se@section.io and we will be happy to set up that connection.
",1584
HTTP Anycast,Conditions,"You can use the Section Anycast networks to route traffic to your application.  Our Anycast networks are avaialble to Enterprise plans and is recommended when all of the following are true:
* your site domain is a zone apex (aka bare domain), e.g. `<your-domain-name>.com` instead of `www.<your-domain-name>.com`, and
* your DNS hosting does not support using an ANAME or ALIAS record at the zone apex to simulate a CNAME record, and
* you cannot change your DNS hosting to another DNS provider that does support ANAME records.",122
HTTP Anycast,LocationOptimizer configuration,"To use Anycast on Section, you must set your location-optimizer configuration to use the `anycast` policy. The anycast policy has a required parameter called `network`.
The network parameter indicates which anycast IP you will be using (see below). The default value for the network parameter is ""global-045"".

A minimal LocationOptimizer configuration for anycast looks like this:
```json title=""minimal locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast""
  }
}
```

The minimal configuration is equivalent to:
```json title=""equivalent locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast"",
    ""network"":""global-045""
   }
}
```

An alternative configuration is:
```json title=""equivalent locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast"",
    ""network"":""global-103""
  }
}
```
### Anycast Networks and IP addresses
Both anycast networks are suitable to global traffic and the global-045 network is preferred for the majority of customers. However, if you have a preponderance of traffic in Australia and would benefit from finer geographic resolution there, then the global-103 network would be desirable.

The table below shows the IP address that you would enter into your DNS records to access these networks:

**Network** | **IP Address**
:-----------|:--------------|
global-045|45.154.183.183
global-103|103.107.226.226",338
HTTP Anycast,How and why to use the Anycast location policy for your application,"
### Conditions
You can use the Section Anycast networks to route traffic to your application.  Our Anycast networks are avaialble to Enterprise plans and is recommended when all of the following are true:
* your site domain is a zone apex (aka bare domain), e.g. `<your-domain-name>.com` instead of `www.<your-domain-name>.com`, and
* your DNS hosting does not support using an ANAME or ALIAS record at the zone apex to simulate a CNAME record, and
* you cannot change your DNS hosting to another DNS provider that does support ANAME records.

### LocationOptimizer configuration
To use Anycast on Section, you must set your location-optimizer configuration to use the `anycast` policy. The anycast policy has a required parameter called `network`.
The network parameter indicates which anycast IP you will be using (see below). The default value for the network parameter is ""global-045"".

A minimal LocationOptimizer configuration for anycast looks like this:
```json title=""minimal locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast""
  }
}
```

The minimal configuration is equivalent to:
```json title=""equivalent locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast"",
    ""network"":""global-045""
   }
}
```

An alternative configuration is:
```json title=""equivalent locationOptimizer.json""
{
  ""strategy"":""SolverServiceV1"",
  ""params"":{
    ""policy"":""anycast"",
    ""network"":""global-103""
  }
}
```
### Anycast Networks and IP addresses
Both anycast networks are suitable to global traffic and the global-045 network is preferred for the majority of customers. However, if you have a preponderance of traffic in Australia and would benefit from finer geographic resolution there, then the global-103 network would be desirable.

The table below shows the IP address that you would enter into your DNS records to access these networks:

**Network** | **IP Address**
:-----------|:--------------|
global-045|45.154.183.183
global-103|103.107.226.226
",470
Composable Edge Cloud,Current Providers,"We have selected a range of compute providers to deliver:
* Global Reach and Local Proximity
* Multi Provider Redundancy
* Extreme Scalability

The many locations available from these providers form a heterogeneous compute footprint on which the Section endpoint clusters (and hence your applications) may be run.

Current compute providers in the Composable Edge Cloud (CEC) include:
* Lumen
* DigitalOcean
* Equinix
* AWS
* GCP
* Azure
* Rackcorp

Section is constantly reviewing additional providers to add to the CEC and will update this list as new providers are added.",124
Composable Edge Cloud,Special Use CEC Options,"In addition to targeting workload generally at the Section CEC and letting the Section [Adaptive Edge Engine](/explanations/aee/) optimize the placement of the workload across the entire network, workload can be targeted at a subset of the compute locations for Special Use Options by declaring specific placement policies through the [Kubernetes API](/explanations/kubernetes/).

Examples may include restricting workload placement to specific locations, regions or providers in the Section CEC to achieve your desired performance, cost, security and compliance outcomes.

Read more about [how to define placement policies with Section](/guides/projects/set-edge-locations/).

In addition, compute locations and providers can be added to the CEC by customers to achieve their product, security or compliance goals. Once added to the CEC, those locations can be specifically identified for the customer’s workload by the CEC and workload addressed to that network by the AEE. [Contact Us](https://www.section.io/contact-us/) to find out more about bringing custom locations to the CEC.",210
Composable Edge Cloud,What is Section’s Composable Edge Cloud (CEC)?,"
Section provides a malleable edge deployment surface on which your application can run consisting of a global network of compute locations.

The CEC includes the ability compose your custom edge cloud by
* Restricting your application to specific providers or geo locations for specific use cases; or
* Bringing additional locations and providers to your application edge network.

## Current Providers

We have selected a range of compute providers to deliver:
* Global Reach and Local Proximity
* Multi Provider Redundancy
* Extreme Scalability

The many locations available from these providers form a heterogeneous compute footprint on which the Section endpoint clusters (and hence your applications) may be run.

Current compute providers in the Composable Edge Cloud (CEC) include:
* Lumen
* DigitalOcean
* Equinix
* AWS
* GCP
* Azure
* Rackcorp

Section is constantly reviewing additional providers to add to the CEC and will update this list as new providers are added.

## Special Use CEC Options

In addition to targeting workload generally at the Section CEC and letting the Section [Adaptive Edge Engine](/explanations/aee/) optimize the placement of the workload across the entire network, workload can be targeted at a subset of the compute locations for Special Use Options by declaring specific placement policies through the [Kubernetes API](/explanations/kubernetes/).

Examples may include restricting workload placement to specific locations, regions or providers in the Section CEC to achieve your desired performance, cost, security and compliance outcomes.

Read more about [how to define placement policies with Section](/guides/projects/set-edge-locations/).

In addition, compute locations and providers can be added to the CEC by customers to achieve their product, security or compliance goals. Once added to the CEC, those locations can be specifically identified for the customer’s workload by the CEC and workload addressed to that network by the AEE. [Contact Us](https://www.section.io/contact-us/) to find out more about bringing custom locations to the CEC.
",414
Persistent Volumes,Persistent Storage on Section,"Application developers are able to use standard Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) to give their applications persistent storage that lives beyond the lifetime of a pod. Without persistent volumes, pods are able to read and write data to [ephemeral disk](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/), but the lifecycle of such data ends when the pod terminates. So a database of any kind placed on an ephmeral disk only lasts as long as the pod and then it is gone. Persistent storage solves this problem by giving data a lifecycle longer than that of a single pod.

Persistent storage can be used for:
* horizontal scaling of a pod, so that the multiple replicas have access to common data, such as a cache
* different pods of a microservice application, giving those pods a common source of truth for whatever data they might need
* data that needs to survive a pod that crashes and restarts
* a database for your distributed application, such as [Postgres](/tutorials/data/postgres-on-pvc), MySQL, SQLite, or others
* a document store
* a [persistent cache](https://www.varnish-software.com/solutions/varnish-enterprise/persistence/), which might be used with our [Varnish](/tutorials/varnish-caching.md) tutorial
* a KV store
* an object store, such as [MinIO](https://min.io/)",306
Persistent Volumes,How it Works,"Persistent volumes are managed by standard Kubernetes tooling, such as `kubectl` or .yaml files that contain the required fields and object specification. Section supports the storage class ""nfs-client"".

Persistent volumes in Section are created dynamically as a result of a claim: when your deployment to Section includes a [Persistent Volume Claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim), then a persistent volume is created dynamically. Then your pod mounts a volume with that claim to gain access to the data.

The claim may fail if resouces are not available. And writes to the volume can fail if you exceed the capacity.",136
Persistent Volumes,Managing Data Between Locations,"Persistent volumes are created within a cluster, within a Section location. Note that a persistent volume does not relocate with your project when [Section moves it due to changes in traffic](/explanations/aee.md). 

Strategies for sharing data between locations include configuring replication to occur between persistent volumes residing in different locations. We have a guide that explains how you can set this up (coming). You might choose to use a static location configuration for your project and then replicate data between them in order to provide a globally-distributed persistent data.

Strategies for making your persistent data available in new locations as Section creates them include the idea of continuously streaming changes to AWS S3, Azure Blob Storage, etc. [Litestream.io](https://litestream.io/) supports this idea for SQLite, allowing you to quickly restore data when your project becomes live in a new location.",176
Persistent Volumes,Note,"Section provisions the volume as resources allow. But the application developer is responsible for replication, data backup, disaster recovery, compliance and cryptographic requirements, and disk destruction.",32
Persistent Volumes,Persistent Storage on Section,"
# Persistent Storage on Section

Application developers are able to use standard Kubernetes [Persistent Volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) to give their applications persistent storage that lives beyond the lifetime of a pod. Without persistent volumes, pods are able to read and write data to [ephemeral disk](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/), but the lifecycle of such data ends when the pod terminates. So a database of any kind placed on an ephmeral disk only lasts as long as the pod and then it is gone. Persistent storage solves this problem by giving data a lifecycle longer than that of a single pod.

Persistent storage can be used for:
* horizontal scaling of a pod, so that the multiple replicas have access to common data, such as a cache
* different pods of a microservice application, giving those pods a common source of truth for whatever data they might need
* data that needs to survive a pod that crashes and restarts
* a database for your distributed application, such as [Postgres](/tutorials/data/postgres-on-pvc), MySQL, SQLite, or others
* a document store
* a [persistent cache](https://www.varnish-software.com/solutions/varnish-enterprise/persistence/), which might be used with our [Varnish](/tutorials/varnish-caching.md) tutorial
* a KV store
* an object store, such as [MinIO](https://min.io/)

## How it Works
Persistent volumes are managed by standard Kubernetes tooling, such as `kubectl` or .yaml files that contain the required fields and object specification. Section supports the storage class ""nfs-client"".

Persistent volumes in Section are created dynamically as a result of a claim: when your deployment to Section includes a [Persistent Volume Claim](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#lifecycle-of-a-volume-and-claim), then a persistent volume is created dynamically. Then your pod mounts a volume with that claim to gain access to the data.

The claim may fail if resouces are not available. And writes to the volume can fail if you exceed the capacity.

## Managing Data Between Locations

Persistent volumes are created within a cluster, within a Section location. Note that a persistent volume does not relocate with your project when [Section moves it due to changes in traffic](/explanations/aee.md). 

Strategies for sharing data between locations include configuring replication to occur between persistent volumes residing in different locations. We have a guide that explains how you can set this up (coming). You might choose to use a static location configuration for your project and then replicate data between them in order to provide a globally-distributed persistent data.

Strategies for making your persistent data available in new locations as Section creates them include the idea of continuously streaming changes to AWS S3, Azure Blob Storage, etc. [Litestream.io](https://litestream.io/) supports this idea for SQLite, allowing you to quickly restore data when your project becomes live in a new location.

### Note
Section provisions the volume as resources allow. But the application developer is responsible for replication, data backup, disaster recovery, compliance and cryptographic requirements, and disk destruction.",672
Adaptive Edge Engine,LocationOptimizer,The LocationOptimizer calls the **SolverServiceV1** strategy to solve for the locations of servers for your Project. This strategy implements various policies to deliver a variety of different deployment styles. You define parameters to pass to the policy to modify its operation to best deliver the desired results. The policies and their parameters are discussed in more detail below.,68
Adaptive Edge Engine,Dynamic,"This policy uses traffic data (e.g., http requests per second) to find an optimal set of locations for your Project subject to your parameter specifications. The optimization function selects locations to minimize the geographic distance travelled by the requests sent by your end users. With the dynamic policy, the selected set of locations is expected to change over time as your Project's traffic patterns change. See [here](/explanations/traffic-signal/) for more information on the traffic signal used for dynamic location selection. 

Special handling is required when the Project traffic data is NULL or 0. In cases of no traffic signal (e.g., the Project is not receiving any HTTP requests) the LocationOptimizer will meet the **minimumLocations** condition by selecting arbitrary locations. The **mustInclude** parameters will be honored if present, but if there are fewer of these than **minimumLocations**, additional locations will be chosen regardless of geography.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""dynamic""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
* Optional parameters
  * **minimumLocations**: minimumLocations ensures that the Project will be deployed to a minimum number of locations to ensure availability. Default is 2.
  * **maximumLocations**: maximumLocations can be used to set a coarse upper limit on the number of locations. Default is 5.
  * **mustInclude**: mustInclude conditions represent “include this in my solution” conditions. Multiple conditions can be specified as key:value pairs. The LocationOptimizer selects one location per condition specified in the mustInclude array. For example, mustInclude = [{""region"": ""europe""},{""region"": ""northamerica""},{""region"": ""northamerica""}] will result in 3 locations, one in Europe and 2 in North America. See table of available terms [here](/reference/solver-service-parameters/). With the dynamic policy, your selected locations may include additional locations determined by traffic. Default is NULL.
  * **mustNotInclude**: These represent conditions that must not appear in the solution set. They are specified in the same manner as the **mustInclude** conditions. Default is NULL.
  * **chooseFrom**: This feature is only available on the Enterprise plan.

A dynamic LocationOptimizer configuration specifying all parameters looks like this (note that **minimumLocations** and **maximumLocations** are equal to their defaults in this example):

```yaml title=""location-optimizer.yaml""
{
apiVersion: v1
kind: ConfigMap
metadata:
  name: location-optimizer
  namespace: default
data:
  strategy: |
    {
      ""strategy"":""SolverServiceV1"",
      ""params"": {
        ""policy"": ""dynamic"",
        ""mustInclude"": [
          {""region"":""europe""}
        ],
        ""mustNotInclude"": [
          {""region"":""asia""}
        ],
        ""minimumLocations"": 2,
        ""maximumLocations"": 5
      }
    }
}
```",612
Adaptive Edge Engine,Static,"This policy results in a fixed set of locations that meet additional, stated requirements. Deploys your Project to a fixed set of locations that meet the **mustInclude** conditions. Upon the first implementation of the static policy, the SolverService will solve for a set of locations that meets the specifications. This set will continue to be used as long as it meets the specifications. If it does not, as when your specifications or the underlying network have been changed, then a new set is obtained.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""static"", ""mustInclude"":[{""region"":""europe""},{""region"":""oceania""}]}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
  * **mustInclude**: In this policy, the mustInclude parameter is the only input used to define the desired result. mustInclude conditions represent “include this in my solution” conditions. Multiple conditions can be specified as key:value pairs. The LocationOptimizer selects one location per condition specified in the mustInclude array. For example, mustInclude = [{""region"": ""europe""},{""region"": ""northamerica""},{""region"": ""northamerica""}] will result in 3 locations, one in Europe and 2 in North America. See table of available terms [here](/reference/solver-service-parameters/).
* Optional parameters
  * **chooseFrom**: This feature is only available on the Enterprise plan.

#### Anycast
This policy is available on the Enterprise plan. It should only be used under specific circumstances and requires supporting actions such as changing your DNS records. See the [Anycast explanation](/explanations/anycast/) to know if and how you should use our Anycast networks.

This policy results in a set of locations that are part of our Anycast network.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""anycast"",""network"":""global-045""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
  * **network**: Indicates which Anycast IP space you are using in your DNS records as explained [here](/explanations/anycast/).
* Optional parameters
  * None",464
Adaptive Edge Engine,Development,"This policy results in a set of locations that are dedicated to non-production and micro-environments.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""development""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
* Optional parameters
  * None

## TrafficDirector
The TrafficDirector is responsible for routing traffic to edge deployments and it has multiple strategies it can execute to manage this. Two DNS-based strategies are currently available with the default being a geo-DNS strategy that selects routes based on geographic proximity.

## HealthChecker
The HealthChecker executes one or more strategies for each Project to determine if the Project has been deployed/scheduled successfully and is ready to accept traffic. The HealthChecker also executes additional background strategies to monitor the health of the locations hosting Projects.

Two strategies are currently available to the HealthChecker configuration. Those strategies are:
* **deploymentMetricsHealthCheck**: Monitors platform metrics to detect that the minimum replicas per container are running for each deployment.
* **envHTTPHealthCheck**: An agent queries the Project with an HTTP POST request and monitors and interprets the response

The deploymentMetricsHealthCheck is included by default for all Project. The envHTTPHealthCheck is included by default for HTTP Projects.",258
Adaptive Edge Engine,Learn About Section's Adaptive Edge Engine,"
The Adaptive Edge Engine (AEE) is a collection of components that manage different aspects of operating your Project on our dynamic, multi-provider, global hosting platform.

1. **LocationOptimizer** manages the selection of locations where the Projects run.
2. **HealthChecker** monitors Project health.
3. **TrafficDirector** manages the routing of traffic to healthy Projects wherever they are deployed. 

The above pieces work together in the following flow:
* The LocationOptimizer selects locations for your Project based upon your strategy definition in your location-optimizer ConfigMap (see below). The locations selected by the AEE may be ones where the workload is currently running, or they may be new ones.
* Next, the Project is deployed to any newly selected locations. The Project deployments are tested by the HealthChecker, and when they become healthy they are classified as ""ready to receive traffic"".
* Once an instance of a Project is ready, the TrafficDirector updates Internet routing information so traffic can be ""directed"" to instances of your Project. 

If the selected locations are the same as your current locations, you will see no changes.

Other components of the AEE manage additional dimensions of your Project in the background such as location orchestration and scaling.

## LocationOptimizer
The LocationOptimizer calls the **SolverServiceV1** strategy to solve for the locations of servers for your Project. This strategy implements various policies to deliver a variety of different deployment styles. You define parameters to pass to the policy to modify its operation to best deliver the desired results. The policies and their parameters are discussed in more detail below.

### Policy descriptions and examples

#### Dynamic
This policy uses traffic data (e.g., http requests per second) to find an optimal set of locations for your Project subject to your parameter specifications. The optimization function selects locations to minimize the geographic distance travelled by the requests sent by your end users. With the dynamic policy, the selected set of locations is expected to change over time as your Project's traffic patterns change. See [here](/explanations/traffic-signal/) for more information on the traffic signal used for dynamic location selection. 

Special handling is required when the Project traffic data is NULL or 0. In cases of no traffic signal (e.g., the Project is not receiving any HTTP requests) the LocationOptimizer will meet the **minimumLocations** condition by selecting arbitrary locations. The **mustInclude** parameters will be honored if present, but if there are fewer of these than **minimumLocations**, additional locations will be chosen regardless of geography.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""dynamic""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
* Optional parameters
  * **minimumLocations**: minimumLocations ensures that the Project will be deployed to a minimum number of locations to ensure availability. Default is 2.
  * **maximumLocations**: maximumLocations can be used to set a coarse upper limit on the number of locations. Default is 5.
  * **mustInclude**: mustInclude conditions represent “include this in my solution” conditions. Multiple conditions can be specified as key:value pairs. The LocationOptimizer selects one location per condition specified in the mustInclude array. For example, mustInclude = [{""region"": ""europe""},{""region"": ""northamerica""},{""region"": ""northamerica""}] will result in 3 locations, one in Europe and 2 in North America. See table of available terms [here](/reference/solver-service-parameters/). With the dynamic policy, your selected locations may include additional locations determined by traffic. Default is NULL.
  * **mustNotInclude**: These represent conditions that must not appear in the solution set. They are specified in the same manner as the **mustInclude** conditions. Default is NULL.
  * **chooseFrom**: This feature is only available on the Enterprise plan.

A dynamic LocationOptimizer configuration specifying all parameters looks like this (note that **minimumLocations** and **maximumLocations** are equal to their defaults in this example):

```yaml title=""location-optimizer.yaml""
{
apiVersion: v1
kind: ConfigMap
metadata:
  name: location-optimizer
  namespace: default
data:
  strategy: |
    {
      ""strategy"":""SolverServiceV1"",
      ""params"": {
        ""policy"": ""dynamic"",
        ""mustInclude"": [
          {""region"":""europe""}
        ],
        ""mustNotInclude"": [
          {""region"":""asia""}
        ],
        ""minimumLocations"": 2,
        ""maximumLocations"": 5
      }
    }
}
```

#### Static
This policy results in a fixed set of locations that meet additional, stated requirements. Deploys your Project to a fixed set of locations that meet the **mustInclude** conditions. Upon the first implementation of the static policy, the SolverService will solve for a set of locations that meets the specifications. This set will continue to be used as long as it meets the specifications. If it does not, as when your specifications or the underlying network have been changed, then a new set is obtained.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""static"", ""mustInclude"":[{""region"":""europe""},{""region"":""oceania""}]}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
  * **mustInclude**: In this policy, the mustInclude parameter is the only input used to define the desired result. mustInclude conditions represent “include this in my solution” conditions. Multiple conditions can be specified as key:value pairs. The LocationOptimizer selects one location per condition specified in the mustInclude array. For example, mustInclude = [{""region"": ""europe""},{""region"": ""northamerica""},{""region"": ""northamerica""}] will result in 3 locations, one in Europe and 2 in North America. See table of available terms [here](/reference/solver-service-parameters/).
* Optional parameters
  * **chooseFrom**: This feature is only available on the Enterprise plan.

#### Anycast
This policy is available on the Enterprise plan. It should only be used under specific circumstances and requires supporting actions such as changing your DNS records. See the [Anycast explanation](/explanations/anycast/) to know if and how you should use our Anycast networks.

This policy results in a set of locations that are part of our Anycast network.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""anycast"",""network"":""global-045""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
  * **network**: Indicates which Anycast IP space you are using in your DNS records as explained [here](/explanations/anycast/).
* Optional parameters
  * None

#### Development
This policy results in a set of locations that are dedicated to non-production and micro-environments.

```json
{""strategy"":""SolverServiceV1"",""params"":{""policy"":""development""}}
```

* Required parameters
  * **policy**: This is required to obtain the desired solution methods.
* Optional parameters
  * None

## TrafficDirector
The TrafficDirector is responsible for routing traffic to edge deployments and it has multiple strategies it can execute to manage this. Two DNS-based strategies are currently available with the default being a geo-DNS strategy that selects routes based on geographic proximity.

## HealthChecker
The HealthChecker executes one or more strategies for each Project to determine if the Project has been deployed/scheduled successfully and is ready to accept traffic. The HealthChecker also executes additional background strategies to monitor the health of the locations hosting Projects.

Two strategies are currently available to the HealthChecker configuration. Those strategies are:
* **deploymentMetricsHealthCheck**: Monitors platform metrics to detect that the minimum replicas per container are running for each deployment.
* **envHTTPHealthCheck**: An agent queries the Project with an HTTP POST request and monitors and interprets the response

The deploymentMetricsHealthCheck is included by default for all Project. The envHTTPHealthCheck is included by default for HTTP Projects.
",1672
Traffic Signal,Metric,The base metric is the count of incoming HTTP requests per second (rps) collected at the ingress of the environment. The metric has labels for the 2-digit geohash encompassing the geo-ip of the end-user. The metric identifies the incoming traffic rate per 2-digit geohash quadrangle.,63
Traffic Signal,Processing,"The metric is smoothed with a moving average of 2 minute increments over a 30 minute span. In cases of low traffic, the smoothing span is extended to 24 hours to ensure that a stable signal is obtained. The ""low traffic"" condition is met when the total traffic rate for the environment is less than the **minimumUsagePerLocation** * **minimumLocations**. When total traffic is less than that product, the solution is dominated by the **minimumLocations** parameter and the challenge is to identify a stable selection of locations over time. At low levels, the traffic signal is prone to a low signal:noise ratio. Minor variations in traffic rate, variation that is not systematically related to valuable end-user behavior, can induce flapping in the location selections. To ameliorate this, we extend the smoothing period for low-traffic environments to enable better signal qualities and better location optimization.",181
Traffic Signal,A reference of how the traffic signal is processed for consumption by the LocationOptimizer,"
For LocationOptimizer ConfigMaps that have **policy**: dynamic, the LocationOptimizer needs a traffic signal to select locations. The following describes the traffic signal currently in use.

# Metric
The base metric is the count of incoming HTTP requests per second (rps) collected at the ingress of the environment. The metric has labels for the 2-digit geohash encompassing the geo-ip of the end-user. The metric identifies the incoming traffic rate per 2-digit geohash quadrangle. 

# Processing
The metric is smoothed with a moving average of 2 minute increments over a 30 minute span. In cases of low traffic, the smoothing span is extended to 24 hours to ensure that a stable signal is obtained. The ""low traffic"" condition is met when the total traffic rate for the environment is less than the **minimumUsagePerLocation** * **minimumLocations**. When total traffic is less than that product, the solution is dominated by the **minimumLocations** parameter and the challenge is to identify a stable selection of locations over time. At low levels, the traffic signal is prone to a low signal:noise ratio. Minor variations in traffic rate, variation that is not systematically related to valuable end-user behavior, can induce flapping in the location selections. To ameliorate this, we extend the smoothing period for low-traffic environments to enable better signal qualities and better location optimization.
",285
Recommended Partners,Security,"| Partner                                                                  | Function                 | Description                                                                                 |
|--------------------------------------------------------------------------|--------------------------|---------------------------------------------------------------------------------------------|
| [PerimeterX](https://www.perimeterx.com)                                 | Bot management           | Non-human bot traffic detection and management.                                             |
| [Radware Bot Manager](https://www.radwarebotmanager.com)                 | Bot management           | Non-human bot traffic detection and management.                                             |
| [Signal Sciences](https://www.signalsciences.com)                        | Web application firewall | Real-time protection for an application under attack and integrates into DevOps toolchains. |
| [ThreatX](https://www.threatx.com)                                       | Web application firewall | Web application firewall based on dynamic rules.                                            |
| [Wallarm](https://www.wallarm.com)                                       | Web application firewall | Web application firewall based on dynamic rules.                                            |
| [QuantWAF](https://www.section.io/docs/classic/guides/modules/quantwaf/) | Web application firewall | Web application firewall based on dynamic rules.                                            |",214
Recommended Partners,Performance and Optimization,"| Partner                                | Function           | Description                                          |
|----------------------------------------|--------------------|------------------------------------------------------|
| [Optidash](https://optidash.ai)        | Image optimization | Optimize images to reduce page weight and load time. |
| [SiteSpect](https://www.sitespect.com) | A/B testing        | JavaScript and tag free A/B testing.                 |",79
Recommended Partners,See a list of recommended partners from Section and learn more about their tools,"
Section customers have worked with partners who offer easy to deploy solutions for Section edge environments.

[Contact us](https://www.section.io/contact-us/) or the recommended partner directly to secure a license to run the
partner solution on Section.

## Security

| Partner                                                                  | Function                 | Description                                                                                 |
|--------------------------------------------------------------------------|--------------------------|---------------------------------------------------------------------------------------------|
| [PerimeterX](https://www.perimeterx.com)                                 | Bot management           | Non-human bot traffic detection and management.                                             |
| [Radware Bot Manager](https://www.radwarebotmanager.com)                 | Bot management           | Non-human bot traffic detection and management.                                             |
| [Signal Sciences](https://www.signalsciences.com)                        | Web application firewall | Real-time protection for an application under attack and integrates into DevOps toolchains. |
| [ThreatX](https://www.threatx.com)                                       | Web application firewall | Web application firewall based on dynamic rules.                                            |
| [Wallarm](https://www.wallarm.com)                                       | Web application firewall | Web application firewall based on dynamic rules.                                            |
| [QuantWAF](https://www.section.io/docs/classic/guides/modules/quantwaf/) | Web application firewall | Web application firewall based on dynamic rules.                                            |

## Performance and Optimization

| Partner                                | Function           | Description                                          |
|----------------------------------------|--------------------|------------------------------------------------------|
| [Optidash](https://optidash.ai)        | Image optimization | Optimize images to reduce page weight and load time. |
| [SiteSpect](https://www.sitespect.com) | A/B testing        | JavaScript and tag free A/B testing.                 |
",349
Support,Platform Status,View current platform status on our [Status Page](https://status.section.io/) to be alerted to any platform wide issues.,25
Support,Community Support,"Join our [Slack Community](https://sectionio-community.slack.com/) and chat about all things Edge, Apps, Kubernetes and the rest of the stuff you need help with.",37
Support,Lodge a Support Request,"In your Section Console, contact us for help or with any suggestions.

![Settings](/img/docs/support.png)",23
Support,Other Questions?,You can always contact us here at Section by dropping us a line from [our Contact Form](https://www.section.io/contact-us/),27
Support,Find the best help and support options,"
We have lots of ways for you to get the help and support you need to be successful with your application on Section.


## Platform Status

View current platform status on our [Status Page](https://status.section.io/) to be alerted to any platform wide issues.

## Community Support

Join our [Slack Community](https://sectionio-community.slack.com/) and chat about all things Edge, Apps, Kubernetes and the rest of the stuff you need help with.

## Lodge a Support Request

In your Section Console, contact us for help or with any suggestions.

![Settings](/img/docs/support.png)

## Other Questions?

You can always contact us here at Section by dropping us a line from [our Contact Form](https://www.section.io/contact-us/)",155
Security,SOC 2 Type II Compliance,"Section has successfully completed a System and Organization Controls (SOC) 2 Type II audit, performed by [Sensiba San Filippo, LLP](https://ssfllp.com/) (SSF).",40
Security,DDoS Protection,Network-layer DDoS protection is included by default across the entire Section network to protect against all Layer 3/4 attacks. Section’s DDoS protection includes dually redundant DDoS protection including two of the world’s largest DDoS networks.,52
Security,Container Isolation,Applications cannot view or access processes outside of their isolated environment.,12
Security,Namespace NetworkPolicy Control,Kubernetes NetworkPolicies restrict communications across namespaces.,10
Security,Private Repositories & Registries,"Maintain your application code, configuration and deployment manifests in your own code management systems and image registries.",21
Security,HTTP Extensions,"Section supports several containerized solutions that are available for general use by Section customers and include security-focused features.  These include:
* Activate IP blocking (via Section HTTP Ingress)
* Geo IP range blocking, and User Agent detection and blocking (via Varnish Cache)
* TLS Certificate Management (via Section HTTP Ingress)",66
Security,Geographic Delivery Control,Control delivery to locations consistent with your GDPR or other compliance requirements.,13
Security,Vendor Delivery Control,Restrict delivery nodes to a specific provider consistent with your compliance and security requirements.,16
Security,PCI Compliance,"Section is a certified PCI DSS Level 1 Service Provider.  Section utilizes [Tevora](https://www.tevora.com/) a Qualified Security Assessor (QSA) to conduct an annual compliance audit and provide a PCI DSS Attestation of Compliance (AOC). 

Section offers PCI DSS Level 1 Compliant Service as a premium service, enabling customers to build PCI-compliant systems that leverage all the benefits of Section.",92
Security,GDPR Compliance,Section’s privacy practices align to compliance with GDPR.,10
Security,Access Control,"* SSO
* API tokens",7
Security,Security options and information,"
Securing Section's global infrastructure and your applications is an important problem to solve and we take it seriously. Section’s security practice is led by our CISSP-qualified VP of Security and encompasses areas such as compliance protocols, corporate governance, data privacy, change management, and more. 

[Our comprehensive **Security Statement**](/docs/about/terms-of-service/security_compliance/) includes details with respect to all security and compliance factors at Section.

Please contact us at support@section.io if you have a security concern, or believe you’ve found a vulnerability in any part of our platform. 

## SOC 2 Type II Compliance
Section has successfully completed a System and Organization Controls (SOC) 2 Type II audit, performed by [Sensiba San Filippo, LLP](https://ssfllp.com/) (SSF). 

## DDoS Protection
Network-layer DDoS protection is included by default across the entire Section network to protect against all Layer 3/4 attacks. Section’s DDoS protection includes dually redundant DDoS protection including two of the world’s largest DDoS networks.

## Compute Framework Security
### Container Isolation
Applications cannot view or access processes outside of their isolated environment.

### Namespace NetworkPolicy Control
Kubernetes NetworkPolicies restrict communications across namespaces.

### Private Repositories & Registries
Maintain your application code, configuration and deployment manifests in your own code management systems and image registries.

## Security Platform Extensions
### HTTP Extensions
Section supports several containerized solutions that are available for general use by Section customers and include security-focused features.  These include:
* Activate IP blocking (via Section HTTP Ingress)
* Geo IP range blocking, and User Agent detection and blocking (via Varnish Cache)
* TLS Certificate Management (via Section HTTP Ingress)

## Additional Security Features

### Geographic Delivery Control
Control delivery to locations consistent with your GDPR or other compliance requirements.

### Vendor Delivery Control
Restrict delivery nodes to a specific provider consistent with your compliance and security requirements.

### PCI Compliance
Section is a certified PCI DSS Level 1 Service Provider.  Section utilizes [Tevora](https://www.tevora.com/) a Qualified Security Assessor (QSA) to conduct an annual compliance audit and provide a PCI DSS Attestation of Compliance (AOC). 

Section offers PCI DSS Level 1 Compliant Service as a premium service, enabling customers to build PCI-compliant systems that leverage all the benefits of Section.

### GDPR Compliance
Section’s privacy practices align to compliance with GDPR.

### Access Control
* SSO
* API tokens
",535
Service Level Agreement,"Contract Entered After January 1, 2022",(for contract entered prior to Jan 1 2022 see relevant [terms and conditions](/docs/about/terms-of-service/sla/#contract-entered-prior-to-january-1-2022)),42
Service Level Agreement,SERVICE COMMITMENT,"Section will use commercially reasonable efforts to make available the Customer’s Application Edge Network (defined below) with a Monthly Uptime Percentage (defined below) of at least 99.99% during any Measurement Period (the “Service Commitment”). In the event Section does not meet the Service Commitment, Customer will be eligible to receive a Service Credit as described below.",73
Service Level Agreement,DEFINITIONS,"- ""Excluded Downtime” means:
    - Any unavailability caused by circumstances beyond Section’s reasonable control, including, for example, an act of God, act of government, flood, fire, earthquake, civil unrest, act of terror, strike, or other labor problem (other than one involving Section employees), Internet service provider failure or delay, non-Section service or application, or denial of service attack, any force majeure event, Internet access or related problems beyond the demarcation point of Section.
    - Any unavailability that results from any actions or inactions of Customer or any third party.
    - Any unavailability that results from Customer’s equipment, software or other technology and/or third-party equipment, software or other technology (other than third party equipment within our direct control); 
- “Monthly Uptime Percentage” is calculated by subtracting from 100% the percentage of minutes during the applicable Measurement Period in which all Utilized PoPs running Customer applications had no external connectivity to the Internet as a result of circumstances other than Excluded Downtime.   
- A “Service Credit” is a dollar credit, calculated as set forth below, that Section may credit back to a Customer account.  
- A “Utilized PoP” is a PoP to which the Customer’s application is deployed.
- A “PoP” is any Section infrastructure location available for hosting Customer applications.
- “Customer’s Application Edge Network” is defined as a Customer specification of 2 or more Utilized PoPs.
- “Measurement Period” means a calendar month.",320
Service Level Agreement,SERVICE CREDIT,"Service Credits are calculated as a percentage of the total monthly services charges (other than professional service fees) paid by Customer for the Section service(s) that did not meet the Service Commitment for the Measurement Period in which the impact occurred in accordance with the schedule below.

| Monthly Uptime Percentage              | Service Credit Percentage           | 
|----------------------------------------|--------------------|
| Less than 99.99% but greater than or equal to 99.0% | 10% |
| Less than 99.0% but greater than or equal to 95.0% | 25% |
| Less than 95.0% | 100% | 

Section will apply any Service Credits for the impacted application(s) for the applicable Measurement Period, in the amount specified in the table above. A Service Credit will be applicable and issued only if the credit amount for the applicable monthly billing cycle is greater than one dollar ($1 USD). Unless otherwise provided, Customer’s sole and exclusive remedy for any unavailability, non-performance, or other failure by Section to meet the Uptime Percentage is the receipt of a Service Credit (if eligible) in accordance with the terms of this SLA.",239
Service Level Agreement,CREDIT REQUESTS AND PAYMENT PROCEDURES,"To receive a Service Credit, Customer must submit a request to Section (support@section.io). To be eligible, the credit request must be received by us by the end of the second calendar month (UTC) after which the incident occurred.

If the Monthly Uptime Percentage applicable to the Measurement Period of such a request is confirmed by Section, then Section will issue the Service Credit to Customer within one Measurement Period following the Measurement Period in which Customer’s request is confirmed by Section.",94
Service Level Agreement,SUMMARY,"Section utilizes multiple, redundant data centres to ensure the Service is always available to you. Section shall use all reasonable commercial efforts to limit an outage to less than 30 minutes in a calendar month. In the unlikely event that an outage exceeds 30 minutes, you will be eligible to receive a credit as set forth below.

Section Service Level Agreement declares that the Section Network will be available 100% of the time. If Section fails to meet this SLA during any given calendar month, Customer's account will be credited.

Upon Customer's request, Section will issue a credit to Customer for Network Outages in an amount equal to one day’s worth of the Monthly Fee paid by Customer, multiplied by 10 minute period (or portion thereof rounded to the next hour) of the cumulative duration of such Network Outages during a particular month.",167
Service Level Agreement,DEFINITIONS,"1. ""Section Network"" means the Section owned and operated website acceleration equipment.

2. ""Network Outage"" means a period of time that the Section Network was not available to deliver content to the internet for 2 or more consecutive 2 minute periods.

3. ""Monthly Fee"" consists solely of the base monthly fee paid by Customer for the affected Section service.",74
Service Level Agreement,CREDIT REQUEST AND PAYMENT PROCEDURES,"Each request in connection with a Network Outage must be received by Section within thirty (30) days of the date the SLA was not met.

Each valid credit will be applied to an invoice of Customer no later than two billing cycles after Section receipt and verification of Customer's request. Credits are exclusive of any applicable taxes, duties, fees, or surcharges imposed by any controlling taxing authority.

The total amount credited to Customer shall not exceed the Monthly Fee paid by Customer for such month",97
Service Level Agreement,EXCEPTIONS,"An Outage does not include website inaccessibility due to:

1. Failure or errors with your hardware, network, or website code;

2. DNS issues beyond Section’s direct control;

3. Your failure to report an Outage to Section support once you have discovered it;

4. Section scheduled or emergency maintenance;

5. Any actions or inactions of you or any third parties that are outside of Section’s direct control; or

6. Other events outside of Section’s reasonable control, including but not limited to Force Majeure.",109
Service Level Agreement,TERMINATION,"If you experience Outages for three consecutive calendar months, then either Party may terminate the Subscription Agreement in the following (fourth) month. Your Credits for Outages, which shall survive termination, shall be your sole and exclusive remedy and Section’s sole and exclusive liability for any failure to maintain the availability of the Services, or any claim based on an issue covered by this SLA. If you repeatedly submit false or unsubstantiated claims for Credits, Section may terminate your Subscription Agreement.",97
Service Level Agreement,Service Level Agreement,"
# Service Level Agreement
## Contract Entered After January 1, 2022
(for contract entered prior to Jan 1 2022 see relevant [terms and conditions](/docs/about/terms-of-service/sla/#contract-entered-prior-to-january-1-2022))

### SERVICE COMMITMENT
Section will use commercially reasonable efforts to make available the Customer’s Application Edge Network (defined below) with a Monthly Uptime Percentage (defined below) of at least 99.99% during any Measurement Period (the “Service Commitment”). In the event Section does not meet the Service Commitment, Customer will be eligible to receive a Service Credit as described below.

### DEFINITIONS
- ""Excluded Downtime” means:
    - Any unavailability caused by circumstances beyond Section’s reasonable control, including, for example, an act of God, act of government, flood, fire, earthquake, civil unrest, act of terror, strike, or other labor problem (other than one involving Section employees), Internet service provider failure or delay, non-Section service or application, or denial of service attack, any force majeure event, Internet access or related problems beyond the demarcation point of Section.
    - Any unavailability that results from any actions or inactions of Customer or any third party.
    - Any unavailability that results from Customer’s equipment, software or other technology and/or third-party equipment, software or other technology (other than third party equipment within our direct control); 
- “Monthly Uptime Percentage” is calculated by subtracting from 100% the percentage of minutes during the applicable Measurement Period in which all Utilized PoPs running Customer applications had no external connectivity to the Internet as a result of circumstances other than Excluded Downtime.   
- A “Service Credit” is a dollar credit, calculated as set forth below, that Section may credit back to a Customer account.  
- A “Utilized PoP” is a PoP to which the Customer’s application is deployed.
- A “PoP” is any Section infrastructure location available for hosting Customer applications.
- “Customer’s Application Edge Network” is defined as a Customer specification of 2 or more Utilized PoPs.
- “Measurement Period” means a calendar month.

### SERVICE CREDIT
Service Credits are calculated as a percentage of the total monthly services charges (other than professional service fees) paid by Customer for the Section service(s) that did not meet the Service Commitment for the Measurement Period in which the impact occurred in accordance with the schedule below.

| Monthly Uptime Percentage              | Service Credit Percentage           | 
|----------------------------------------|--------------------|
| Less than 99.99% but greater than or equal to 99.0% | 10% |
| Less than 99.0% but greater than or equal to 95.0% | 25% |
| Less than 95.0% | 100% | 

Section will apply any Service Credits for the impacted application(s) for the applicable Measurement Period, in the amount specified in the table above. A Service Credit will be applicable and issued only if the credit amount for the applicable monthly billing cycle is greater than one dollar ($1 USD). Unless otherwise provided, Customer’s sole and exclusive remedy for any unavailability, non-performance, or other failure by Section to meet the Uptime Percentage is the receipt of a Service Credit (if eligible) in accordance with the terms of this SLA. 

### CREDIT REQUESTS AND PAYMENT PROCEDURES
To receive a Service Credit, Customer must submit a request to Section (support@section.io). To be eligible, the credit request must be received by us by the end of the second calendar month (UTC) after which the incident occurred.

If the Monthly Uptime Percentage applicable to the Measurement Period of such a request is confirmed by Section, then Section will issue the Service Credit to Customer within one Measurement Period following the Measurement Period in which Customer’s request is confirmed by Section.





# Service Level Agreement
## Contract Entered Prior to January 1, 2022

##### SUMMARY
Section utilizes multiple, redundant data centres to ensure the Service is always available to you. Section shall use all reasonable commercial efforts to limit an outage to less than 30 minutes in a calendar month. In the unlikely event that an outage exceeds 30 minutes, you will be eligible to receive a credit as set forth below.

Section Service Level Agreement declares that the Section Network will be available 100% of the time. If Section fails to meet this SLA during any given calendar month, Customer's account will be credited.

Upon Customer's request, Section will issue a credit to Customer for Network Outages in an amount equal to one day’s worth of the Monthly Fee paid by Customer, multiplied by 10 minute period (or portion thereof rounded to the next hour) of the cumulative duration of such Network Outages during a particular month.

##### DEFINITIONS
1. ""Section Network"" means the Section owned and operated website acceleration equipment.

2. ""Network Outage"" means a period of time that the Section Network was not available to deliver content to the internet for 2 or more consecutive 2 minute periods.

3. ""Monthly Fee"" consists solely of the base monthly fee paid by Customer for the affected Section service.

##### CREDIT REQUEST AND PAYMENT PROCEDURES
Each request in connection with a Network Outage must be received by Section within thirty (30) days of the date the SLA was not met.

Each valid credit will be applied to an invoice of Customer no later than two billing cycles after Section receipt and verification of Customer's request. Credits are exclusive of any applicable taxes, duties, fees, or surcharges imposed by any controlling taxing authority.

The total amount credited to Customer shall not exceed the Monthly Fee paid by Customer for such month

##### EXCEPTIONS
An Outage does not include website inaccessibility due to:

1. Failure or errors with your hardware, network, or website code;

2. DNS issues beyond Section’s direct control;

3. Your failure to report an Outage to Section support once you have discovered it;

4. Section scheduled or emergency maintenance;

5. Any actions or inactions of you or any third parties that are outside of Section’s direct control; or

6. Other events outside of Section’s reasonable control, including but not limited to Force Majeure.

##### TERMINATION
If you experience Outages for three consecutive calendar months, then either Party may terminate the Subscription Agreement in the following (fourth) month. Your Credits for Outages, which shall survive termination, shall be your sole and exclusive remedy and Section’s sole and exclusive liability for any failure to maintain the availability of the Services, or any claim based on an issue covered by this SLA. If you repeatedly submit false or unsubstantiated claims for Credits, Section may terminate your Subscription Agreement.
",1402
Acceptable Use Policy,A. THE FOLLOWING ACTIVITIES ARE EXPRESSLY PROHIBITED:,"1. Spam / Unsolicited Commercial Email.
Section does not tolerate spam, whether legal or illegal.
Bulk unsolicited commercial email is automatically considered spam, unless sent to recipients who have requested such messages through an opt-in procedure.
Take no action that leads to a Spamhaus listing for any Section-provided IP address, including your own IP addresses. If any action on your part causes a Spamhaus listing, you are required promptly to cooperate with Section, at your own expense, in clearing the IP address(es) in question from the Spamhaus RBL (and such assistance does not limit any other rights or remedies of Section).
Do not fake e-mail header information to misdirect or otherwise ""spoof"" another party for any reason, including without limitation to evade detection.
Do not set up a mail relay that is not password protected or that could allow anyone to send spam.
Do not host “payload” websites that are linked to spam emails, even if a third party sent those emails.
Do not provide domain name services to entities that violate any of these rules.

2. Intellectual property infringement, including violations of copyright, trademark, and patent rights, and use or distribution of pirated software.

3. Hacking or perpetration of any security breach.

4. Unauthorized access to any computer or system, including intrusion into or scanning of other Section accounts.

5. Dissemination of deliberately offensive material, including any message or information that is or may be threatening, libelous, obscene, or harassing.

6. Child pornography or any other activity harmful to minors.

7. Fraud.

8. Violation of privacy rights, publicity rights, trade secret rights, or information security, including dissemination of material non-public information about companies without authorization, as well as harvesting or collecting information about Website visitors without their express consent.

9. Use of third party software without a proper license or other appropriate permission, including Microsoft software.

10. Network attacks or any unfriendly effort to interfere with service on another network or server.

11. Virus distribution or distribution of any worm or other harmful code.

12. Use of Internet relay chat (“IRC”), including hosting of an IRC server, running IRC bots, use of a Section server as an IRC client or proxy, and use of IRC scripts or programs that interfere with service to other users on any server or network.

13. Illegal activities of any kind.",478
Acceptable Use Policy,B. DISRUPTIONS & SECURITY:,"Section may suspend the Service in whole or in part if it reasonably suspects an AUP violation. Customer will reimburse Section for any expenses resulting from Customer’s violation of the AUP, including legal fees. Section may also disable Customer’s service if Section suspects that such service is the target of an attack or in any way interferes with services provided to other customers, even if Customer is not at fault. Section does not issue refunds for terminating service due to any of the causes above. Customer is responsible for the use of its service, including use by hackers and other unauthorized third parties. Customer’s responsibility includes payment for exceeding transfer and bandwidth limits.

Except where Section specifically accepts such responsibility in Customer’s Signup (pursuant to the Terms of Service), Customer is responsible for maintaining security, including disaster recovery systems and backups.",163
Acceptable Use Policy,C. REPORTING AUP VIOLATIONS:,"Section requests that anyone with information about a violation of this AUP, or of Section’s Terms of Service, report it by contacting Section at its then publicised contact details.",35
Acceptable Use Policy,Acceptable Use Policy,"Together with the Terms of Service and the other documents referenced therein, this AUP forms a binding agreement (the ""Agreement"") between Section and Customer. The Agreement, including this AUP, binds all customers and users of Section's Service and all visitors to Section's website; all such customers and users are included in the term ""Customer."" All other terms defined in the Terms of Service will have the same meaning when used in this AUP.

Section requires that all Customers conduct themselves as good citizens of the Internet and at all times behave with respect for others. Each Customer is responsible for preventing violations of this AUP by third parties accessing the Service through Customer's computers or accounts, including without limitation hackers and Customer's own users.

### A. THE FOLLOWING ACTIVITIES ARE EXPRESSLY PROHIBITED:

1. Spam / Unsolicited Commercial Email.
Section does not tolerate spam, whether legal or illegal.
Bulk unsolicited commercial email is automatically considered spam, unless sent to recipients who have requested such messages through an opt-in procedure.
Take no action that leads to a Spamhaus listing for any Section-provided IP address, including your own IP addresses. If any action on your part causes a Spamhaus listing, you are required promptly to cooperate with Section, at your own expense, in clearing the IP address(es) in question from the Spamhaus RBL (and such assistance does not limit any other rights or remedies of Section).
Do not fake e-mail header information to misdirect or otherwise ""spoof"" another party for any reason, including without limitation to evade detection.
Do not set up a mail relay that is not password protected or that could allow anyone to send spam.
Do not host “payload” websites that are linked to spam emails, even if a third party sent those emails.
Do not provide domain name services to entities that violate any of these rules.

2. Intellectual property infringement, including violations of copyright, trademark, and patent rights, and use or distribution of pirated software.

3. Hacking or perpetration of any security breach.

4. Unauthorized access to any computer or system, including intrusion into or scanning of other Section accounts.

5. Dissemination of deliberately offensive material, including any message or information that is or may be threatening, libelous, obscene, or harassing.

6. Child pornography or any other activity harmful to minors.

7. Fraud.

8. Violation of privacy rights, publicity rights, trade secret rights, or information security, including dissemination of material non-public information about companies without authorization, as well as harvesting or collecting information about Website visitors without their express consent.

9. Use of third party software without a proper license or other appropriate permission, including Microsoft software.

10. Network attacks or any unfriendly effort to interfere with service on another network or server.

11. Virus distribution or distribution of any worm or other harmful code.

12. Use of Internet relay chat (“IRC”), including hosting of an IRC server, running IRC bots, use of a Section server as an IRC client or proxy, and use of IRC scripts or programs that interfere with service to other users on any server or network.

13. Illegal activities of any kind.

### B. DISRUPTIONS & SECURITY:
Section may suspend the Service in whole or in part if it reasonably suspects an AUP violation. Customer will reimburse Section for any expenses resulting from Customer’s violation of the AUP, including legal fees. Section may also disable Customer’s service if Section suspects that such service is the target of an attack or in any way interferes with services provided to other customers, even if Customer is not at fault. Section does not issue refunds for terminating service due to any of the causes above. Customer is responsible for the use of its service, including use by hackers and other unauthorized third parties. Customer’s responsibility includes payment for exceeding transfer and bandwidth limits.

Except where Section specifically accepts such responsibility in Customer’s Signup (pursuant to the Terms of Service), Customer is responsible for maintaining security, including disaster recovery systems and backups.

### C. REPORTING AUP VIOLATIONS:
Section requests that anyone with information about a violation of this AUP, or of Section’s Terms of Service, report it by contacting Section at its then publicised contact details.
",861
Privacy Policy,WHAT THIS PRIVACY POLICY COVERS,"This policy covers how Section treats personal information that Section collects and receives, including through the www.Section web site (the “Site”). As the Section site is only to be accessed by persons 18 years or older, we do not intend to collect any personal information from children under 13. Personal information is information about you that is personally identifiable, including your name, email address, billing address, phone number, credit card information, birth date, and other information, that is not otherwise publicly available. This policy does not apply to the practices of companies that Section does not own or control, or to people that Section does not employ or manage.",129
Privacy Policy,WHY WE COLLECT PERSONAL INFORMATION,"Section collects your personal information because it helps us deliver a superior level of customer service. It enables us to give you convenient access to and/or ordering of our products and services, as well as to focus on categories of greatest interest to you. In addition, your personal information helps us keep you posted on the latest product announcements, software updates, special offers, and events that you might like to hear about. Our emails to you should always contain an unsubscribe option.",92
Privacy Policy,INFORMATION COLLECTION AND USE,"Section may ask for your personal information when you’re purchasing a product, registering a product, participating in a forum, discussing service issues, downloading a software update, registering for an event or participating in an online survey. In such cases, we may collect personal information relevant to the situation, such as your name, mailing address, phone number, email address, and contact information; your credit card information and information about the products you own; and information relating to a support or service issue. Section gathers certain generic information with respect to your use of the Site, such as the frequency with which you visit the Site, the applications and the services you use, and the areas on the Site that you favour. We only use this type of data in aggregate – that is, we look at the data on a collective basis, in summary form, rather than on an individual basis. This data helps us determine the extent to which our customers use certain parts of our Site which, in turn, enables us to make it as appealing as possible. We may also provide statistical information about how our customers collectively use our Site to partners, advertisers, sponsors, and other companies with which we do business. We do this so they, too, can understand how often people use their areas of the Site in order for them to provide you with the best possible experience. This statistical information does not contain any personally identifiable information about you or any of our users. We may use also personal information to provide products that you have requested as well as for auditing, research, and analysis to improve our products.

Section may set and access cookies on your computer. A cookie is a small text file containing a unique identification number that is transferred from a Web site to the hard drive of your computer so that the site administrator may identify your computer and passively track its activities on the Site. This unique number identifies your web browser to our computer system. A cookie will not allow a Web site to learn any personally identifiable information (such as your real name and address) that you have not otherwise disclosed. Cookies allow us to automatically remember your web browser when you visit our site or Service. The use of cookies is an industry standard, and they are currently used on most major Web sites. It is possible to adjust your Web browser preferences to alert you when a cookie is sent to your hard drive, or to refuse cookies altogether. While we do not require you to use cookies, keep in mind that certain sites and services may not function properly if you set your browser to refuse all cookies.",508
Privacy Policy,DISCLOSURE EXCEPTIONS,"Any information given to us shall be stored and managed with our best possible care, and will not be used in any ways to which you have not consented. We will not sell, rent or exchange such personality identifiable information to any other organization or entities, unless the user is first notified and expressly agreed to. Notwithstanding the above, we may indeed disclose personal information if required to do so in response to legal process, such as a court order or subpoena, or when such disclosure is made for establishment or exercise of legal rights or in defending against legal claim or as otherwise required by law. And, as we mention above, we may share with aggregated statistical information about the use of Section, its services and other future services.",145
Privacy Policy,MINORS,"If a parent believes that his or her minor chilled has submitted personal information to us, he or she can contact us via e-mail or postal mail as listed below. We believe that parents should supervise their children’s online activities and consider using parental control tools available from online services and software manufacturers that help provide a kid-friendly online environment. These tools can also prevent children from otherwise disclosing online their name, address and other personal information without parental permission.",88
Privacy Policy,THIRD PARTY SITES,"Links on this Site to other Web sites are provided as a convenience to you. Such linked sites are outside our control and responsibility and are not covered by this policy. If you use any such linked sites, you should consult the privacy policies posted on those Web sites.  Our site contains links to other Web sites. We are not responsible for the privacy practices or the content of such Web sites. Our site may also link to Web sites that feature our trademarks and names along with trademarks and names of other companies. You should look at the privacy policy on that co-branded Web site, as the co-branded Web site may not be under our control.

Applications you use or download from our site may contain third party digital rights management systems (“DRMS”) which may allow for communication between your software and the third party and utilize security features (e.g., preventing distribution of or access to the applications in the event of unauthorized use). These DRMS are subject to their own license agreements and you agree that we shall not be responsible for any loss or damage of any sort relating to the use of the DRMS or your dealings with such third parties. The applications may also contain DRMS produced by us which allow for communication between the applications you use or download and our systems and which collect information describing your computer system in order to prevent illegal or unauthorized use of the applications.

For reasons such as improving user experience and providing customized communications to our users, we may receive information about you from third party sources and our Web logs and add it to the information that we have received from you. Web logs automatically record anything a Web server sees, which may include e-mail addresses you enter into a form or pages viewed by a user at a particular IP address.",348
Privacy Policy,INFORMATION SHARING,"We will not share your personally identifiable information with third parties, aside from entities that perform services for us, such as fulfilling orders or processing payment, that either are bound to comply with our privacy policy or have privacy policies that protect your information unless you have ‘opted-in’ to such sharing. If you have previously opted-in to such sharing under a prior privacy policy version, you are still considered to have opted-in under this privacy policy. As stated, and whether or not you have opted-in, we may use third parties to accept and process orders and payments for merchandise and products, including software, and such third parties may get access to your personal information for the purposes of providing services or products to you on our behalf. In addition, if you opt-out, we may share that information with third parties who send e-mails on our behalf so that they do not e-mail you.

However, we may disclose information you provide if required to do so by law or if we have a good faith belief that disclosure is necessary to (1) comply with the law or with legal process served on us; (2) protect and defend our rights or property; or (3) act in an emergency to protect someone’s safety.

We may request demographic information from you (for example, your age, education level or household income) from time to time. We will not share that information in a manner that identifies you as an individual with any other entity, unless we let you know that at the time of collection or we have your permission. When we share demographic information with third parties, we will give them aggregate information only.",324
Privacy Policy,SALE OF ASSETS,"In the event that Section is ever sold, acquired, merged, liquidated, reorganized, or otherwise transferred, we reserve the right to transfer our user databases together with any personally identifiable information contained therein, to a third-party acquiring Section’s assets.",50
Privacy Policy,CHANGES TO THIS PRIVACY POLICY,"We may change our privacy policy from time to time by updating the posting, provided however that you will always know what information we gather, how we might use that information and whether we will disclose it to anyone.",42
Privacy Policy,Privacy Policy,"
Section cares about your privacy and those of others.

### WHAT THIS PRIVACY POLICY COVERS
This policy covers how Section treats personal information that Section collects and receives, including through the www.Section web site (the “Site”). As the Section site is only to be accessed by persons 18 years or older, we do not intend to collect any personal information from children under 13. Personal information is information about you that is personally identifiable, including your name, email address, billing address, phone number, credit card information, birth date, and other information, that is not otherwise publicly available. This policy does not apply to the practices of companies that Section does not own or control, or to people that Section does not employ or manage.

### WHY WE COLLECT PERSONAL INFORMATION
Section collects your personal information because it helps us deliver a superior level of customer service. It enables us to give you convenient access to and/or ordering of our products and services, as well as to focus on categories of greatest interest to you. In addition, your personal information helps us keep you posted on the latest product announcements, software updates, special offers, and events that you might like to hear about. Our emails to you should always contain an unsubscribe option.

### INFORMATION COLLECTION AND USE
Section may ask for your personal information when you’re purchasing a product, registering a product, participating in a forum, discussing service issues, downloading a software update, registering for an event or participating in an online survey. In such cases, we may collect personal information relevant to the situation, such as your name, mailing address, phone number, email address, and contact information; your credit card information and information about the products you own; and information relating to a support or service issue. Section gathers certain generic information with respect to your use of the Site, such as the frequency with which you visit the Site, the applications and the services you use, and the areas on the Site that you favour. We only use this type of data in aggregate – that is, we look at the data on a collective basis, in summary form, rather than on an individual basis. This data helps us determine the extent to which our customers use certain parts of our Site which, in turn, enables us to make it as appealing as possible. We may also provide statistical information about how our customers collectively use our Site to partners, advertisers, sponsors, and other companies with which we do business. We do this so they, too, can understand how often people use their areas of the Site in order for them to provide you with the best possible experience. This statistical information does not contain any personally identifiable information about you or any of our users. We may use also personal information to provide products that you have requested as well as for auditing, research, and analysis to improve our products.

Section may set and access cookies on your computer. A cookie is a small text file containing a unique identification number that is transferred from a Web site to the hard drive of your computer so that the site administrator may identify your computer and passively track its activities on the Site. This unique number identifies your web browser to our computer system. A cookie will not allow a Web site to learn any personally identifiable information (such as your real name and address) that you have not otherwise disclosed. Cookies allow us to automatically remember your web browser when you visit our site or Service. The use of cookies is an industry standard, and they are currently used on most major Web sites. It is possible to adjust your Web browser preferences to alert you when a cookie is sent to your hard drive, or to refuse cookies altogether. While we do not require you to use cookies, keep in mind that certain sites and services may not function properly if you set your browser to refuse all cookies.

### DISCLOSURE EXCEPTIONS
Any information given to us shall be stored and managed with our best possible care, and will not be used in any ways to which you have not consented. We will not sell, rent or exchange such personality identifiable information to any other organization or entities, unless the user is first notified and expressly agreed to. Notwithstanding the above, we may indeed disclose personal information if required to do so in response to legal process, such as a court order or subpoena, or when such disclosure is made for establishment or exercise of legal rights or in defending against legal claim or as otherwise required by law. And, as we mention above, we may share with aggregated statistical information about the use of Section, its services and other future services.

### MINORS
If a parent believes that his or her minor chilled has submitted personal information to us, he or she can contact us via e-mail or postal mail as listed below. We believe that parents should supervise their children’s online activities and consider using parental control tools available from online services and software manufacturers that help provide a kid-friendly online environment. These tools can also prevent children from otherwise disclosing online their name, address and other personal information without parental permission.

### THIRD PARTY SITES
Links on this Site to other Web sites are provided as a convenience to you. Such linked sites are outside our control and responsibility and are not covered by this policy. If you use any such linked sites, you should consult the privacy policies posted on those Web sites.  Our site contains links to other Web sites. We are not responsible for the privacy practices or the content of such Web sites. Our site may also link to Web sites that feature our trademarks and names along with trademarks and names of other companies. You should look at the privacy policy on that co-branded Web site, as the co-branded Web site may not be under our control.

Applications you use or download from our site may contain third party digital rights management systems (“DRMS”) which may allow for communication between your software and the third party and utilize security features (e.g., preventing distribution of or access to the applications in the event of unauthorized use). These DRMS are subject to their own license agreements and you agree that we shall not be responsible for any loss or damage of any sort relating to the use of the DRMS or your dealings with such third parties. The applications may also contain DRMS produced by us which allow for communication between the applications you use or download and our systems and which collect information describing your computer system in order to prevent illegal or unauthorized use of the applications.

For reasons such as improving user experience and providing customized communications to our users, we may receive information about you from third party sources and our Web logs and add it to the information that we have received from you. Web logs automatically record anything a Web server sees, which may include e-mail addresses you enter into a form or pages viewed by a user at a particular IP address.

### INFORMATION SHARING
We will not share your personally identifiable information with third parties, aside from entities that perform services for us, such as fulfilling orders or processing payment, that either are bound to comply with our privacy policy or have privacy policies that protect your information unless you have ‘opted-in’ to such sharing. If you have previously opted-in to such sharing under a prior privacy policy version, you are still considered to have opted-in under this privacy policy. As stated, and whether or not you have opted-in, we may use third parties to accept and process orders and payments for merchandise and products, including software, and such third parties may get access to your personal information for the purposes of providing services or products to you on our behalf. In addition, if you opt-out, we may share that information with third parties who send e-mails on our behalf so that they do not e-mail you.

However, we may disclose information you provide if required to do so by law or if we have a good faith belief that disclosure is necessary to (1) comply with the law or with legal process served on us; (2) protect and defend our rights or property; or (3) act in an emergency to protect someone’s safety.

We may request demographic information from you (for example, your age, education level or household income) from time to time. We will not share that information in a manner that identifies you as an individual with any other entity, unless we let you know that at the time of collection or we have your permission. When we share demographic information with third parties, we will give them aggregate information only.

### SALE OF ASSETS
In the event that Section is ever sold, acquired, merged, liquidated, reorganized, or otherwise transferred, we reserve the right to transfer our user databases together with any personally identifiable information contained therein, to a third-party acquiring Section’s assets.

### CHANGES TO THIS PRIVACY POLICY
We may change our privacy policy from time to time by updating the posting, provided however that you will always know what information we gather, how we might use that information and whether we will disclose it to anyone.",1801
Security and Compliance,SOC 2 TYPE II COMPLIANCE,"Section has successfully completed a System and Organization Controls (SOC) 2 Type II audit, performed by [Sensiba San Filippo, LLP](https://ssfllp.com/) (SSF). Developed by the American Institute of Certified Public Accountants (AICPA), the SOC 2 information security standard is an audit report on the examination of controls relevant to the trust services criteria categories covering security, availability, processing integrity, confidentiality and privacy. A SOC 2 Type II report describes a service organization's systems and whether the design of specified controls meets the relevant trust services categories at a point-in-time. Section’s SOC 2 Type II report did not have any noted exceptions and therefore was issued with a “clean” audit opinion from SSF.

![SOC2 logo](/img/docs/soc2-logo.png)

We leverage [Drata](https://drata.com/)'s compliance automation platform to continuously monitor and report on security controls across the organization, ensuring that we are always meeting and exceeding the most up-to-date and highest security standards.",212
Security and Compliance,PCI DSS COMPLIANCE,"Section is a certified PCI DSS Level 1 Service Provider.  Section utilizes [Tevora](https://www.tevora.com/) a Qualified Security Assessor (QSA) to conduct an annual compliance audit and provide a PCI DSS Attestation of Compliance (AOC).   Developed by the Payment Card Industry Security Standards Council, the PCI DSS standard was created to increase controls around cardholder data and to reduce credit card fraud. The AOC report describes a service organization’s systems and whether the design of specified controls protect cardholder data. An annual AOC is important to Section because it is an independent assessment that assures customers that Section meets all of our PCI compliance requirements.  If you wish to receive a copy of Sections AOC please work with your sales representative.  Section requires an NDA signed by senior management to receive a copy of our AOC.",178
Security and Compliance,CORPORATE GOVERNANCE,"**Policies.** The Section security policies and procedures are focused on preserving security in our systems, processes, and practices.

**Information Security Team.** Our security team works across the entire business to secure and protect any sensitive information related to the Section service. This team also formally reviews policies and procedures.

**Risk Management.** Section undertakes risk assessment practices to understand, prevent and manage and information security risks.",83
Security and Compliance,PEOPLE,"**Employee Screening.** Section screens new employees before they join the Section team and these screening activities may include criminal background and reference checks.

**Confidentiality.** Section employees are required to agree to protect and preserve any information they may view, process, or transmit as part of their job functions where that information may be deemed sensitive.

**Security Training.** Section trains our team to protect sensitive information and the devices they use. This training will include new hire awareness training and annual or ad-hoc training as required.",104
Security and Compliance,DATA PRIVACY,"**Privacy Policy.** The Section [privacy policy](/docs/about/terms-of-service/privacy/) describes how Section will collect, use, and protect the customer information where customer may use or interact with Section websites or configuring services in the Section Console.

**Personal Data Transfer.** See our [terms of service](/docs/about/terms-of-service/terms-and-conditions/) for additional information about regarding processing of personal data. The Section services by default do not process personal data.",95
Security and Compliance,CHANGE MANAGEMENT,"**Change Processes.** Section follows a procedural process when developing and deploying changes to technologies. Changes considered include systems and software which form the Section service

**Change Testing.** During the stages of development, Section will test changes. In advance of moving a proposed change to a production system, Section's team will confirm tests are successful.

**Change Notification.** Section prepare change notices to maintain awareness among the team. These notices are reviewed and approved by relevant team members involved with system management.

**Change Review.** Following the introduction of changes to the production systems, Section review and agree changes have been successful.",122
Security and Compliance,ACCESS MANAGEMENT,"**Access Requests.** Section documents requests for access to the Section systems. Our team responsible for security, will then approve and grant access only where appropriate.

**Access Management.** Section amend any employees' levels of access to the Section systems subject to any change in an employee's role and/or responsibilities at Section.

**Access Review Process.** Section reviews access levels across the team and systems to ensure the appropriate access to Section systems and data is maintained.",91
Security and Compliance,SYSTEM AUTHENTICATION AND ACCOUNT ACCESS,"**User Accounts and Privileges.** Section assigns unique accounts per user to each user who needs to access Section systems so we can manage, understand and enforce user-level accountability.

**User Roles and Access Privileges.** Roles and access levels are assigned to users to restrict access per system to the level required for each individual users to conduct their responsibilities.

**Two factor authentication.** Section systems require two factor authentication.",83
Security and Compliance,APPLICATION SECURITY,**Secure development practices and processes.** Section trains our development and operations teams to prevent common vulnerabilities.,20
Security and Compliance,NETWORK AND INFRASTRUCTURE,"**Security Scans and Tests.** Section performs vulnerability scans and security tests on the Section systems. Section considers and deals with the findings of these scans and tests in an appropriate way in order to assist with maintaining system security.

**Standards for System Configuration.** For maintenance of system security, Section has documented Standards for System Configuration which the team is required to follow. These standards cover a range of system configuration elements including (but not limited to) ports, services and protocols.

**Security Patching.** Section monitors lists of security vulnerabilities so that when new items are raised, those vulnerabilities can be addressed immediately. Patches and updates are applied subject to the time frame necessary for criticality of the vulnerability identified.",143
Security and Compliance,DATA ENCRYPTION,"**Encrypted Data Transmission.** Section's platform supports TLS and will provide customers with a solution to encrypt connections to end users and to the customers' origin servers.

**Encryption Keys.** To maintain security of customer Encryption Keys, Section protects access to the encryption keys provisioned by Section customers.",59
Security and Compliance,CONTINUITY AND AVAILABILITY,"**Fail Over.** Section's network is built to support fail over of traffic from any individual delivery node (PoP) within a network should that PoP become unavailable for any reason. In addition, Section can move customer data to alternate networks without any interruption should for some reason an individual network provided by Section become unresponsive.

**Redundancy.** Leveraging major cloud providers such as AWS and Azure, Section has multiple services and peering access points available to the Section networks.

**Monitoring.** Section's operations teams monitor a wide range of alerting interfaces to detect, monitor and understand degraded or otherwise detrimentally affected services 24x7x365.

**Reporting.** Section keep customers updated using real time alerting tools and methods (such as status.section.io). For specific customer issues, Section may contact a customer directly.",169
Security and Compliance,INCIDENTS,"**Response Plan.** Section has a documented response plan to bring to bear in the case of an incident on the Section platform or systems. The plan is reviewed and updated subject to the changing nature of the Section platform and threat profile of the Internet. the plan include communication processes, systems and team management.

**Notifications of Unauthorized Access.** Section will notify customers who may be affected by any validated breach of the Section systems or any unauthorized disclosure of that customer's confidential information.",95
Security and Compliance,"ANALYSIS, MONITORING AND DETECTION","**Analysis.** Section aggregate and securely store logs reflecting the activity on Section systems. Section monitor these logs to understand, alert, diagnose and manage security threats and or incidents.

**Monitoring.** Section use a number of systems to track system changes and ensure accountability and enforcement of the Section security standards.

**Detection.** Section has systems to help surface potential threats, incidents and intrusions. The Section team will be alerted to anomalies in these detection systems.",91
Security and Compliance,CUSTOMER DATA,"**Cached Data.** Temporarily cached data (what data, where and for how long) is managed by Section customers. From time to time, Section may directly manage these settings on behalf of the customer should the need arise per law as customers permit.

**IP Addresses.** Section may retain indefinitely any non-anonymized, non-aggregated client or subscriber IP addresses associated with suspicious activity that may pose a risk to the Section network or our customers, or that are associated with administrative connections to the Section service.

**HTTP Requests.** Customer and end-user content which passes through the Section network in response to requests launched by end-users creates data in the Section systems which Section use over time to monitor and manage the Section system reliability, availability, performance and security.

**Physical Security.** Section relies on major cloud infrastructure providers such as AWS and Azure which have the physical environment security which accompanies these standards.

**Business Continuity.** Section has deployed PoPs across a number of zones and networks and as such can seamlessly move customer traffic between nodes and/or networks without customer downtime.",216
Security and Compliance,QUESTIONS?,"If you have any questions about security on the Section website, please email us at support@section.io.

Last updated: 28th June 2022",30
Security and Compliance,Security and Compliance,"
Section's systems and processes are built and managed with the highest and most current security standards in mind. Our security practice encompasses areas such as compliance protocols, corporate governance, data privacy, change management, and more. 

### SOC 2 TYPE II COMPLIANCE
Section has successfully completed a System and Organization Controls (SOC) 2 Type II audit, performed by [Sensiba San Filippo, LLP](https://ssfllp.com/) (SSF). Developed by the American Institute of Certified Public Accountants (AICPA), the SOC 2 information security standard is an audit report on the examination of controls relevant to the trust services criteria categories covering security, availability, processing integrity, confidentiality and privacy. A SOC 2 Type II report describes a service organization's systems and whether the design of specified controls meets the relevant trust services categories at a point-in-time. Section’s SOC 2 Type II report did not have any noted exceptions and therefore was issued with a “clean” audit opinion from SSF.

![SOC2 logo](/img/docs/soc2-logo.png)

We leverage [Drata](https://drata.com/)'s compliance automation platform to continuously monitor and report on security controls across the organization, ensuring that we are always meeting and exceeding the most up-to-date and highest security standards.

### PCI DSS COMPLIANCE
Section is a certified PCI DSS Level 1 Service Provider.  Section utilizes [Tevora](https://www.tevora.com/) a Qualified Security Assessor (QSA) to conduct an annual compliance audit and provide a PCI DSS Attestation of Compliance (AOC).   Developed by the Payment Card Industry Security Standards Council, the PCI DSS standard was created to increase controls around cardholder data and to reduce credit card fraud. The AOC report describes a service organization’s systems and whether the design of specified controls protect cardholder data. An annual AOC is important to Section because it is an independent assessment that assures customers that Section meets all of our PCI compliance requirements.  If you wish to receive a copy of Sections AOC please work with your sales representative.  Section requires an NDA signed by senior management to receive a copy of our AOC.

### CORPORATE GOVERNANCE
**Policies.** The Section security policies and procedures are focused on preserving security in our systems, processes, and practices.

**Information Security Team.** Our security team works across the entire business to secure and protect any sensitive information related to the Section service. This team also formally reviews policies and procedures.

**Risk Management.** Section undertakes risk assessment practices to understand, prevent and manage and information security risks.

### PEOPLE
**Employee Screening.** Section screens new employees before they join the Section team and these screening activities may include criminal background and reference checks.

**Confidentiality.** Section employees are required to agree to protect and preserve any information they may view, process, or transmit as part of their job functions where that information may be deemed sensitive.

**Security Training.** Section trains our team to protect sensitive information and the devices they use. This training will include new hire awareness training and annual or ad-hoc training as required.

### DATA PRIVACY
**Privacy Policy.** The Section [privacy policy](/docs/about/terms-of-service/privacy/) describes how Section will collect, use, and protect the customer information where customer may use or interact with Section websites or configuring services in the Section Console.

**Personal Data Transfer.** See our [terms of service](/docs/about/terms-of-service/terms-and-conditions/) for additional information about regarding processing of personal data. The Section services by default do not process personal data.

### CHANGE MANAGEMENT
**Change Processes.** Section follows a procedural process when developing and deploying changes to technologies. Changes considered include systems and software which form the Section service

**Change Testing.** During the stages of development, Section will test changes. In advance of moving a proposed change to a production system, Section's team will confirm tests are successful.

**Change Notification.** Section prepare change notices to maintain awareness among the team. These notices are reviewed and approved by relevant team members involved with system management.

**Change Review.** Following the introduction of changes to the production systems, Section review and agree changes have been successful.

### ACCESS MANAGEMENT
**Access Requests.** Section documents requests for access to the Section systems. Our team responsible for security, will then approve and grant access only where appropriate.

**Access Management.** Section amend any employees' levels of access to the Section systems subject to any change in an employee's role and/or responsibilities at Section.

**Access Review Process.** Section reviews access levels across the team and systems to ensure the appropriate access to Section systems and data is maintained.

### SYSTEM AUTHENTICATION AND ACCOUNT ACCESS
**User Accounts and Privileges.** Section assigns unique accounts per user to each user who needs to access Section systems so we can manage, understand and enforce user-level accountability.

**User Roles and Access Privileges.** Roles and access levels are assigned to users to restrict access per system to the level required for each individual users to conduct their responsibilities.

**Two factor authentication.** Section systems require two factor authentication.

### APPLICATION SECURITY
**Secure development practices and processes.** Section trains our development and operations teams to prevent common vulnerabilities.

### NETWORK AND INFRASTRUCTURE
**Security Scans and Tests.** Section performs vulnerability scans and security tests on the Section systems. Section considers and deals with the findings of these scans and tests in an appropriate way in order to assist with maintaining system security.

**Standards for System Configuration.** For maintenance of system security, Section has documented Standards for System Configuration which the team is required to follow. These standards cover a range of system configuration elements including (but not limited to) ports, services and protocols.

**Security Patching.** Section monitors lists of security vulnerabilities so that when new items are raised, those vulnerabilities can be addressed immediately. Patches and updates are applied subject to the time frame necessary for criticality of the vulnerability identified.

### DATA ENCRYPTION
**Encrypted Data Transmission.** Section's platform supports TLS and will provide customers with a solution to encrypt connections to end users and to the customers' origin servers.

**Encryption Keys.** To maintain security of customer Encryption Keys, Section protects access to the encryption keys provisioned by Section customers.

### CONTINUITY AND AVAILABILITY
**Fail Over.** Section's network is built to support fail over of traffic from any individual delivery node (PoP) within a network should that PoP become unavailable for any reason. In addition, Section can move customer data to alternate networks without any interruption should for some reason an individual network provided by Section become unresponsive.

**Redundancy.** Leveraging major cloud providers such as AWS and Azure, Section has multiple services and peering access points available to the Section networks.

**Monitoring.** Section's operations teams monitor a wide range of alerting interfaces to detect, monitor and understand degraded or otherwise detrimentally affected services 24x7x365.

**Reporting.** Section keep customers updated using real time alerting tools and methods (such as status.section.io). For specific customer issues, Section may contact a customer directly.

### INCIDENTS
**Response Plan.** Section has a documented response plan to bring to bear in the case of an incident on the Section platform or systems. The plan is reviewed and updated subject to the changing nature of the Section platform and threat profile of the Internet. the plan include communication processes, systems and team management.

**Notifications of Unauthorized Access.** Section will notify customers who may be affected by any validated breach of the Section systems or any unauthorized disclosure of that customer's confidential information.

### ANALYSIS, MONITORING AND DETECTION
**Analysis.** Section aggregate and securely store logs reflecting the activity on Section systems. Section monitor these logs to understand, alert, diagnose and manage security threats and or incidents.

**Monitoring.** Section use a number of systems to track system changes and ensure accountability and enforcement of the Section security standards.

**Detection.** Section has systems to help surface potential threats, incidents and intrusions. The Section team will be alerted to anomalies in these detection systems.

### CUSTOMER DATA
**Cached Data.** Temporarily cached data (what data, where and for how long) is managed by Section customers. From time to time, Section may directly manage these settings on behalf of the customer should the need arise per law as customers permit.

**IP Addresses.** Section may retain indefinitely any non-anonymized, non-aggregated client or subscriber IP addresses associated with suspicious activity that may pose a risk to the Section network or our customers, or that are associated with administrative connections to the Section service.

**HTTP Requests.** Customer and end-user content which passes through the Section network in response to requests launched by end-users creates data in the Section systems which Section use over time to monitor and manage the Section system reliability, availability, performance and security.

**Physical Security.** Section relies on major cloud infrastructure providers such as AWS and Azure which have the physical environment security which accompanies these standards.

**Business Continuity.** Section has deployed PoPs across a number of zones and networks and as such can seamlessly move customer traffic between nodes and/or networks without customer downtime.

### QUESTIONS?
If you have any questions about security on the Section website, please email us at support@section.io.

Last updated: 28th June 2022",1938
Kubernetes Dashboard,Kubernetes Dashboard Section Provides the Kubernetes Dashboard for Basic Monitoring,"The open source Kubernetes Dashboard is deployed when you create your project and can be viewed with the ""Launch Dashboard"" button on the Project card. It is a general purpose, web-based UI for Kubernetes. It allows users to manage applications and troubleshoot them. You can read more about it on [GitHub](https://github.com/kubernetes/dashboard) or in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/).

![Kubernetes Dashboard](/img/docs/k8s-monitoring-dashboard.png)

We expect that most application developers will want to connect their favorite observability tooling to Section and we've provided [guides](/guides/monitor/exporting-telemetry/) to show how this is done.",150
Kubernetes Dashboard,Section Provides the Kubernetes Dashboard for Basic Monitoring,"
The open source Kubernetes Dashboard is deployed when you create your project and can be viewed with the ""Launch Dashboard"" button on the Project card. It is a general purpose, web-based UI for Kubernetes. It allows users to manage applications and troubleshoot them. You can read more about it on [GitHub](https://github.com/kubernetes/dashboard) or in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/).

![Kubernetes Dashboard](/img/docs/k8s-monitoring-dashboard.png)

We expect that most application developers will want to connect their favorite observability tooling to Section and we've provided [guides](/guides/monitor/exporting-telemetry/) to show how this is done.",151
Sample Datadog Dashboard,Datadog Dashboard,"Section provides a sample dashboard that you can import into your [Datadog account](https://app.datadoghq.com/) so that you can view metrics for your running projects.

![Datadog dashboard](/img/docs/datadog-dashboard.png)

In order to use it, first make a new dashboard.  Then copy the JSON below and paste it into the ""Import dashboard JSON..."" command that you'll find in the settings menu (gear icon) of your new dashboard.

```json title=""sample-dashboard.json""
{
   ""description"" : ""Example dashboard for monitoring workloads on Section."",
   ""id"" : ""vtr-j9w-3qj"",
   ""is_read_only"" : false,
   ""layout_type"" : ""ordered"",
   ""notify_list"" : [],
   ""reflow_type"" : ""fixed"",
   ""template_variables"" : [
      {
         ""available_values"" : [],
         ""default"" : ""*"",
         ""name"" : ""Environment_ID"",
         ""prefix"" : ""section_io_environment_id""
      },
      {
         ""available_values"" : [],
         ""default"" : ""*"",
         ""name"" : ""section_io_account_id"",
         ""prefix"" : ""section_io_account_id""
      }
   ],
   ""title"" : ""Section Demo"",
   ""widgets"" : [
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""Japan"",
                        ""formula"" : ""query2""
                     },
                     {
                        ""alias"" : ""US West"",
                        ""formula"" : ""query1""
                     },
                     {
                        ""alias"" : ""Europe West"",
                        ""formula"" : ""query3""
                     },
                     {
                        ""alias"" : ""Australia East"",
                        ""formula"" : ""query4""
                     },
                     {
                        ""alias"" : ""Australia West"",
                        ""formula"" : ""query5""
                     },
                     {
                        ""alias"" : ""Europe South"",
                        ""formula"" : ""query6""
                     },
                     {
                        ""alias"" : ""US East"",
                        ""formula"" : ""query7""
                     },
                     {
                        ""alias"" : ""Spain"",
                        ""formula"" : ""query8""
                     },
                     {
                        ""alias"" : ""Canada West"",
                        ""formula"" : ""query9""
                     },
                     {
                        ""alias"" : ""Canada East"",
                        ""formula"" : ""query10""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query2"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:x*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:9*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query3"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:u*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query4"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:r*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query5"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:q*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query6"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:s*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query7"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:d*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query8"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:e*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query9"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:c*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query10"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:f*,$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Traffic Rate by User Location (GeoHash)"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 1717949282816228,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 0
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""line"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""Memory Bytes"",
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_container_memory_usage_bytes{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Memory Usage"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 9006760416875348,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 4,
            ""y"" : 0
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{$Environment_ID,$section_io_account_id} by {traffic_monitor_region}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Traffic Rate by Hosting Region"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 7753833068014912,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 2
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""CPU Seconds"",
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_container_cpu_usage_seconds_total{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""CPU Usage"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 3621217572418110,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 4,
            ""y"" : 2
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_kube_pod_status_ready{$Environment_ID,$section_io_account_id} by {cluster_name}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Pod Replica Counts per PoP"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 7859494613804218,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 4
         }
      },
      {
         ""definition"" : {
            ""autoscale"" : true,
            ""precision"" : 2,
            ""requests"" : [
               {
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""aggregator"" : ""sum"",
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""avg:section.section_container_memory_usage_bytes{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""scalar""
               }
            ],
            ""time"" : {
               ""live_span"" : ""1mo""
            },
            ""timeseries_background"" : {
               ""type"" : ""area""
            },
            ""title"" : ""Past One Month Memory-(Hour??) Consumption"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""query_value""
         },
         ""id"" : 531138098289886,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 2,
            ""x"" : 4,
            ""y"" : 4
         }
      },
      {
         ""definition"" : {
            ""legend"" : {
               ""type"" : ""automatic""
            },
            ""requests"" : [
               {
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1"",
                        ""limit"" : {
                           ""order"" : ""desc""
                        }
                     }
                  ],
                  ""queries"" : [
                     {
                        ""aggregator"" : ""last"",
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_kube_pod_status_ready{$Environment_ID,$section_io_account_id} by {traffic_monitor_region}""
                     }
                  ],
                  ""response_format"" : ""scalar""
               }
            ],
            ""title"" : ""Current Pod Replica Counts per Hosting Region"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""sunburst""
         },
         ""id"" : 3390659206599044,
         ""layout"" : {
            ""height"" : 4,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 6
         }
      }
   ]
}
```",2967
Sample Datadog Dashboard,Importing and Using a Sample Datadog Dashboard,"## Datadog Dashboard
Section provides a sample dashboard that you can import into your [Datadog account](https://app.datadoghq.com/) so that you can view metrics for your running projects.

![Datadog dashboard](/img/docs/datadog-dashboard.png)

In order to use it, first make a new dashboard.  Then copy the JSON below and paste it into the ""Import dashboard JSON..."" command that you'll find in the settings menu (gear icon) of your new dashboard.

```json title=""sample-dashboard.json""
{
   ""description"" : ""Example dashboard for monitoring workloads on Section."",
   ""id"" : ""vtr-j9w-3qj"",
   ""is_read_only"" : false,
   ""layout_type"" : ""ordered"",
   ""notify_list"" : [],
   ""reflow_type"" : ""fixed"",
   ""template_variables"" : [
      {
         ""available_values"" : [],
         ""default"" : ""*"",
         ""name"" : ""Environment_ID"",
         ""prefix"" : ""section_io_environment_id""
      },
      {
         ""available_values"" : [],
         ""default"" : ""*"",
         ""name"" : ""section_io_account_id"",
         ""prefix"" : ""section_io_account_id""
      }
   ],
   ""title"" : ""Section Demo"",
   ""widgets"" : [
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""Japan"",
                        ""formula"" : ""query2""
                     },
                     {
                        ""alias"" : ""US West"",
                        ""formula"" : ""query1""
                     },
                     {
                        ""alias"" : ""Europe West"",
                        ""formula"" : ""query3""
                     },
                     {
                        ""alias"" : ""Australia East"",
                        ""formula"" : ""query4""
                     },
                     {
                        ""alias"" : ""Australia West"",
                        ""formula"" : ""query5""
                     },
                     {
                        ""alias"" : ""Europe South"",
                        ""formula"" : ""query6""
                     },
                     {
                        ""alias"" : ""US East"",
                        ""formula"" : ""query7""
                     },
                     {
                        ""alias"" : ""Spain"",
                        ""formula"" : ""query8""
                     },
                     {
                        ""alias"" : ""Canada West"",
                        ""formula"" : ""query9""
                     },
                     {
                        ""alias"" : ""Canada East"",
                        ""formula"" : ""query10""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query2"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:x*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:9*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query3"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:u*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query4"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:r*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query5"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:q*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query6"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:s*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query7"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:d*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query8"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:e*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query9"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:c*,$Environment_ID,$section_io_account_id}""
                     },
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query10"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{geo_hash:f*,$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Traffic Rate by User Location (GeoHash)"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 1717949282816228,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 0
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""line"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""Memory Bytes"",
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_container_memory_usage_bytes{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Memory Usage"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 9006760416875348,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 4,
            ""y"" : 0
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_http_request_count_total_sum_rate{$Environment_ID,$section_io_account_id} by {traffic_monitor_region}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Traffic Rate by Hosting Region"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 7753833068014912,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 2
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""alias"" : ""CPU Seconds"",
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_container_cpu_usage_seconds_total{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""CPU Usage"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 3621217572418110,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 4,
            ""y"" : 2
         }
      },
      {
         ""definition"" : {
            ""legend_columns"" : [
               ""avg"",
               ""min"",
               ""max"",
               ""value"",
               ""sum""
            ],
            ""legend_layout"" : ""auto"",
            ""requests"" : [
               {
                  ""display_type"" : ""bars"",
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_kube_pod_status_ready{$Environment_ID,$section_io_account_id} by {cluster_name}""
                     }
                  ],
                  ""response_format"" : ""timeseries"",
                  ""style"" : {
                     ""line_type"" : ""solid"",
                     ""line_width"" : ""normal"",
                     ""palette"" : ""dog_classic""
                  }
               }
            ],
            ""show_legend"" : true,
            ""title"" : ""Pod Replica Counts per PoP"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""timeseries""
         },
         ""id"" : 7859494613804218,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 4
         }
      },
      {
         ""definition"" : {
            ""autoscale"" : true,
            ""precision"" : 2,
            ""requests"" : [
               {
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1""
                     }
                  ],
                  ""queries"" : [
                     {
                        ""aggregator"" : ""sum"",
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""avg:section.section_container_memory_usage_bytes{$Environment_ID,$section_io_account_id}""
                     }
                  ],
                  ""response_format"" : ""scalar""
               }
            ],
            ""time"" : {
               ""live_span"" : ""1mo""
            },
            ""timeseries_background"" : {
               ""type"" : ""area""
            },
            ""title"" : ""Past One Month Memory-(Hour??) Consumption"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""query_value""
         },
         ""id"" : 531138098289886,
         ""layout"" : {
            ""height"" : 2,
            ""width"" : 2,
            ""x"" : 4,
            ""y"" : 4
         }
      },
      {
         ""definition"" : {
            ""legend"" : {
               ""type"" : ""automatic""
            },
            ""requests"" : [
               {
                  ""formulas"" : [
                     {
                        ""formula"" : ""query1"",
                        ""limit"" : {
                           ""order"" : ""desc""
                        }
                     }
                  ],
                  ""queries"" : [
                     {
                        ""aggregator"" : ""last"",
                        ""data_source"" : ""metrics"",
                        ""name"" : ""query1"",
                        ""query"" : ""sum:section.section_kube_pod_status_ready{$Environment_ID,$section_io_account_id} by {traffic_monitor_region}""
                     }
                  ],
                  ""response_format"" : ""scalar""
               }
            ],
            ""title"" : ""Current Pod Replica Counts per Hosting Region"",
            ""title_align"" : ""left"",
            ""title_size"" : ""16"",
            ""type"" : ""sunburst""
         },
         ""id"" : 3390659206599044,
         ""layout"" : {
            ""height"" : 4,
            ""width"" : 4,
            ""x"" : 0,
            ""y"" : 6
         }
      }
   ]
}
```",2973
Exporting to Grafana Cloud using Section,Monitoring with Grafana Cloud,"Here we provide a simple example of how to scrape Section metrics into Grafana Cloud by using a deployment of the Prometheus Agent on Section itself as a project separate from your production workload project. The basic idea is that you need to run a Prometheus Agent that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics for your entire account, and writes the results into Grafana Cloud using “remote write”. (At the time of this writing Grafana Cloud has no way to do the scraping itself.) We have [another guide](/guides/monitor/exporting-telemetry/grafana-docker) that shows how to do this with `docker run` if you prefer.  But here we show how to make a deployment to Section that runs 24x7 in a single location. A single Prometheus agent collects metrics for all projects in your account.

![Grafana](/img/docs/monitor-grafana.png)

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`:
  * Is of the form: https://…/api/prom/push",291
Exporting to Grafana Cloud using Section,Useful Links,"* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)
* Learn about how to setup your [Prometheus Agent](https://grafana.com/docs/grafana-cloud/metrics-prometheus/)",45
Exporting to Grafana Cloud using Section,Deployment,"The following deployment will run the Prometheus agent on Section. 

```yaml title=""grafana-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: grafanaagent
  name: grafanaagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafanaagent
  template:
    metadata:
      labels:
        app: grafanaagent
    spec:
      containers:
      - image: prom/prometheus
        imagePullPolicy: Always
        name: grafanaagent
        volumeMounts:
          - name: grafanaagent-config
            mountPath: /etc/prometheus
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
      volumes:
        - name: grafanaagent-config
          configMap:
            name: grafanaagent-config
```

Deploy it with `kubectl apply -f grafana-deployment.yaml`.

## Configuration
The following YAML file defines a ConfigMap with configuration for the Prometheus agent. Replace the following accordingly: `SECTION_ACCOUNT_ID`, `SECTION_API_TOKEN`, `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`, `GRAFANA_METRICS_INSTANCE_ID`, `GRAFANA_API_KEY`. Learn how to obtain the SECTION items [here](/guides/monitor/exporting-telemetry/metrics-endpoint/).

```yaml title=""configmap.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafanaagent-config
data:
  prometheus.yml: |
    # my global config
    global:
      scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).

      # Attach these labels to any time series or alerts when communicating with
      # external systems (federation, remote storage, Alertmanager).
      external_labels:
          monitor: 'section-monitor'

    # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
    rule_files:
      # - ""first.rules""
      # - ""second.rules""

    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
      - job_name: 'section-federation'
        metrics_path: '/prometheus/account/SECTION_ACCOUNT_ID/federate'
        params:
          'match[]':
            - '{__name__=~"".+""}'
        scheme: 'https'
        authorization:
          type: Bearer
          credentials: SECTION_API_TOKEN
        static_configs:
          - targets: ['console.section.io']
    remote_write:
    - url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
      basic_auth:
        username: GRAFANA_METRICS_INSTANCE_ID
        password: GRAFANA_API_KEY
```
Deploy it with `kubectl apply -f configmap.yaml`.",671
Exporting to Grafana Cloud using Section,Location Strategy,"By default, Section will run this project in 2 locations. We only need to collect metrics from your account once, so let's provide a location optimizer strategy that runs the project in only a single location. Read more about [location strategies](/explanations/aee/).
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```
Deploy it with `kubectl apply -f location-optimizer.yaml`. 

## View Metrics in Grafana
Login to your [Grafana Cloud account](https://grafana.com/products/cloud) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/grafana-sample-dashboard/) to get you started.",210
Exporting to Grafana Cloud using Section,Exporting to Grafana Cloud by Deploying Your Agent to Section,"## Monitoring with Grafana Cloud
Here we provide a simple example of how to scrape Section metrics into Grafana Cloud by using a deployment of the Prometheus Agent on Section itself as a project separate from your production workload project. The basic idea is that you need to run a Prometheus Agent that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics for your entire account, and writes the results into Grafana Cloud using “remote write”. (At the time of this writing Grafana Cloud has no way to do the scraping itself.) We have [another guide](/guides/monitor/exporting-telemetry/grafana-docker) that shows how to do this with `docker run` if you prefer.  But here we show how to make a deployment to Section that runs 24x7 in a single location. A single Prometheus agent collects metrics for all projects in your account.

![Grafana](/img/docs/monitor-grafana.png)

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`:
  * Is of the form: https://…/api/prom/push

### Useful Links
* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)
* Learn about how to setup your [Prometheus Agent](https://grafana.com/docs/grafana-cloud/metrics-prometheus/)

## Deployment
The following deployment will run the Prometheus agent on Section. 

```yaml title=""grafana-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: grafanaagent
  name: grafanaagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafanaagent
  template:
    metadata:
      labels:
        app: grafanaagent
    spec:
      containers:
      - image: prom/prometheus
        imagePullPolicy: Always
        name: grafanaagent
        volumeMounts:
          - name: grafanaagent-config
            mountPath: /etc/prometheus
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
      volumes:
        - name: grafanaagent-config
          configMap:
            name: grafanaagent-config
```

Deploy it with `kubectl apply -f grafana-deployment.yaml`.

## Configuration
The following YAML file defines a ConfigMap with configuration for the Prometheus agent. Replace the following accordingly: `SECTION_ACCOUNT_ID`, `SECTION_API_TOKEN`, `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`, `GRAFANA_METRICS_INSTANCE_ID`, `GRAFANA_API_KEY`. Learn how to obtain the SECTION items [here](/guides/monitor/exporting-telemetry/metrics-endpoint/).

```yaml title=""configmap.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafanaagent-config
data:
  prometheus.yml: |
    # my global config
    global:
      scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
      evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
      # scrape_timeout is set to the global default (10s).

      # Attach these labels to any time series or alerts when communicating with
      # external systems (federation, remote storage, Alertmanager).
      external_labels:
          monitor: 'section-monitor'

    # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
    rule_files:
      # - ""first.rules""
      # - ""second.rules""

    # A scrape configuration containing exactly one endpoint to scrape:
    # Here it's Prometheus itself.
    scrape_configs:
      # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
      - job_name: 'section-federation'
        metrics_path: '/prometheus/account/SECTION_ACCOUNT_ID/federate'
        params:
          'match[]':
            - '{__name__=~"".+""}'
        scheme: 'https'
        authorization:
          type: Bearer
          credentials: SECTION_API_TOKEN
        static_configs:
          - targets: ['console.section.io']
    remote_write:
    - url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
      basic_auth:
        username: GRAFANA_METRICS_INSTANCE_ID
        password: GRAFANA_API_KEY
```
Deploy it with `kubectl apply -f configmap.yaml`.

## Location Strategy
By default, Section will run this project in 2 locations. We only need to collect metrics from your account once, so let's provide a location optimizer strategy that runs the project in only a single location. Read more about [location strategies](/explanations/aee/).
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```
Deploy it with `kubectl apply -f location-optimizer.yaml`. 

## View Metrics in Grafana
Login to your [Grafana Cloud account](https://grafana.com/products/cloud) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/grafana-sample-dashboard/) to get you started.",1237
Exporting to Datadog using Docker,Monitoring with Datadog,"Here we provide a simple example of how to scrape Section metrics into Datadog with `docker run`. The basic idea is that you need to run a [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics, and writes the results into your Datadog cloud instance. You may run this agent on any infrastructure of your choice. It could be in a docker container on your PC, hosted on a cloud provider, or even [hosted at Section](/guides/monitor/exporting-telemetry/datadog-section/). A single agent is used to scrape all of the metrics for your account.

![Datadog](/img/docs/monitor-datadog.png)

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.",238
Exporting to Datadog using Docker,Useful Links,"* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Specifically, you are using the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/), so you will want to learn about that.",59
Exporting to Datadog using Docker,Configuration for a Datadog Agent Docker Container,"The following YAML file defines a docker container that will run the Datadog agent, scrape Section’s /federate endpoint, and write the results into your Datadog cloud instance.
* Replace `SECTION_ACCOUNT_ID` with your Section Account ID, typically of the form `1234`.
* Note that the ```Bearer TOKEN``` is not something that you are supposed to replace, just leave it alone.

```yaml title=""conf.yaml""
init_config:
instances:
  - namespace: section
    openmetrics_endpoint: https://console.section.io/prometheus/account/SECTION_ACCOUNT_ID/federate
    auth_token:
      reader:
        type: file
        path: /etc/datadog-agent/conf.d/openmetrics.d/token
        pattern: ^(.+)$
      writer:
        type: header
        name: Authorization
        value: ""Bearer <TOKEN>""
    metrics:
      - .+:
          type: gauge
```

The `token` file should be filled with one line containing your `SECTION_API_TOKEN`. Shown below is an obfuscated Section API token.

``` title=""token""
*********************************************************************4c57
```

The command to run the container above follows. Note the following:
* You'll need to properly specify the location of the files `datadog/openmetrics.d/conf.yaml` and `datadog/openmetrics.d/token`. In the example below they are specified as being in `/home/myhome`.
* And be sure to replace `DATADOG_API_KEY`.

```
docker run -d --name dd-agent \
    -v /var/run/docker.sock:/var/run/docker.sock:ro \
    -v /proc/:/host/proc/:ro \
    -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \
    -v /home/myhome/datadog/openmetrics.d/conf.yaml:/etc/datadog-agent/conf.d/openmetrics.d/conf.yaml \
    -v /home/myhome/datadog/openmetrics.d/token:/etc/datadog-agent/conf.d/openmetrics.d/token \
    -e DD_API_KEY=DATADOG_API_KEY \
    -e DD_SITE=""datadoghq.com"" \
    gcr.io/datadoghq/agent:7
```

If you are having trouble, try adding debugging to the list of command line arguments.

```
    -e DD_LOG_LEVEL=debug \
```",492
Exporting to Datadog using Docker,View Metrics in Datadog,Login to your [Datadog account](https://app.datadoghq.com/) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/datadog-sample-dashboard/) to get you started.,51
Exporting to Datadog using Docker,Support for Cloud-Native Monitoring Tools such as Datadog,"## Monitoring with Datadog
Here we provide a simple example of how to scrape Section metrics into Datadog with `docker run`. The basic idea is that you need to run a [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics, and writes the results into your Datadog cloud instance. You may run this agent on any infrastructure of your choice. It could be in a docker container on your PC, hosted on a cloud provider, or even [hosted at Section](/guides/monitor/exporting-telemetry/datadog-section/). A single agent is used to scrape all of the metrics for your account.

![Datadog](/img/docs/monitor-datadog.png)

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.

### Useful Links
* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Specifically, you are using the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/), so you will want to learn about that.

### Configuration for a Datadog Agent Docker Container
The following YAML file defines a docker container that will run the Datadog agent, scrape Section’s /federate endpoint, and write the results into your Datadog cloud instance.
* Replace `SECTION_ACCOUNT_ID` with your Section Account ID, typically of the form `1234`.
* Note that the ```Bearer TOKEN``` is not something that you are supposed to replace, just leave it alone.

```yaml title=""conf.yaml""
init_config:
instances:
  - namespace: section
    openmetrics_endpoint: https://console.section.io/prometheus/account/SECTION_ACCOUNT_ID/federate
    auth_token:
      reader:
        type: file
        path: /etc/datadog-agent/conf.d/openmetrics.d/token
        pattern: ^(.+)$
      writer:
        type: header
        name: Authorization
        value: ""Bearer <TOKEN>""
    metrics:
      - .+:
          type: gauge
```

The `token` file should be filled with one line containing your `SECTION_API_TOKEN`. Shown below is an obfuscated Section API token.

``` title=""token""
*********************************************************************4c57
```

The command to run the container above follows. Note the following:
* You'll need to properly specify the location of the files `datadog/openmetrics.d/conf.yaml` and `datadog/openmetrics.d/token`. In the example below they are specified as being in `/home/myhome`.
* And be sure to replace `DATADOG_API_KEY`.

```
docker run -d --name dd-agent \
    -v /var/run/docker.sock:/var/run/docker.sock:ro \
    -v /proc/:/host/proc/:ro \
    -v /sys/fs/cgroup/:/host/sys/fs/cgroup:ro \
    -v /home/myhome/datadog/openmetrics.d/conf.yaml:/etc/datadog-agent/conf.d/openmetrics.d/conf.yaml \
    -v /home/myhome/datadog/openmetrics.d/token:/etc/datadog-agent/conf.d/openmetrics.d/token \
    -e DD_API_KEY=DATADOG_API_KEY \
    -e DD_SITE=""datadoghq.com"" \
    gcr.io/datadoghq/agent:7
```

If you are having trouble, try adding debugging to the list of command line arguments.

```
    -e DD_LOG_LEVEL=debug \
```
## View Metrics in Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/datadog-sample-dashboard/) to get you started.",871
Exporting to New Relic using Section,Monitoring with New Relic,Coming.,2
Exporting to New Relic using Section,Running the New Relic Agent on Section,"## Monitoring with New Relic
Coming.",9
Sample Grafana Cloud Dashboard,Grafana Cloud Dashboard,"Section provides a [sample dashboard](https://grafana.com/grafana/dashboards/17393-section-project/) that you can import into your [Grafana Cloud account](https://grafana.com/products/cloud/) so that you can view metrics for your running projects.

![Grafana Cloud dashboard](/img/docs/grafana-dashboard.png)

In order to use it, click on the link above and follow Grafana's instructions.

In order to understand the panels on the dashboard, it is useful to understand the [AEE workflow](/explanations/aee/).

Panels on the dashboard:
* Locations Selected - locations that are selected by the [AEE](/explanations/aee/), but maybe have not yet become ready yet.
* Is Ready - locations that are deployed and healthy and ready for traffic to be directed.
* Is Traffic Directed - the AEE has updated Internet records so that traffic will find the workload.
* Average RPS by geohash (map view) - average requests per second by 2-letter geohash of the end-user location (the origination of the request). The size of the circles are the magnitude of the signal. There are two circles representing two points in time. Learn about [geohash](https://chrishewett.com/blog/geohash-explorer/).
* RPS by Location - requests per second arriving at each location (from where the requests are served).
* Average RPS by geohash (time series view) - average requests per second by 2-letter geohash of the end-user location (the origination of the request). It is a 30 minute moving average. The intent of the 30 minute moving average is that it creates a clearer signal. This is the same metric as shown in the map view.
* Additional panels show traditional Prometheus metrics for your pods.",383
Sample Grafana Cloud Dashboard,Importing and Using a Sample Grafana Cloud Dashboard,"## Grafana Cloud Dashboard
Section provides a [sample dashboard](https://grafana.com/grafana/dashboards/17393-section-project/) that you can import into your [Grafana Cloud account](https://grafana.com/products/cloud/) so that you can view metrics for your running projects.

![Grafana Cloud dashboard](/img/docs/grafana-dashboard.png)

In order to use it, click on the link above and follow Grafana's instructions.

In order to understand the panels on the dashboard, it is useful to understand the [AEE workflow](/explanations/aee/).

Panels on the dashboard:
* Locations Selected - locations that are selected by the [AEE](/explanations/aee/), but maybe have not yet become ready yet.
* Is Ready - locations that are deployed and healthy and ready for traffic to be directed.
* Is Traffic Directed - the AEE has updated Internet records so that traffic will find the workload.
* Average RPS by geohash (map view) - average requests per second by 2-letter geohash of the end-user location (the origination of the request). The size of the circles are the magnitude of the signal. There are two circles representing two points in time. Learn about [geohash](https://chrishewett.com/blog/geohash-explorer/).
* RPS by Location - requests per second arriving at each location (from where the requests are served).
* Average RPS by geohash (time series view) - average requests per second by 2-letter geohash of the end-user location (the origination of the request). It is a 30 minute moving average. The intent of the 30 minute moving average is that it creates a clearer signal. This is the same metric as shown in the map view.
* Additional panels show traditional Prometheus metrics for your pods.
",389
Exporting to Grafana using Docker,Monitoring with Grafana Cloud,"Here we provide a simple example of how to scrape Section metrics into Grafana Cloud using `docker run`. The basic idea is that you need to run a Prometheus agent that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics, and writes the results into Grafana Cloud using “remote write”. (At the time of this writing Grafana Cloud has no way to do the scraping itself.)

![Grafana Cloud](/img/docs/monitor-grafana.png)

You may run the Prometheus agent on any infrastructure of your choice. It could be in a docker container on your PC, hosted on a cloud provider, or even hosted at Section. A single agent can be used to scrape all of the metrics for your account.

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`:
  * Is of the form: https://…/api/prom/push",252
Exporting to Grafana using Docker,Useful Links,"* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)
* Learn about how to setup your [Prometheus Agent](https://grafana.com/docs/grafana-cloud/metrics-prometheus/)",45
Exporting to Grafana using Docker,Configuration for a Prometheus Docker Container,"The following YAML file configures a docker container that will run Prometheus, scrape Section’s /federate endpoint, and write the results into Grafana Cloud. Replace the following accordingly: `SECTION_ACCOUNT_ID`, `SECTION_API_TOKEN`, `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`, `GRAFANA_METRICS_INSTANCE_ID`, `GRAFANA_API_KEY`.

```yaml title=""prometheus.yaml""",82
Exporting to Grafana using Docker,my global config,"global:
  scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'section-monitor'",105
Exporting to Grafana using Docker,Load rules once and periodically evaluate them according to the global 'evaluation_interval'.,"rule_files:
  # - ""first.rules""
  # - ""second.rules""",17
Exporting to Grafana using Docker,Here it's Prometheus itself.,"scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'section-federation'
    metrics_path: '/prometheus/account/SECTION_ACCOUNT_ID/federate'
    params:
      'match[]':
        - '{__name__=~"".+""}'
    scheme: 'https'
    authorization:
      type: Bearer
      credentials: SECTION_API_TOKEN
    static_configs:
      - targets: ['console.section.io']
remote_write:
- url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
  basic_auth:
    username: GRAFANA_METRICS_INSTANCE_ID
    password: GRAFANA_API_KEY
```

The command to run the container is as follows. Replace `/home/myhome` accordingly.

```
docker run \
    -p 9090:9090 \
    -v /home/myhome/prometheus.yml:/etc/prometheus/prometheus.yml \
    prom/prometheus
```",207
Exporting to Grafana using Docker,Support for Cloud-Native Monitoring Tools such as Grafana,"## Monitoring with Grafana Cloud
Here we provide a simple example of how to scrape Section metrics into Grafana Cloud using `docker run`. The basic idea is that you need to run a Prometheus agent that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics, and writes the results into Grafana Cloud using “remote write”. (At the time of this writing Grafana Cloud has no way to do the scraping itself.)

![Grafana Cloud](/img/docs/monitor-grafana.png)

You may run the Prometheus agent on any infrastructure of your choice. It could be in a docker container on your PC, hosted on a cloud provider, or even hosted at Section. A single agent can be used to scrape all of the metrics for your account.

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`:
  * Is of the form: https://…/api/prom/push

### Useful Links
* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)
* Learn about how to setup your [Prometheus Agent](https://grafana.com/docs/grafana-cloud/metrics-prometheus/)

### Configuration for a Prometheus Docker Container
The following YAML file configures a docker container that will run Prometheus, scrape Section’s /federate endpoint, and write the results into Grafana Cloud. Replace the following accordingly: `SECTION_ACCOUNT_ID`, `SECTION_API_TOKEN`, `GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT`, `GRAFANA_METRICS_INSTANCE_ID`, `GRAFANA_API_KEY`.

```yaml title=""prometheus.yaml""
# my global config
global:
  scrape_interval:     30s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 30s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

  # Attach these labels to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  external_labels:
      monitor: 'section-monitor'

# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
  # - ""first.rules""
  # - ""second.rules""

# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: 'section-federation'
    metrics_path: '/prometheus/account/SECTION_ACCOUNT_ID/federate'
    params:
      'match[]':
        - '{__name__=~"".+""}'
    scheme: 'https'
    authorization:
      type: Bearer
      credentials: SECTION_API_TOKEN
    static_configs:
      - targets: ['console.section.io']
remote_write:
- url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
  basic_auth:
    username: GRAFANA_METRICS_INSTANCE_ID
    password: GRAFANA_API_KEY
```

The command to run the container is as follows. Replace `/home/myhome` accordingly.

```
docker run \
    -p 9090:9090 \
    -v /home/myhome/prometheus.yml:/etc/prometheus/prometheus.yml \
    prom/prometheus
```",768
Exporting to Datadog using Section,Monitoring with Datadog,"Here we provide a simple example of how to scrape Section metrics into Datadog by using a deployment of the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) on Section itself as a project separate from your production workload project. The basic idea is that you need to run a [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics for your entire account, and writes the results into your Datadog cloud instance. We have [another guide](/guides/monitor/exporting-telemetry/datadog-docker) that shows how to do this with `docker run` if you prefer.  But here we show how to make a deployment to Section that runs 24x7 in a single location. A single Datadog agent collects metrics for all projects in your account.

![Datadog](/img/docs/monitor-datadog.png)

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.",283
Exporting to Datadog using Section,Useful Links,"* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Specifically, you are using the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/), so you will want to learn about that.",59
Exporting to Datadog using Section,Deployment,"The following deployment will run the agent on Section.  Substitute `DATADOG_API_KEY` accordingly.
```yaml title=""datadog-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ddagent
  name: ddagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ddagent
  template:
    metadata:
      labels:
        app: ddagent
    spec:
      containers:
      - image: gcr.io/datadoghq/agent:7
        imagePullPolicy: Always
        name: ddagent
        env:
          - name: DD_API_KEY
            value: DATADOG_API_KEY
          - name: DD_SITE
            value: ""datadoghq.com""
          - name: DD_HOSTNAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName            
        volumeMounts:
          - name: ddagent-config
            mountPath: /etc/datadog-agent/conf.d/openmetrics.d
        resources:
          requests:
            memory: ""250Mi""
            cpu: ""250m""
          limits:
            memory: ""250Mi""
            cpu: ""250m""
      volumes:
        - name: ddagent-config
          configMap:
            name: ddagent-config
```

Deploy it with `kubectl apply -f datadog-deployment.yaml`.

## Configuration
The following YAML file defines a ConfigMap with configuration for the Datadog agent. It identifies the Section /federate endpoint for your account, names the metrics to collect, and identifies the SECTION_API_TOKEN.
* Replace `SECTION_ACCOUNT_ID` with your Section Account ID, typically of the form `1234`.
* The `token` section should be filled with one line containing your `SECTION_API_TOKEN`. It will be of the form `*********************************************************************4c57`.
* Learn how to obtain the SECTION items [here](/guides/monitor/exporting-telemetry/metrics-endpoint/).
* Note that the ```Bearer <TOKEN>``` is not something that you should replace, just leave it alone.
```yaml title=""configmap.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: ddagent-config
data:
  conf.yaml: |
    init_config:
    instances:
      - namespace: section
        openmetrics_endpoint: https://console.section.io/prometheus/account/SECTION_ACCOUNT_ID/federate
        auth_token:
          reader:
            type: file
            path: /etc/datadog-agent/conf.d/openmetrics.d/token
            pattern: ^(.+)$
          writer:
            type: header
            name: Authorization
            value: ""Bearer <TOKEN>""
        metrics:
          - section.+:
              type: gauge
  token: |
    SECTION_API_TOKEN
```
Deploy it with `kubectl apply -f configmap.yaml`.",606
Exporting to Datadog using Section,Location Strategy,"By default, Section will run this project in 2 locations. We only need to collect metrics from your account once, so let's provide a location optimizer strategy that runs the project in only a single location. Read more about [location strategies](/explanations/aee/).
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```
Deploy it with `kubectl apply -f location-optimizer.yaml`. 

## View Metrics in Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/datadog-sample-dashboard/) to get you started.",211
Exporting to Datadog using Section,Running the Datadog Agent on Section,"## Monitoring with Datadog
Here we provide a simple example of how to scrape Section metrics into Datadog by using a deployment of the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) on Section itself as a project separate from your production workload project. The basic idea is that you need to run a [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/) that regularly scrapes [Section’s /federate endpoint](/guides/monitor/exporting-telemetry/metrics-endpoint/) to fetch metrics for your entire account, and writes the results into your Datadog cloud instance. We have [another guide](/guides/monitor/exporting-telemetry/datadog-docker) that shows how to do this with `docker run` if you prefer.  But here we show how to make a deployment to Section that runs 24x7 in a single location. A single Datadog agent collects metrics for all projects in your account.

![Datadog](/img/docs/monitor-datadog.png)

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.

### Useful Links
* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Specifically, you are using the [Datadog Agent](https://docs.datadoghq.com/getting_started/agent/), so you will want to learn about that.

## Deployment
The following deployment will run the agent on Section.  Substitute `DATADOG_API_KEY` accordingly.
```yaml title=""datadog-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ddagent
  name: ddagent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ddagent
  template:
    metadata:
      labels:
        app: ddagent
    spec:
      containers:
      - image: gcr.io/datadoghq/agent:7
        imagePullPolicy: Always
        name: ddagent
        env:
          - name: DD_API_KEY
            value: DATADOG_API_KEY
          - name: DD_SITE
            value: ""datadoghq.com""
          - name: DD_HOSTNAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName            
        volumeMounts:
          - name: ddagent-config
            mountPath: /etc/datadog-agent/conf.d/openmetrics.d
        resources:
          requests:
            memory: ""250Mi""
            cpu: ""250m""
          limits:
            memory: ""250Mi""
            cpu: ""250m""
      volumes:
        - name: ddagent-config
          configMap:
            name: ddagent-config
```

Deploy it with `kubectl apply -f datadog-deployment.yaml`.

## Configuration
The following YAML file defines a ConfigMap with configuration for the Datadog agent. It identifies the Section /federate endpoint for your account, names the metrics to collect, and identifies the SECTION_API_TOKEN.
* Replace `SECTION_ACCOUNT_ID` with your Section Account ID, typically of the form `1234`.
* The `token` section should be filled with one line containing your `SECTION_API_TOKEN`. It will be of the form `*********************************************************************4c57`.
* Learn how to obtain the SECTION items [here](/guides/monitor/exporting-telemetry/metrics-endpoint/).
* Note that the ```Bearer <TOKEN>``` is not something that you should replace, just leave it alone.
```yaml title=""configmap.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: ddagent-config
data:
  conf.yaml: |
    init_config:
    instances:
      - namespace: section
        openmetrics_endpoint: https://console.section.io/prometheus/account/SECTION_ACCOUNT_ID/federate
        auth_token:
          reader:
            type: file
            path: /etc/datadog-agent/conf.d/openmetrics.d/token
            pattern: ^(.+)$
          writer:
            type: header
            name: Authorization
            value: ""Bearer <TOKEN>""
        metrics:
          - section.+:
              type: gauge
  token: |
    SECTION_API_TOKEN
```
Deploy it with `kubectl apply -f configmap.yaml`.
## Location Strategy
By default, Section will run this project in 2 locations. We only need to collect metrics from your account once, so let's provide a location optimizer strategy that runs the project in only a single location. Read more about [location strategies](/explanations/aee/).
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```
Deploy it with `kubectl apply -f location-optimizer.yaml`. 

## View Metrics in Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your metrics. Try our [sample dashboard](/guides/monitor/exporting-telemetry/datadog-sample-dashboard/) to get you started.",1177
Metrics Endpoint,Exporting Metrics,"Section provides a /federate endpoint from which you can gather live metrics pertaining to your account. The systems underlying this endpoint are Prometheus systems. But the metrics returned are OpenMetrics compatible, and therefore you can use any compatible tooling in order to process them.

We have a number of guides to help you scrape your metrics from our endpoint and export them to [Grafana Cloud](https://grafana.com/products/cloud/), [Datadog](https://www.datadoghq.com/), and others.",105
Metrics Endpoint,Connecting to Section’s /federate Endpoint,"To setup remote collection for any of the following 3rd party systems you will need to collect the following information from Section Console:

* `SECTION_ACCOUNT_ID` is 4 or more digits, as in “1234”. Get your account ID by following [these instructions](/docs/guides/kubernetes-ui/kubernetes-api/create-environment/#get-your-account-id).
* URL: console.section.io/prometheus/account/`SECTION_ACCOUNT_ID`/federate
* Authentication type: Bearer
* `SECTION_API_TOKEN`: get an API token using instructions [here](/guides/iam/api-tokens).",124
Metrics Endpoint,Useful Links,* Learn more about Prometheus and OpenMetrics at this [primer](https://scoutapm.com/blog/prometheus-metrics).,26
Metrics Endpoint,Section Metrics Endpoint used by All Tools,"## Exporting Metrics
Section provides a /federate endpoint from which you can gather live metrics pertaining to your account. The systems underlying this endpoint are Prometheus systems. But the metrics returned are OpenMetrics compatible, and therefore you can use any compatible tooling in order to process them.

We have a number of guides to help you scrape your metrics from our endpoint and export them to [Grafana Cloud](https://grafana.com/products/cloud/), [Datadog](https://www.datadoghq.com/), and others.

## Connecting to Section’s /federate Endpoint
To setup remote collection for any of the following 3rd party systems you will need to collect the following information from Section Console:

* `SECTION_ACCOUNT_ID` is 4 or more digits, as in “1234”. Get your account ID by following [these instructions](/docs/guides/kubernetes-ui/kubernetes-api/create-environment/#get-your-account-id).
* URL: console.section.io/prometheus/account/`SECTION_ACCOUNT_ID`/federate
* Authentication type: Bearer
* `SECTION_API_TOKEN`: get an API token using instructions [here](/guides/iam/api-tokens).

### Useful Links
* Learn more about Prometheus and OpenMetrics at this [primer](https://scoutapm.com/blog/prometheus-metrics).",275
Using kubectl logs,Using kubectl logs Logging support by using kubectl,"Logs for your application can be seen by using `kubectl logs`. A few examples of using this command are shown below. See the [Kubernetes documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs) for complete details.

| Command                                          | Result                                                                                                            |
|--------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| kubectl logs MY_POD                              | # dump pod logs (stdout)                                                                                          |
| kubectl logs -l name=myLabel                     | # dump pod logs, with label name=myLabel (stdout)                                                                 |
| kubectl logs MY_POD --previous                   | # dump pod logs (stdout) for a previous instantiation of a container                                              |
| kubectl logs MY_POD -c MY_CONTAINER              | # dump pod container logs (stdout, multi-container case)                                                          |
| kubectl logs -l name=myLabel -c MY_CONTAINER     | # dump pod logs, with label name=myLabel (stdout)                                                                 |
| kubectl logs MY_POD -c MY_CONTAINER --previous   | # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container              |
| kubectl logs -f MY_POD                           | # stream pod logs (stdout)                                                                                        |
| kubectl logs -f MY_POD -c MY_CONTAINER           | # stream all pods logs with label name=myLabel (stdout)# stream pod container logs (stdout, multi-container case) |
| kubectl logs -f -l name=myLabel --all-containers | # stream all pods logs with label name=myLabel (stdout)                                                           |

Note that if a pod is deleted, manually or due to scaling, rescheduling, or if its node fails, then earlier logs will be unavailable for that pod. So [log streaming](/guides/monitor/logs/log-streaming/) is a technique you should leverage.",379
Using kubectl logs,Logging support by using kubectl,"
Logs for your application can be seen by using `kubectl logs`. A few examples of using this command are shown below. See the [Kubernetes documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs) for complete details.

| Command                                          | Result                                                                                                            |
|--------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| kubectl logs MY_POD                              | # dump pod logs (stdout)                                                                                          |
| kubectl logs -l name=myLabel                     | # dump pod logs, with label name=myLabel (stdout)                                                                 |
| kubectl logs MY_POD --previous                   | # dump pod logs (stdout) for a previous instantiation of a container                                              |
| kubectl logs MY_POD -c MY_CONTAINER              | # dump pod container logs (stdout, multi-container case)                                                          |
| kubectl logs -l name=myLabel -c MY_CONTAINER     | # dump pod logs, with label name=myLabel (stdout)                                                                 |
| kubectl logs MY_POD -c MY_CONTAINER --previous   | # dump pod container logs (stdout, multi-container case) for a previous instantiation of a container              |
| kubectl logs -f MY_POD                           | # stream pod logs (stdout)                                                                                        |
| kubectl logs -f MY_POD -c MY_CONTAINER           | # stream all pods logs with label name=myLabel (stdout)# stream pod container logs (stdout, multi-container case) |
| kubectl logs -f -l name=myLabel --all-containers | # stream all pods logs with label name=myLabel (stdout)                                                           |

Note that if a pod is deleted, manually or due to scaling, rescheduling, or if its node fails, then earlier logs will be unavailable for that pod. So [log streaming](/guides/monitor/logs/log-streaming/) is a technique you should leverage.",380
Log Streaming to External Tools,Differences from Metric Collection,It is worth noting that our [metric collection guides](/guides/monitor/exporting-telemetry/) are presented as a single deployment that scrapes metrics from all projects in an account. Whereas our log forwarders are presented as a deployment in each project to be logged.,55
Log Streaming to External Tools,Logging support for cloud-native monitoring tools,"
Stream logs to external tools to get more robust support for log viewing than is possible using [`kubectl logs`](/guides/monitor/logs/using-kubectl-logs/).

If your Project includes a Kubernetes Service decorated with the label `section.io/logstream-destination: ""true""` and listening on UDP port 5160, the Section Platform will collect the container logs for all the Pods in your Project and forward them to this Service's Endpoint(s) in JSON format, similar to this (pretty-printed here):
```json title=""example-log-entry.json""
{
  ""@timestamp"": ""2022-07-01T23:45:56.999Z"",
  ""kubernetes"": {
    ""container"": {
      ""name"": ""frontend""
    },
    ""labels"": {
      ""app"": ""contoso"",
      ""pod-template-hash"": ""69bc8cd654""
    },
    ""namespace"": ""default"",
    ""node"": {
      ""name"": ""syd-umvnp""
    },
    ""pod"": {
      ""ip"": ""172.17.0.5"",
      ""name"": ""contoso-69bc8cd654-6lhrm""
    }
  },
  ""log"": {
    ""offset"": 23456
  },
  ""message"": ""Application has completed initialization"",
  ""stream"": ""stdout""
}
```

Your Service Endpoint Pods can receive these log packets, process them, and forward them to an external log ingestion service for storage and querying. The JSON over UDP/5160 wire format is simple enough if you want to implement your own application to process, however the popular CNCF project [Fluentd has a UDP input plugin](https://docs.fluentd.org/input/udp) exactly for this. Here is an example Fluentd source configuration:
```html title=""example-source-configuration""
<source>
  @type udp
  tag all_section_logs
  <parse>
    @type json
  </parse>
  port 5160
  message_length_limit 1MB # only 4096 bytes by default
</source>
```

And then you'll need a matching output configuration, which contains specifics for your log destination. The following matches all logs tagged with `all_section_logs` from the `<source>` configuration.
```html title=""prototype-output-configuration""
<match all_section_logs>
  YOUR_LOG_DESTINATION_CONFIG_HERE
</match>
```

Popular log ingestion services will have a Fluentd output plugin and configuration for shipping logs to their system. The table below lists a number of examples with detailed Section integrations where available.

| Log Destination | Documentation | Plugin | Section Log Integration |
| --------------- | ------------- | ------ | ------------------- |
| Datadog         | [Documentation](https://docs.datadoghq.com/integrations/fluentd/) | [Plugin](https://github.com/DataDog/fluent-plugin-datadog) | [Log Integration](/guides/monitor/logs/datadog-logs/) |
| S3 | [Documentation](https://docs.fluentd.org/output/s3) | | |
| Splunk | [Documentation](https://docs.splunk.com/Observability/logs/logs.html#fluentd) | [Plugin](https://github.com/splunk/fluent-plugin-splunk-hec) | |
| Sumo Logic | | [Plugin](https://github.com/SumoLogic/fluentd-output-sumologic) | |
| Grafana Loki | [Documentation](https://grafana.com/docs/loki/latest/clients/fluentd/) | [Plugin](https://github.com/grafana/loki/tree/main/clients/cmd/fluentd) | [Log Integration](/guides/monitor/logs/grafana-loki/) |
| Dynatrace | | [Plugin](https://github.com/dynatrace-oss/fluent-plugin-dynatrace) | |
| Elasticsearch | | [Plugin](https://github.com/uken/fluent-plugin-elasticsearch) | |
| New Relic | | [Plugin](https://github.com/newrelic/newrelic-fluentd-output) | [Log Integration](/guides/monitor/logs/newrelic-logs/) |
| Google Cloud | | [Plugin](https://github.com/GoogleCloudPlatform/fluent-plugin-google-cloud) |
| Logtail | | [Plugin](https://github.com/logtail/fluentd-plugin-logtail) |

Here is an example Kubernetes yaml to deploy Fluentd for log shipping where we put it all together:
```yaml title=""log-shipping.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: YOUR_FLUENTD_IMAGE_HERE
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag all_section_logs
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>

    <match all_section_logs>
      YOUR_LOG_DESTINATION_CONFIG_HERE
    </match>
```

Our specific integration guides give exactly the code needed for `YOUR_LOG_DESTINATION_CONFIG_HERE` and `YOUR_FLUENTD_IMAGE_HERE` to make configuration easy.

## Differences from Metric Collection
It is worth noting that our [metric collection guides](/guides/monitor/exporting-telemetry/) are presented as a single deployment that scrapes metrics from all projects in an account. Whereas our log forwarders are presented as a deployment in each project to be logged.
",1429
New Relic Logs,Log Streaming to New Relic,"Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for New Relic.

Obtain the following information from your instance of New Relic:
* `NEWRELIC_LICENSE_KEY`: Learn about New Relic [API Keys](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys).",85
New Relic Logs,Useful Links,"* Get started with [New Relic](https://docs.newrelic.com/).
* Section has already built the container for [Fluentd with New Relic Output Plugin](https://github.com/section/fluentd-newrelic) for you use, named in the YAML below.",60
New Relic Logs,Deployment,"The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project. Substitute `NEWRELIC_LICENSE_KEY` accordingly.

```yaml title=""newrelic-logs-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: ghcr.io/section/fluentd-newrelic:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: ""200Mi""
            cpu: ""200m""
          limits:
            memory: ""200Mi""
            cpu: ""200m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag all_section_logs
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>

    <filter all_section_logs> 
      @type record_transformer 
      <record> 
        offset ${record[""log""][""offset""]} 
      </record> 
      enable_ruby true
      remove_keys log 
    </filter>

    <match all_section_logs>
      @type newrelic
      license_key NEWRELIC_LICENSE_KEY
    </match>
```

Apply the above resources with `kubectl apply -f newrelic-logs-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in New Relic
Login to your [New Relic account](https://one.newrelic.com/) in order to see your logs.",566
New Relic Logs,Logging streaming support for New Relic,"## Log Streaming to New Relic
Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for New Relic.

Obtain the following information from your instance of New Relic:
* `NEWRELIC_LICENSE_KEY`: Learn about New Relic [API Keys](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys).

### Useful Links
* Get started with [New Relic](https://docs.newrelic.com/).
* Section has already built the container for [Fluentd with New Relic Output Plugin](https://github.com/section/fluentd-newrelic) for you use, named in the YAML below.

## Deployment
The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project. Substitute `NEWRELIC_LICENSE_KEY` accordingly.

```yaml title=""newrelic-logs-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: ghcr.io/section/fluentd-newrelic:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: ""200Mi""
            cpu: ""200m""
          limits:
            memory: ""200Mi""
            cpu: ""200m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag all_section_logs
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>

    <filter all_section_logs> 
      @type record_transformer 
      <record> 
        offset ${record[""log""][""offset""]} 
      </record> 
      enable_ruby true
      remove_keys log 
    </filter>

    <match all_section_logs>
      @type newrelic
      license_key NEWRELIC_LICENSE_KEY
    </match>
```

Apply the above resources with `kubectl apply -f newrelic-logs-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in New Relic
Login to your [New Relic account](https://one.newrelic.com/) in order to see your logs.",726
Grafana Loki Logs,Log Streaming to Grafana Cloud,"Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for Grafana Loki.

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_LOGGING_ENDPOINT`:
  * It is something like: https://logs-prod3.grafana.net",123
Grafana Loki Logs,Useful Links,* Get started with [Grafana Cloud](https://grafana.com/products/cloud/),18
Grafana Loki Logs,Deployment,"The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project.

```yaml title=""grafana-loki-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: grafana/fluent-plugin-loki:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag udpsource
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>


    <match udpsource>
      @type loki
      url GRAFANA_LOGGING_ENDPOINT
      username GRAFANA_METRICS_INSTANCE_ID
      password GRAFANA_API_KEY
      extra_labels {""env"":""prod""}
    </match>
```

Apply the above resources with `kubectl apply -f grafana-loki-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in Grafana Loki
Login to your [Grafana Cloud account](https://grafana.com/products/cloud) in order to see your logs.",526
Grafana Loki Logs,Logging streaming support for Grafana Loki,"## Log Streaming to Grafana Cloud
Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for Grafana Loki.

Obtain the following information from your instance of Grafana Cloud:

* `GRAFANA_METRICS_INSTANCE_ID`: obtain from Grafana Cloud
* `GRAFANA_API_KEY`: obtain from [Grafana Cloud](https://grafana.com/docs/grafana-cloud/reference/create-api-key/)
* `GRAFANA_LOGGING_ENDPOINT`:
  * It is something like: https://logs-prod3.grafana.net


### Useful Links
* Get started with [Grafana Cloud](https://grafana.com/products/cloud/)

## Deployment
The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project.

```yaml title=""grafana-loki-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: grafana/fluent-plugin-loki:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag udpsource
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>


    <match udpsource>
      @type loki
      url GRAFANA_LOGGING_ENDPOINT
      username GRAFANA_METRICS_INSTANCE_ID
      password GRAFANA_API_KEY
      extra_labels {""env"":""prod""}
    </match>
```

Apply the above resources with `kubectl apply -f grafana-loki-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in Grafana Loki
Login to your [Grafana Cloud account](https://grafana.com/products/cloud) in order to see your logs.",684
Datadog Logs,Log Streaming to Datadog,"Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for Datadog.

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.",98
Datadog Logs,Useful Links,"* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Section has already built the container for [Fluentd with Datadog Output Plugin](https://github.com/section/fluentd-datadog) for you use, named in the YAML below.",64
Datadog Logs,Deployment,"The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project. Substitute `DATADOG_API_KEY` accordingly.

```yaml title=""datadog-logs-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: ghcr.io/section/fluentd-datadog:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag all_section_logs
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>

    <match all_section_logs>
      @type datadog
      @id awesome_agent
      api_key DATADOG_API_KEY

      <buffer>
              @type memory
              flush_thread_count 4
              flush_interval 3s
              chunk_limit_size 5m
              chunk_limit_records 500
      </buffer>
    </match>
```

Apply the above resources with `kubectl apply -f datadog-logs-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your logs.",565
Datadog Logs,Logging streaming support for Datadog,"## Log Streaming to Datadog
Following the [general pattern](/guides/monitor/logs/log-streaming/) for log streaming from applications running on Section, in this guide we give specifics for Datadog.

Obtain the following information from your instance of Datadog:

* `DATADOG_API_KEY`: this will appear during the Datadog setup wizard.  Or generate one by visiting the [API Keys](https://app.datadoghq.com/organization-settings/api-keys) area of Organization Settings.

### Useful Links
* Get started with [Datadog](https://docs.datadoghq.com/getting_started/).
* Section has already built the container for [Fluentd with Datadog Output Plugin](https://github.com/section/fluentd-datadog) for you use, named in the YAML below.

## Deployment
The following deployment will run the Fluentd log forwarder in your Section project, gathering logs from other pods in that same project. Substitute `DATADOG_API_KEY` accordingly.

```yaml title=""datadog-logs-deployment.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: fluentd
    section.io/logstream-destination: ""true""
  name: fluentd
  namespace: default
spec:
  ports:
  - name: fluentdudp
    port: 5160
    protocol: UDP
    targetPort: 5160
  selector:
    app: fluentd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fluentd
  namespace: default
  labels:
    app: fluentd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        section.io/logstream-collect: ""false""
    spec:
      containers:
      - name: fluentd
        image: ghcr.io/section/fluentd-datadog:master
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc/fluent.conf
          readOnly: true
          subPath: fluent.conf
      volumes:
      - name: config
        configMap:
          name: fluent-conf
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-conf
  namespace: default
data:
  fluent.conf: |-
    <source>
      @type udp
      tag all_section_logs
      <parse>
        @type json
      </parse>
      port 5160
      message_length_limit 1MB
    </source>

    <match all_section_logs>
      @type datadog
      @id awesome_agent
      api_key DATADOG_API_KEY

      <buffer>
              @type memory
              flush_thread_count 4
              flush_interval 3s
              chunk_limit_size 5m
              chunk_limit_records 500
      </buffer>
    </match>
```

Apply the above resources with `kubectl apply -f datadog-logs-deployment.yaml` into the same project where the pods to be logged are running.

## View Logs in Datadog
Login to your [Datadog account](https://app.datadoghq.com/) in order to see your logs.",742
HTTP Ingress,HTTP Ingress Overview,"For each Project deployed on Section, we deploy an [HTTP Ingress](/reference/http-extensions/http-ingress/), so the Project will accept inbound HTTP traffic from the Internet.",37
HTTP Ingress,View or Modify HTTP Ingress,"To view or modify the HTTP Ingress controller for your Section Project, view the **Kubernetes Service** we deploy for your Project with the specific name of `ingress-upstream`.

![Settings](/img/docs/ingress.png)

Use the [K8s Dashboard](/docs/guides/projects/manage-resources/) we provide or Kubectl to deploy or modify the Service Project if you wish to change items such as the Port your Application needs to communicate with the upstream Ingress.



````yaml title=""http-ingress.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
    namespace: default
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
        app: YOUR_APP_NAME
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
````",207
HTTP Ingress,Enable Section HTTP Ingress,"
## HTTP Ingress Overview
For each Project deployed on Section, we deploy an [HTTP Ingress](/reference/http-extensions/http-ingress/), so the Project will accept inbound HTTP traffic from the Internet.

### View or Modify HTTP Ingress
To view or modify the HTTP Ingress controller for your Section Project, view the **Kubernetes Service** we deploy for your Project with the specific name of `ingress-upstream`.

![Settings](/img/docs/ingress.png)

Use the [K8s Dashboard](/docs/guides/projects/manage-resources/) we provide or Kubectl to deploy or modify the Service Project if you wish to change items such as the Port your Application needs to communicate with the upstream Ingress.



````yaml title=""http-ingress.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
    namespace: default
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
        app: YOUR_APP_NAME
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
````


",260
HTTP Egress,Egress Module Deployment,This guide will walk you through deploying the Section Egress reverse proxy. The Egress reverse proxy is a simple reverse proxy that normalizes and routes requests to an external service. You can read more about the [Egress technical details](/reference/http-extensions/http-egress/). You will need the following Kubernetes resources to deploy the Egress reverse proxy:,73
HTTP Egress,ConfigMap,"Create the following ConfigMap defining an egress.json file:

```yaml title=""egress-configmap.yaml""
kind: ConfigMap
apiVersion: v1
metadata:
    name: egress-config
    namespace: default
data:
    egress.json: |
        {
            ""origin"": {
                ""address"":""my-external-service.example.com"",
                ""enable_sni"":true
            },
            ""alternate_origins"": {
                ""another_endpoint"": {
                    ""address"":""my-external-service2.example.com""
                }
            }
        }
```

This example will define a default origin routing traffic to **my-external-service.example.com**. The default origin will be used when no other origin is specified. The **section-origin** HTTP request header can be used to specify an alternate origin based on the matching key and value, e.g. *another_endpoint*.

#### Deployment

The deployment object is defined via the following YAML example:

```yaml title=""egress-deployment.yaml""
kind: Deployment
apiVersion: apps/v1
metadata:
  name: egress
  namespace: default
  labels:
    app: egress
spec:
  replicas: 2
  selector:
    matchLabels:
      app: egress
  template:
    metadata:
      labels:
        app: egress
    spec:
      volumes:
        - name: config-mount
          configMap:
            name: egress-config
      containers:
        - name: proxy
          image: 'gcr.io/section-io/k8s-egress:2.1.1'
          ports:
            - containerPort: 80
              protocol: TCP
          env:
            - name: SECTION_PROXY_NAME
              value: egress
            - name: REDIS_HOST
              value: 127.0.0.1
            - name: PROXY_REGO_KEY
              value: abc
            - name: LIST_KEY_PREFIX
              value: abc
            - name: LIST_KEY_SUFFIX
              value: abcingress
          resources:
            limits:
              cpu: '0.2'
              memory: 400Mi
            requests:
              cpu: '0.2'
              memory: 400Mi
          volumeMounts:
            - name: config-mount
              readOnly: true
              mountPath: /opt/proxy_config
          livenessProbe:
            httpGet:
              path: /.well-known/section-io/egress-status
              port: 80
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 10
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /.well-known/section-io/egress-status
              port: 80
              scheme: HTTP
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          imagePullPolicy: Always
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
```

*Note:* The environment variables are not explicitly used but are required for the container to function properly.",694
HTTP Egress,Service,"In order to route traffic to the egress container, we need to define a service. The service is defined via the following YAML example:

```yaml title=""egress-service.yaml""
apiVersion: v1
kind: Service
metadata:
  name: egress-service
spec:
  selector:
    app: egress
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 80
```

In order to route traffic to the egress service we will use the following Nginx example below.

```nginx
location / {
    proxy_set_header X-Forwarded-For $http_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $http_x_forwarded_proto;
    proxy_set_header Host $host;
    proxy_pass http://egress-service:8080;
}
```",175
HTTP Egress,Enable Section Egress Module,"
In order to direct traffic from an application on the Section platform to an alternate Origin you can deploy Section's Egress Module as an HTTP extension.

## Egress Module Deployment

This guide will walk you through deploying the Section Egress reverse proxy. The Egress reverse proxy is a simple reverse proxy that normalizes and routes requests to an external service. You can read more about the [Egress technical details](/reference/http-extensions/http-egress/). You will need the following Kubernetes resources to deploy the Egress reverse proxy:

### ConfigMap

Create the following ConfigMap defining an egress.json file:

```yaml title=""egress-configmap.yaml""
kind: ConfigMap
apiVersion: v1
metadata:
    name: egress-config
    namespace: default
data:
    egress.json: |
        {
            ""origin"": {
                ""address"":""my-external-service.example.com"",
                ""enable_sni"":true
            },
            ""alternate_origins"": {
                ""another_endpoint"": {
                    ""address"":""my-external-service2.example.com""
                }
            }
        }
```

This example will define a default origin routing traffic to **my-external-service.example.com**. The default origin will be used when no other origin is specified. The **section-origin** HTTP request header can be used to specify an alternate origin based on the matching key and value, e.g. *another_endpoint*.

#### Deployment

The deployment object is defined via the following YAML example:

```yaml title=""egress-deployment.yaml""
kind: Deployment
apiVersion: apps/v1
metadata:
  name: egress
  namespace: default
  labels:
    app: egress
spec:
  replicas: 2
  selector:
    matchLabels:
      app: egress
  template:
    metadata:
      labels:
        app: egress
    spec:
      volumes:
        - name: config-mount
          configMap:
            name: egress-config
      containers:
        - name: proxy
          image: 'gcr.io/section-io/k8s-egress:2.1.1'
          ports:
            - containerPort: 80
              protocol: TCP
          env:
            - name: SECTION_PROXY_NAME
              value: egress
            - name: REDIS_HOST
              value: 127.0.0.1
            - name: PROXY_REGO_KEY
              value: abc
            - name: LIST_KEY_PREFIX
              value: abc
            - name: LIST_KEY_SUFFIX
              value: abcingress
          resources:
            limits:
              cpu: '0.2'
              memory: 400Mi
            requests:
              cpu: '0.2'
              memory: 400Mi
          volumeMounts:
            - name: config-mount
              readOnly: true
              mountPath: /opt/proxy_config
          livenessProbe:
            httpGet:
              path: /.well-known/section-io/egress-status
              port: 80
              scheme: HTTP
            initialDelaySeconds: 30
            timeoutSeconds: 10
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /.well-known/section-io/egress-status
              port: 80
              scheme: HTTP
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          imagePullPolicy: Always
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
```

*Note:* The environment variables are not explicitly used but are required for the container to function properly.

#### Service

In order to route traffic to the egress container, we need to define a service. The service is defined via the following YAML example:

```yaml title=""egress-service.yaml""
apiVersion: v1
kind: Service
metadata:
  name: egress-service
spec:
  selector:
    app: egress
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 80
```

In order to route traffic to the egress service we will use the following Nginx example below.

```nginx
location / {
    proxy_set_header X-Forwarded-For $http_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $http_x_forwarded_proto;
    proxy_set_header Host $host;
    proxy_pass http://egress-service:8080;
}
```
",986
Upgrade Plan,Upgrade Plan How to Update the Billing Plan for your Project,"You may upgrade (or downgrade) your Project Plan at any time.

Simply select the settings for the particular Project you would like to Upgrade and choose ""Update Plan"".

![Upgrade Plan](/img/docs/upgrade-plan.png)

[Choose the plan](https://section.io/pricing/) which makes sense for your application's needs.  

You may be prompted to add a credit card if you do not have one associated with your account already.

![Delete](/img/docs/add-payment.png)

Add your payment details then confirm by clicking ""Add Payment Method""

Then confirm the Project Plan upgrade by clicking ""Update Project Plan"".

That's it - you are ready to make use of the new resource limits and locations.",142
Upgrade Plan,How to Update the Billing Plan for your Project,"
You may upgrade (or downgrade) your Project Plan at any time.

Simply select the settings for the particular Project you would like to Upgrade and choose ""Update Plan"".

![Upgrade Plan](/img/docs/upgrade-plan.png)

[Choose the plan](https://section.io/pricing/) which makes sense for your application's needs.  

You may be prompted to add a credit card if you do not have one associated with your account already.

![Delete](/img/docs/add-payment.png)

Add your payment details then confirm by clicking ""Add Payment Method""

Then confirm the Project Plan upgrade by clicking ""Update Project Plan"".

That's it - you are ready to make use of the new resource limits and locations.",143
Delete Project,Delete Project How to Delete a Project,"You may Delete any of your active Projects at any time.

Simply select the settings for the particular Project you would like to delete and follow the prompts under the General Settings Tab.

![Settings](/img/docs/settings.png)

![Delete](/img/docs/delete.png)",53
Delete Project,How to Delete a Project,"
You may Delete any of your active Projects at any time.

Simply select the settings for the particular Project you would like to delete and follow the prompts under the General Settings Tab.

![Settings](/img/docs/settings.png)

![Delete](/img/docs/delete.png)",54
Set Pod Size (CPU+RAM),Example Configuration,"The following Deployment has one container defined with a request for 0.5 GiB RAM and 0.5 vCPU.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        ports:
        - containerPort: 80
```

Refer to Kubernetes Docs on [managing resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) to learn more.

### Additional Information

* There are no minimum sizes.
* Maximum container sizes are 4 vCPU and 12 GiB.
* For more information please refer to the [product pricing information](https://section.io/pricing/) to understand how your requests are related to your billing.
* Section may alter your YAML to ensure that request = limit, which gives a quality of service [""Guaranteed""](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/).
  * Both request and limit must be specified
  * If request and limit are not equal, Section will use the higher of the two values.
* You cannot request ephemeral storage directly. Section will automatically apply the ephemeral storage limits when the deployment is created.",353
Set Pod Size (CPU+RAM),"Learn to manage resources for Section Projects, set pod resource limits and replica counts","

When you define your Deployment objects, you can specify the CPU and RAM requests for each container instance so you have the right size container/s for your application's needs.

Use the [K8s Dashboard](/docs/guides/projects/manage-resources/) we provide or Kubectl to deploy or modify the Project Deployment for each Project.


:::note
You will not be able to specify specific locations or a minimum number of locations if they will cause your usage of RAM and CPU to exceed the boundaries of your current project plan (see [Pricing Plans](https://section.io/pricing/)).
:::

Use the standard Kubernetes methods for specifying your container's requirements.

### Example Configuration
The following Deployment has one container defined with a request for 0.5 GiB RAM and 0.5 vCPU.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        ports:
        - containerPort: 80
```

Refer to Kubernetes Docs on [managing resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) to learn more.

### Additional Information

* There are no minimum sizes.
* Maximum container sizes are 4 vCPU and 12 GiB.
* For more information please refer to the [product pricing information](https://section.io/pricing/) to understand how your requests are related to your billing.
* Section may alter your YAML to ensure that request = limit, which gives a quality of service [""Guaranteed""](https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/).
  * Both request and limit must be specified
  * If request and limit are not equal, Section will use the higher of the two values.
* You cannot request ephemeral storage directly. Section will automatically apply the ephemeral storage limits when the deployment is created.
",491
SSL Certificates ,1) Navigate to HTTPS page,"Log into Section Console, navigate to the project, then **Settings** then **TLS certificates**.

![Settings](/img/docs/custom-cert.png)",30
SSL Certificates ,2) Select your desired domain,"The dropdown will automatically fill with the first domain listed in your application. If that's not the domain you want, select the correct one from the dropdown. 

If you have multiple sub-domains in this application and wish to upload a wild card certificate or a certificate with multiple domains in the Subject Alternate Name(SAN), you will need to upload the certificate for each domain.

You will see a button entitled ""Specify a custom certificate"", or ""Change custom certificate"" depending on whether you have previously uploaded a custom certificate for the application. This will render two input boxes on the page — one for ""Public certificate & chain"" and the other for ""Private key.""",133
SSL Certificates ,3) Paste certificate and key,"You will need to copy the site certificate and any intermediate certificates into the ""Public certificate & chain"" box.

The order of certificates needs to be domain certificate first, followed by any intermediate certificate(s) in order. Make sure NOT to include the root certificate. The certificates should be PEM encoded and will look like this in a text editor:

    
    -----BEGIN CERTIFICATE-----
    /* contents of domain certificate */
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate */
    -----END CERTIFICATE-----

If there are multiple intermediate certificates, you will need to make sure they are in the correct order.

    -----BEGIN CERTIFICATE-----
    /* contents of domain certificate */
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate 1*/
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate 2*/
    -----END CERTIFICATE-----

Now copy and paste your private key in to the ""Private key"" input area. the private key should look like:

    -----BEGIN RSA PRIVATE KEY-----
    /* contents of private key */
    -----END RSA PRIVATE KEY-----",233
SSL Certificates ,4) Upload,"Once you have copy and pasted the certificates and private key, click the ""Save Changes"" button. The portal will perform a check to make sure the certificate is for the correct domain, and the private key is a match for the certificate. Once that is accepted, a deployment is made to the Section platform and you should see the new certificate on the site in moments. You will also see the uploaded certificate information at the right hand side of the HTTPS page.",93
SSL Certificates ,How to upload a custom SSL certificate in the Section Console.,"
You can take advnatage can take advantage of our free SSL certificates that are automatically configured and renewed for every project using Let’s Encrypt.

You can also upload your own Standard, Wildcard, or Extended Validation certificates. To get started managing SSL certificates see our how to guide on managing HTTP Traffic.

## Adding Custom SSL Certificates

### 1) Navigate to HTTPS page

Log into Section Console, navigate to the project, then **Settings** then **TLS certificates**.

![Settings](/img/docs/custom-cert.png)

### 2) Select your desired domain

The dropdown will automatically fill with the first domain listed in your application. If that's not the domain you want, select the correct one from the dropdown. 

If you have multiple sub-domains in this application and wish to upload a wild card certificate or a certificate with multiple domains in the Subject Alternate Name(SAN), you will need to upload the certificate for each domain.

You will see a button entitled ""Specify a custom certificate"", or ""Change custom certificate"" depending on whether you have previously uploaded a custom certificate for the application. This will render two input boxes on the page — one for ""Public certificate & chain"" and the other for ""Private key.""


### 3) Paste certificate and key

You will need to copy the site certificate and any intermediate certificates into the ""Public certificate & chain"" box.

The order of certificates needs to be domain certificate first, followed by any intermediate certificate(s) in order. Make sure NOT to include the root certificate. The certificates should be PEM encoded and will look like this in a text editor:

    
    -----BEGIN CERTIFICATE-----
    /* contents of domain certificate */
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate */
    -----END CERTIFICATE-----

If there are multiple intermediate certificates, you will need to make sure they are in the correct order.

    -----BEGIN CERTIFICATE-----
    /* contents of domain certificate */
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate 1*/
    -----END CERTIFICATE-----
    -----BEGIN CERTIFICATE-----
    /* contents of intermediate certificate 2*/
    -----END CERTIFICATE-----

Now copy and paste your private key in to the ""Private key"" input area. the private key should look like:

    -----BEGIN RSA PRIVATE KEY-----
    /* contents of private key */
    -----END RSA PRIVATE KEY-----



### 4) Upload 

Once you have copy and pasted the certificates and private key, click the ""Save Changes"" button. The portal will perform a check to make sure the certificate is for the correct domain, and the private key is a match for the certificate. Once that is accepted, a deployment is made to the Section platform and you should see the new certificate on the site in moments. You will also see the uploaded certificate information at the right hand side of the HTTPS page.
",591
Set Location Preferences,Examples,"The first and simplest scenario is when you want your app to be deployed according to current traffic patterns. You are satisfied with the default values of 2, 5, and 20 for the minimumLocations, maximumLocations, and minimumUsagePerLocation, respectively, so you don't need to enter any values for those. Also, you don't have any demands such as ""always have at least one location in Europe"", so the LocationOptimizer is free to choose from any existing location to host your app. That situation would be captured by this ConfigMap:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic""
        }
    }
metadata:
  name: location-optimizer
```

Apply the above ConfigMap to your project using either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or the following `kubectl` command:

```bash
kubectl apply -f location-optimizer.yaml
```

You can adjust the defaults as needed. For example, if you want more coverage across the globe, you could increase minimumLocations from 2 to 5, giving you exactly 5 locations at all times, dynamically positioned according to traffic:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 5
        }
    }
metadata:
  name: location-optimizer
```

A drawback with such a simple ConfigMap above is when you don't have reliable traffic being served by your app. This is common in the early stages of development. The LocationOptimizer is being told to select locations according to traffic but the traffic signal doesn't exist. We have methods to handle these ""fallback"" situations, but you can exert more control to obtain a more desirable, predictable outcome. The best way to handle this is to add add the **mustInclude** parameter to your ConfigMap. (See Tips section below for low/no traffic apps). 

For example, we could add 2 elements to the **mustInclude** array to ensure that we will always have a presence in these locations. If there is no traffic, then these will be your only locations. If there is traffic, you will have these locations and anything else that is warranted by the traffic. Here is what the ConfigMap would look like if we forced a presence in Europe and North America:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

The **mustInclude** parameter accepts an array of objects. Supported keys for the objects are listed in our [Location Parameters](/reference/solver-service-parameters/) reference guide.

It is worth taking a moment to restate what this ConfigMap represents. This is a dynamic config, so traffic patterns will be analysed to choose suitable locations. And there are **mustInclude** conditions, so these must be met under any conditions. It gives you a few known locations but others can be added as needed according to the traffic analysis. If you want your **mustInclude** locations to be more specific, you can consult our [Location Parameters](/reference/solver-service-parameters/) reference guide and use suitable ""locationcode"" references like this:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""locationcode"": ""sfo""
                },
                {
                    ""locationcode"": ""ams""
                },
                {
                    ""locationcode"": ""syd""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

This will cause the selection of Amsterdam and Sydney no matter what, and up to 3 more locations will be added where and when they are needed according to traffic patterns, because the default value for maximumLocations is 5.

Finally, Enterprise plans allow you to require locations according to special capabilities. For example, the following ConfigMap constrains your workload to running at only those locations that support [PCI](https://www.pcisecuritystandards.org/) (Payment Card Industry) compliance: 

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""chooseFrom"": ""pci""
        }
    }
metadata:
  name: location-optimizer
```

Another suitable ConfigMap for apps that do not serve HTTP traffic or for development scenarios (low or no traffic), is to use a static policy. A static policy is simply one in which you specify characteristics of locations that you want and this entirely defines the locations that are selected for you. No traffic signal is analysed. Your app will be deployed to suitable locations and stay there until the ConfigMap is changed. The parameters for static look very similar to the example given above. As noted [here](/explanations/aee/), the static policy does not honor any concept of minimum or maximum locations. The LocationOptimizer will try to meet your **mustInclude** array as exactly as possible. So a static deployment with locations in Europe and North America would be configured like this:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""static"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

The only explicit change is the name of the policy. Implicitly, you are saying: 'pay no attention to traffic and stay in only these locations until I tell you otherwise by updating my ConfigMap.' With static, the locations are stable and ""static"" over time, to the extent that is possible. So the first two locations selected will be used consistently unless some issue forces us to move your app. If we move it, we will again choose locations that meet the **mustInclude** conditions. 

In the dynamic version above where your app will always be in one European and one North American location, but there may be additional locations as well, if traffic is sufficient to warrant additional locations. Also, the locations within Europe and North America will change over time in response to traffic patterns.  

We offer multiple levels of geographic abstraction when specifying **mustInclude** conditions. You can exert more fine-grained control with something like ""locationcode"" and coarser control with ""region"". Beware that if you specify a fine-scale entity, like a city, then the LocationOptimizer has fewer options in meeting this condition. If, for some reason, there are no available facilities in that city, then your app cannot be scheduled there. Fall-back locations in such a scenario may seem arbitrary to the stated COnfigMap, depending on conditions. So the more general you can be, the better in terms of reliably obtaining your desired results.

# Tips
When the LocationOptimizer cannot obtain a solution based on your ConfigMap and current network and traffic conditions, it will check to see where your app is currently deployed. If it has current locations (i.e., is currently deployed), the LocationOptimixer will select those current locations, leaving your app where it is. If your app does not have any current locations, then two fall-back locations are returned, typically in NYC and SFO.

To avoid these situations and take more control over your outcomes:

1. Use a static policy when you initially deploy your app and when there is no meaningful traffic. This forces your app to deploy to locations of your choosing regardless of traffic. 

2. Once your app has been deployed and you want to do some load-testing or you are going to start serving traffic, then switching to a dynamic policy could make sense.

3. When using a dynamic policy with relatively little traffic, the selected locations can change often over a given time period. At low traffic levels, the signal is highly variable and can cause location flapping. If you are doing relatively little traffic, consider using a static policy to get in the places you want and stay there. The dynamic policy works best with strong, geographically-correlated traffic signals.",1874
Set Location Preferences,Learn to configure edge locations using the Kubernetes API,"
You can specify specific locations in which you would like your Project to run or let our Adaptive Edge Engine optimize the placement of your application across our global network of compute based on rules or strategies you define.

Use the [K8s Dashboard](/docs/guides/projects/manage-resources/) we provide or Kubectl to deploy a [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) named `location-optimizer` for each Project. Our platform control plane detects this ConfigMap and will use it to orchestrate the optimal distribution of your Project across our entire [Composable Edge Cloud](/docs/explanations/cec/).

There are many ways to specify the locations that you want. Please see the explanation of the [AEE](/explanations/aee/) to see an overview of LocationOptimizer parameters, their interpretations, and default values. Please also see our reference on the [traffic signal](/explanations/traffic-signal/) used when **policy** is dynamic. This guide will assume familiarity with these terms and concepts and will focus on examples of common scenarios to illustrate possible ConfigMaps.

:::note
You will not be able to specify specific locations or a minimum number of locations if they will cause your usage of RAM and CPU to exceed the boundaries of your current project plan (see [Pricing Plans](https://section.io/pricing/)).
:::

Below is an example of boilerplate ConfigMap to implement a dynamic policy. The `strategy` data is a JSON blob. The ConfigMap Name must be **location-optimizer**:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [],
            ""mustNotInclude"": [],
            ""minimumLocations"": 2,
            ""maximumLocations"": 5,
            ""minimumUsagePerLocation"": 20
        }
    }
metadata:
  name: location-optimizer
```
Above, all parameters except **policy** are at their default values. Therefore, an equivalent and simpler ConfigMap is:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic""
        }
    }
metadata:
  name: location-optimizer
```


# Examples

The first and simplest scenario is when you want your app to be deployed according to current traffic patterns. You are satisfied with the default values of 2, 5, and 20 for the minimumLocations, maximumLocations, and minimumUsagePerLocation, respectively, so you don't need to enter any values for those. Also, you don't have any demands such as ""always have at least one location in Europe"", so the LocationOptimizer is free to choose from any existing location to host your app. That situation would be captured by this ConfigMap:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic""
        }
    }
metadata:
  name: location-optimizer
```

Apply the above ConfigMap to your project using either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or the following `kubectl` command:

```bash
kubectl apply -f location-optimizer.yaml
```

You can adjust the defaults as needed. For example, if you want more coverage across the globe, you could increase minimumLocations from 2 to 5, giving you exactly 5 locations at all times, dynamically positioned according to traffic:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 5
        }
    }
metadata:
  name: location-optimizer
```

A drawback with such a simple ConfigMap above is when you don't have reliable traffic being served by your app. This is common in the early stages of development. The LocationOptimizer is being told to select locations according to traffic but the traffic signal doesn't exist. We have methods to handle these ""fallback"" situations, but you can exert more control to obtain a more desirable, predictable outcome. The best way to handle this is to add add the **mustInclude** parameter to your ConfigMap. (See Tips section below for low/no traffic apps). 

For example, we could add 2 elements to the **mustInclude** array to ensure that we will always have a presence in these locations. If there is no traffic, then these will be your only locations. If there is traffic, you will have these locations and anything else that is warranted by the traffic. Here is what the ConfigMap would look like if we forced a presence in Europe and North America:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

The **mustInclude** parameter accepts an array of objects. Supported keys for the objects are listed in our [Location Parameters](/reference/solver-service-parameters/) reference guide.

It is worth taking a moment to restate what this ConfigMap represents. This is a dynamic config, so traffic patterns will be analysed to choose suitable locations. And there are **mustInclude** conditions, so these must be met under any conditions. It gives you a few known locations but others can be added as needed according to the traffic analysis. If you want your **mustInclude** locations to be more specific, you can consult our [Location Parameters](/reference/solver-service-parameters/) reference guide and use suitable ""locationcode"" references like this:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""locationcode"": ""sfo""
                },
                {
                    ""locationcode"": ""ams""
                },
                {
                    ""locationcode"": ""syd""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

This will cause the selection of Amsterdam and Sydney no matter what, and up to 3 more locations will be added where and when they are needed according to traffic patterns, because the default value for maximumLocations is 5.

Finally, Enterprise plans allow you to require locations according to special capabilities. For example, the following ConfigMap constrains your workload to running at only those locations that support [PCI](https://www.pcisecuritystandards.org/) (Payment Card Industry) compliance: 

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""chooseFrom"": ""pci""
        }
    }
metadata:
  name: location-optimizer
```

Another suitable ConfigMap for apps that do not serve HTTP traffic or for development scenarios (low or no traffic), is to use a static policy. A static policy is simply one in which you specify characteristics of locations that you want and this entirely defines the locations that are selected for you. No traffic signal is analysed. Your app will be deployed to suitable locations and stay there until the ConfigMap is changed. The parameters for static look very similar to the example given above. As noted [here](/explanations/aee/), the static policy does not honor any concept of minimum or maximum locations. The LocationOptimizer will try to meet your **mustInclude** array as exactly as possible. So a static deployment with locations in Europe and North America would be configured like this:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""static"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```

The only explicit change is the name of the policy. Implicitly, you are saying: 'pay no attention to traffic and stay in only these locations until I tell you otherwise by updating my ConfigMap.' With static, the locations are stable and ""static"" over time, to the extent that is possible. So the first two locations selected will be used consistently unless some issue forces us to move your app. If we move it, we will again choose locations that meet the **mustInclude** conditions. 

In the dynamic version above where your app will always be in one European and one North American location, but there may be additional locations as well, if traffic is sufficient to warrant additional locations. Also, the locations within Europe and North America will change over time in response to traffic patterns.  

We offer multiple levels of geographic abstraction when specifying **mustInclude** conditions. You can exert more fine-grained control with something like ""locationcode"" and coarser control with ""region"". Beware that if you specify a fine-scale entity, like a city, then the LocationOptimizer has fewer options in meeting this condition. If, for some reason, there are no available facilities in that city, then your app cannot be scheduled there. Fall-back locations in such a scenario may seem arbitrary to the stated COnfigMap, depending on conditions. So the more general you can be, the better in terms of reliably obtaining your desired results.

# Tips
When the LocationOptimizer cannot obtain a solution based on your ConfigMap and current network and traffic conditions, it will check to see where your app is currently deployed. If it has current locations (i.e., is currently deployed), the LocationOptimixer will select those current locations, leaving your app where it is. If your app does not have any current locations, then two fall-back locations are returned, typically in NYC and SFO.

To avoid these situations and take more control over your outcomes:

1. Use a static policy when you initially deploy your app and when there is no meaningful traffic. This forces your app to deploy to locations of your choosing regardless of traffic. 

2. Once your app has been deployed and you want to do some load-testing or you are going to start serving traffic, then switching to a dynamic policy could make sense.

3. When using a dynamic policy with relatively little traffic, the selected locations can change often over a given time period. At low traffic levels, the signal is highly variable and can cause location flapping. If you are doing relatively little traffic, consider using a static policy to get in the places you want and stay there. The dynamic policy works best with strong, geographically-correlated traffic signals.  
",2391
DNS,Add Domains,"Once you have created a Section Project, you can define the domain/s you would like to point at that Project in addition to or instead of the domain created automatically by our platform for that project. 

Navigate to the settings tab for the Project you would like to add the Domain.

Click Add Domain and enter the details of the Domain you would like to add.

![Settings](/img/docs/add-domain.png)",82
DNS,Change DNS,"After the Domain has been added, you can change your public DNS records to direct traffic for that Domain to be sent to your application on this Section Project.

1. Select the domain name you wish to use.
2. Go to your DNS provider and modify the existing CNAME and A records. 
3. Add the CNAME as per instructions in the Section Console.
4. Click Verify to confirm engagment.

![Settings](/img/docs/changedns.png)

:::note
Propagation of DNS records throughout the Internet and ISP systems can take some time.  Follow our [guide to tracking DNS Propagation](/docs/get-started/dns-propagate/) to understand when your DNS changes are live through the Internet.
:::",148
DNS,Manage DNS and Domains on Section,"
We automatically deploy [HTTP Platform Extensions](/reference/http-extensions/) for each Project launched on our Platform. These extensions are not compulsory to use but do provide a simple way to accept HTTP/S traffic on the Section platform and send HTTP/S traffic to an upstream origin.

### Add Domains

Once you have created a Section Project, you can define the domain/s you would like to point at that Project in addition to or instead of the domain created automatically by our platform for that project. 

Navigate to the settings tab for the Project you would like to add the Domain.

Click Add Domain and enter the details of the Domain you would like to add.

![Settings](/img/docs/add-domain.png)

### Change DNS

After the Domain has been added, you can change your public DNS records to direct traffic for that Domain to be sent to your application on this Section Project.

1. Select the domain name you wish to use.
2. Go to your DNS provider and modify the existing CNAME and A records. 
3. Add the CNAME as per instructions in the Section Console.
4. Click Verify to confirm engagment.

![Settings](/img/docs/changedns.png)

:::note
Propagation of DNS records throughout the Internet and ISP systems can take some time.  Follow our [guide to tracking DNS Propagation](/docs/get-started/dns-propagate/) to understand when your DNS changes are live through the Internet.
:::



",294
Using K8s Dashboard,Launch the Kubernetes Dashboard,"From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page. 

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.",63
Using K8s Dashboard,"Add a K8s ""Resource""","Our tutorial pages have YAML files for Deployments, ConfigMaps, Services, and others. All of these are Kubernetes **resources** that can be applied to your Project using the Kubernetes Dashboard. 

From any of our tutorials, copy the YAML, then go to the dashboard and select the **+** from the upper right hand corner, choose ""Create from input"", and then paste your YAML into the text field. Select **Upload**. 

![location-optimizer](/img/docs/getting-started-location-optimizer.png)

Kubernetes resources such as [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/) and [Services](https://kubernetes.io/docs/concepts/services-networking/service/) can all be applied in this way.",171
Using K8s Dashboard,Delete Deployment,"To delete a resource from the Kubernetes Dashboard, in the list of deployments select the Actions menu (three vertical dots at the far right) and select Delete.

![Delete deployment](/img/docs/delete-deployment.png)",43
Using K8s Dashboard,Delete Service,"To delete a Service from the Kubernetes Dashboard (such as `ingress-upstream`, as required to begin many of our tutorials), in the list of Services select the Actions menu (three vertical dots at the far right) and select Delete.

![Delete Service](/img/docs/delete-service.png)",59
Using K8s Dashboard,Edit Deployment,"On the Kubernetes Dashboard you should see your deployment. In this example and if you chose to deploy our example container the deployment is named `section-project-deployment`. If you wanted to edit the resource consumption you will need to edit the deployment. 

![edit-deployment](/img/docs/getting-started-edit-deployment.png)

Edit the deployment YAML Section generated when you deployed your first project. For more information on configuration of resources see our guide on [Setting Resource Limits](/guides/projects/set-resource-limits).",104
Using K8s Dashboard,Change RAM and CPU Allocation,"To make changes to your resource allocation you will need to update the resources array.

![set-resources](/img/docs/getting-started-set-resources.png)

Select **Update** and your changes will be applied to your deployment. You can then use the Kubernetes dashboard to monitor the changes happening to your workload in real time. 

<!--- Next we will walk through how to [Configure your Project's Analytics Tooling](/get-started/project-analytics/) so you can make full use of Section's dynamic and distributed platform.--->",105
Using K8s Dashboard,Use the Kubernetes Dashboard to manage the Deployment Configuration your application,"
Once you have [Created a Project](/get-started/create-project/) you may want to make various changes, such as adding a new deployment resource, editing CPU and memory resources, deleting a deployment, etc. 

You can make such changes using the Kubernetes Dashboard provided in the Section Console or by using the Kubernetes API and [`kubectl`](/guides/kubernetes-ui/kubernetes-api/basics/). This doc shows how to make simple changes using the Kubernetes dashboard.

## Launch the Kubernetes Dashboard
From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page. 

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.

## Add a K8s ""Resource""
Our tutorial pages have YAML files for Deployments, ConfigMaps, Services, and others. All of these are Kubernetes **resources** that can be applied to your Project using the Kubernetes Dashboard. 

From any of our tutorials, copy the YAML, then go to the dashboard and select the **+** from the upper right hand corner, choose ""Create from input"", and then paste your YAML into the text field. Select **Upload**. 

![location-optimizer](/img/docs/getting-started-location-optimizer.png)

Kubernetes resources such as [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/), [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/) and [Services](https://kubernetes.io/docs/concepts/services-networking/service/) can all be applied in this way.

## Delete Deployment
To delete a resource from the Kubernetes Dashboard, in the list of deployments select the Actions menu (three vertical dots at the far right) and select Delete.

![Delete deployment](/img/docs/delete-deployment.png)

## Delete Service
To delete a Service from the Kubernetes Dashboard (such as `ingress-upstream`, as required to begin many of our tutorials), in the list of Services select the Actions menu (three vertical dots at the far right) and select Delete.

![Delete Service](/img/docs/delete-service.png)

## Edit Deployment
On the Kubernetes Dashboard you should see your deployment. In this example and if you chose to deploy our example container the deployment is named `section-project-deployment`. If you wanted to edit the resource consumption you will need to edit the deployment. 

![edit-deployment](/img/docs/getting-started-edit-deployment.png)

Edit the deployment YAML Section generated when you deployed your first project. For more information on configuration of resources see our guide on [Setting Resource Limits](/guides/projects/set-resource-limits).

## Change RAM and CPU Allocation
To make changes to your resource allocation you will need to update the resources array.

![set-resources](/img/docs/getting-started-set-resources.png)

Select **Update** and your changes will be applied to your deployment. You can then use the Kubernetes dashboard to monitor the changes happening to your workload in real time. 

<!--- Next we will walk through how to [Configure your Project's Analytics Tooling](/get-started/project-analytics/) so you can make full use of Section's dynamic and distributed platform.--->",672
Load Testing Projects,Things to Keep in Mind,"The traffic signal, described [here](/explanations/traffic-signal/), is smoothed. This has 2 impacts on load testing. First, it dilutes small, brief spikes. This is desirable in production, but it means small, brief blips of traffic will not result in very high traffic rates due to the smoothing. Second, if you switch load source locations abruptly, the traffic signal will smooth that out over the span window. You will have to wait while the signal from the first source location decays and that from the second source location grows. An effective load test would aim to deliver a strong signal (10+ rps) over a reasonable duration (5+ minutes) so that your signal is clear. You would then wait approximately 15 minutes before sending traffic from a new source location. The traffic rate you simulate needs to also account for the **minimumUsagePerLocation**.

The **minimumUsagePerLocation** parameter exists to dampen the reactions of the AEE:LO to minor variations in traffic rate and location. Generally, we do not want to spin up in a new location due to a trivial increase in the traffic rate from some distance source. For production traffic, the default of 20 rps has proven to be very valuable and effective. For load testing, you may want to reduce this to 0 to make the system maximally sensitive to the traffic you generate. Again, this is likely too sensitive for production use as it will result in thrashing, but could serve well in load testing.

The **minimumLocations** is another important parameter for production traffic. However, to most clearly see the impact of a load test, you might consider setting this parameter to 1. This way, the selected locations will be more directly attributable to your test traffic.",363
Load Testing Projects,Load Tests vs Production Traffic,"Load testing typically involves sending traffic to your Project from one location. Production traffic typically originates from thousands of different geographic locations simultaneously. It is difficult to evaluate the complex, dynamic response of the LocationOptimizer. As with all good testing, we will show you how to partition out and simplify your ConfigMap in order to evaluate it under even a simple, single-source testing scenario.

First, consider this ConfigMap:
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```
This specifies 2 locations, in Europe and North America, and the implicit default value for the **minimumUsagePerLocation** parameter is 20 rps. Now, suppose we want to test this ConfigMap using a tool like ApacheBench and a VPN. Let's say we want to see a location spin up in Asia, we simulate traffic from Singapore. 

Under these conditions, we will only see a new Asia location in the solution if the load results in 60 rps or greater in the traffic metric over several minutes. This is because we require the European and North American locations. For the given value of **minimumUsagePerLocation**, we will only get a third location if all existing locations can serve at least 20 rps, so it is only when we hit 60 rps that a third location is warranted. At that point, the location of the traffic finally starts to impact my solution and we should get a location close to the load test origin.

So we need to make sure we send a lot of traffic, or we could tailor the ConfigMap so that the solution is more sensitive, as illustrated in the examples below.

# Examples
The following ConfigMap will give a very quick response to simulated traffic from anywhere outside of Europe:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
      ""strategy"": ""SolverServiceV1"",
      ""params"": {
        ""policy"": ""dynamic"",
        ""mustInclude"": [
          {
            ""region"": ""europe""
          }
        ],
        ""minimumLocations"": 1,
        ""minimumUsagePerLocation"": 0
      }
    }
metadata:
  name: location-optimizer
```

The ConfigMap above will require at least one location at all times and that one location must be in Europe, even in the absence of traffic. This is a nice way to ensure that you know where your Project will be running while you prepare to simulate traffic. We can now examine two aspects of the configured behavior with a few different load tests. 

First, we can simulate traffic from various locations inside Europe and see the one location moving to be close to the traffic source. If you are switching the source location of your load quickly from one place to another, you may have multiple traffic signals due to the inherent smoothing. This would result in multiple locations being selected, with a new one added with each new traffic signal location. Because we reduced the **minimumUsagePerLocation** to 0, we can expect to see new locations spin up near any source of traffic.

Second, we can simulate traffic from outside Europe and we will see a nearby location spin up, while we also maintain a single location in Europe.",739
Load Testing Projects,Apache Bench (ab),"[Apache Bench](https://httpd.apache.org/docs/2.4/programs/ab.html) is a tool for benchmarking HTTP servers. You may find it useful in generating traffic against your new Section Project so that you can observe the behavior of the [AEE](/explanations/aee/).  When you use a tool such as this, make sure that you are generating enough traffic relative to your **minimumUsagePerLocation** parameter, as discussed earlier.  Use the ```-c concurrency``` argument to generate multiple requests at a time, using 10 or more as the concurrency value.  For example:

```
$ ab -n 10000 -c 10 https://myenv.myapp.com
```",150
Load Testing Projects,Some tips on running load tests against your applcation on Section.,"
This guide builds off of basic information contained in our [AEE](/explanations/aee/) explainer and the [Set Edge Locations](/guides/projects/set-edge-locations/) guide. We will also refer to parameter values that are catalogued [here](/reference/solver-service-parameters/). Please familiarize yourself with those documents before proceeding.

In the [Set Edge Locations](/guides/projects/set-edge-locations/) we recommend that you start out using a static **policy** in your LocationOptimizer ConfigMap. This is how you will be able to control where your Project is deployed in the absence of traffic. The following information will help you build confidence with a dynamic **policy** and load testing. 

# Things to Keep in Mind
The traffic signal, described [here](/explanations/traffic-signal/), is smoothed. This has 2 impacts on load testing. First, it dilutes small, brief spikes. This is desirable in production, but it means small, brief blips of traffic will not result in very high traffic rates due to the smoothing. Second, if you switch load source locations abruptly, the traffic signal will smooth that out over the span window. You will have to wait while the signal from the first source location decays and that from the second source location grows. An effective load test would aim to deliver a strong signal (10+ rps) over a reasonable duration (5+ minutes) so that your signal is clear. You would then wait approximately 15 minutes before sending traffic from a new source location. The traffic rate you simulate needs to also account for the **minimumUsagePerLocation**.

The **minimumUsagePerLocation** parameter exists to dampen the reactions of the AEE:LO to minor variations in traffic rate and location. Generally, we do not want to spin up in a new location due to a trivial increase in the traffic rate from some distance source. For production traffic, the default of 20 rps has proven to be very valuable and effective. For load testing, you may want to reduce this to 0 to make the system maximally sensitive to the traffic you generate. Again, this is likely too sensitive for production use as it will result in thrashing, but could serve well in load testing.

The **minimumLocations** is another important parameter for production traffic. However, to most clearly see the impact of a load test, you might consider setting this parameter to 1. This way, the selected locations will be more directly attributable to your test traffic. 

# Load Tests vs Production Traffic
Load testing typically involves sending traffic to your Project from one location. Production traffic typically originates from thousands of different geographic locations simultaneously. It is difficult to evaluate the complex, dynamic response of the LocationOptimizer. As with all good testing, we will show you how to partition out and simplify your ConfigMap in order to evaluate it under even a simple, single-source testing scenario.

First, consider this ConfigMap:
```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""mustInclude"": [
                {
                    ""region"": ""europe""
                },
                {
                    ""region"": ""northamerica""
                }
            ]
        }
    }
metadata:
  name: location-optimizer
```
This specifies 2 locations, in Europe and North America, and the implicit default value for the **minimumUsagePerLocation** parameter is 20 rps. Now, suppose we want to test this ConfigMap using a tool like ApacheBench and a VPN. Let's say we want to see a location spin up in Asia, we simulate traffic from Singapore. 

Under these conditions, we will only see a new Asia location in the solution if the load results in 60 rps or greater in the traffic metric over several minutes. This is because we require the European and North American locations. For the given value of **minimumUsagePerLocation**, we will only get a third location if all existing locations can serve at least 20 rps, so it is only when we hit 60 rps that a third location is warranted. At that point, the location of the traffic finally starts to impact my solution and we should get a location close to the load test origin.

So we need to make sure we send a lot of traffic, or we could tailor the ConfigMap so that the solution is more sensitive, as illustrated in the examples below.

# Examples
The following ConfigMap will give a very quick response to simulated traffic from anywhere outside of Europe:

```yaml title=""location-optimizer.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
      ""strategy"": ""SolverServiceV1"",
      ""params"": {
        ""policy"": ""dynamic"",
        ""mustInclude"": [
          {
            ""region"": ""europe""
          }
        ],
        ""minimumLocations"": 1,
        ""minimumUsagePerLocation"": 0
      }
    }
metadata:
  name: location-optimizer
```

The ConfigMap above will require at least one location at all times and that one location must be in Europe, even in the absence of traffic. This is a nice way to ensure that you know where your Project will be running while you prepare to simulate traffic. We can now examine two aspects of the configured behavior with a few different load tests. 

First, we can simulate traffic from various locations inside Europe and see the one location moving to be close to the traffic source. If you are switching the source location of your load quickly from one place to another, you may have multiple traffic signals due to the inherent smoothing. This would result in multiple locations being selected, with a new one added with each new traffic signal location. Because we reduced the **minimumUsagePerLocation** to 0, we can expect to see new locations spin up near any source of traffic.

Second, we can simulate traffic from outside Europe and we will see a nearby location spin up, while we also maintain a single location in Europe.

# Apache Bench (ab)
[Apache Bench](https://httpd.apache.org/docs/2.4/programs/ab.html) is a tool for benchmarking HTTP servers. You may find it useful in generating traffic against your new Section Project so that you can observe the behavior of the [AEE](/explanations/aee/).  When you use a tool such as this, make sure that you are generating enough traffic relative to your **minimumUsagePerLocation** parameter, as discussed earlier.  Use the ```-c concurrency``` argument to generate multiple requests at a time, using 10 or more as the concurrency value.  For example:

```
$ ab -n 10000 -c 10 https://myenv.myapp.com
```
",1423
Project Details,Rename Project,"Every Project will have a unique name and a unique URL which we will create and assign to your project when initally launched.

To rename your Project, select the Settings for that project and then the icon to change the project name.  Note, this will not change the URL assigned to your project.

![Settings](/img/docs/rename.png)",70
Project Details,Section Project ID,"This is your unique Project ID which is not to be confused with your Section Account ID.  You can also see both the Account ID and the Project ID in your browser's URL indicator

`https://console.section.io/overview/account/SECTION_ACCOUNT_ID/project/PROJECT_ID`.",56
Project Details,Kubernetes API Endpoint,"You will also see on the General Settings Tab, a Kubernetes API Endpoint.  This is unique Kubnernetes API to which you can point your existing Kubernetes tooling such as other Kubernetes UIs or your CI/CD tooling.",47
Project Details,View and Change general settings for your Project,"
Once your project has been set up you can view and modify the settings for that Project at any time by selecting the settings for that Project from the main Projects List view.

![Settings](/img/docs/settings.png)

## Rename Project 

Every Project will have a unique name and a unique URL which we will create and assign to your project when initally launched.

To rename your Project, select the Settings for that project and then the icon to change the project name.  Note, this will not change the URL assigned to your project.

![Settings](/img/docs/rename.png)

## Section Project ID

This is your unique Project ID which is not to be confused with your Section Account ID.  You can also see both the Account ID and the Project ID in your browser's URL indicator

`https://console.section.io/overview/account/SECTION_ACCOUNT_ID/project/PROJECT_ID`.


## Kubernetes API Endpoint

You will also see on the General Settings Tab, a Kubernetes API Endpoint.  This is unique Kubnernetes API to which you can point your existing Kubernetes tooling such as other Kubernetes UIs or your CI/CD tooling.

",232
2 Factor Authentication,Enable two-factor authentication,"Follow the steps below to enable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your My Profile setting, toggle the 2FA switch from **Disabled** to **Enabled**.
4. Select how you would like to enable 2FA, either through SMS or an authentication app, and then click **Save**.

![Settings](/img/docs/2fa.png)

:::note
Submit a [support request](https://support.section.io/hc/en-us/requests/new) if you would like 2FA to be required for all users invited to your account.
:::",137
2 Factor Authentication,Disable two-factor authentication,"Follow the steps below to disable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your user setting, toggle the 2FA switch from **Enabled** to **Disabled** and then click **Save**.

:::note
Submit a [support request](https://support.section.io/hc/en-us/requests/new) from your user email address if you are no longer able to provide the second form of authentication and need 2FA to be disabled by Section. Please keep in mind our 2FA reset process requires extensive verification and can take up to 7 days to be completed.
:::",139
2 Factor Authentication,Learn how to enable Two Factor Authentication for your Users.,"---
title: 2 Factor Authentication
description: Learn how to enable Two Factor Authentication for your Users.
keywords:
  - profile
  - 2fa
sidebar_position: 5
---

## Enable two-factor authentication

Follow the steps below to enable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your My Profile setting, toggle the 2FA switch from **Disabled** to **Enabled**.
4. Select how you would like to enable 2FA, either through SMS or an authentication app, and then click **Save**.

![Settings](/img/docs/2fa.png)

:::note
Submit a [support request](https://support.section.io/hc/en-us/requests/new) if you would like 2FA to be required for all users invited to your account.
:::

## Disable two-factor authentication

Follow the steps below to disable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your user setting, toggle the 2FA switch from **Enabled** to **Disabled** and then click **Save**.

:::note
Submit a [support request](https://support.section.io/hc/en-us/requests/new) from your user email address if you are no longer able to provide the second form of authentication and need 2FA to be disabled by Section. Please keep in mind our 2FA reset process requires extensive verification and can take up to 7 days to be completed.
:::",328
Account,Account Details,"You can view and modify your details for the account under the My Account tab

![Settings](/img/docs/account.png)",25
Account,User Access Management,Please see our article on [User Management](/guides/iam/users/) to learn more about how to add users to your account and how to set the appropriate permissions.,35
Account,Delete Account,"If you would like to have your account deleted, please submit a [support request](https://support.section.io/hc/en-us/requests/new).",30
Account,Learn how to Manage your Section Account.,"
When you sign up to Section, for the first time, you will be created as an Admin User on your own Section Account.  In this account you will be able to add Projects and Users.

## Account Details

You can view and modify your details for the account under the My Account tab

![Settings](/img/docs/account.png)


## User Access Management

Please see our article on [User Management](/guides/iam/users/) to learn more about how to add users to your account and how to set the appropriate permissions.


## Delete Account

If you would like to have your account deleted, please submit a [support request](https://support.section.io/hc/en-us/requests/new).
",144
Projects,Add a Section Project,To create a new Section Project follow our getting started [Creating Project](/get-started/create-project/) guide.,23
Projects,For Other Project Actions,View our [Project Guides](/guides/projects/) for more Project actions.,16
Projects,Learn how to manage your Section Projects.,"
A Project is your Section entity for managing each of your applications deployed on the Section platform.  You may have many Projects in your Account and add multiple Users to yor Projects.

The main Projects page for your Account will display a list of your active Projects.

### Add a Section Project

To create a new Section Project follow our getting started [Creating Project](/get-started/create-project/) guide.

### For Other Project Actions

View our [Project Guides](/guides/projects/) for more Project actions.",104
API Tokens,Create API token,"Follow the steps below to create an API token:

1. Log in to the Section console.
2. In the left sidebar, click **API Tokens**.
3. In the **Token Description** field, enter a description for what this API token will be used for and 
4. Copy and Securely store the created API token.
5. then click **Add**.

![Settings](/img/docs/api-token.png)",86
API Tokens,Delete API token,"Follow the steps below to delete an API token:

1. Log in to the Section console.
2. In the left sidebar, click **API Tokens**.
3. In the API tokens list, find the API token you would like to delete and then click the **X** button in the **Delete** column.",64
API Tokens,Learn how to manage API tokens in the Section console.,"
Learn how to manage API tokens in the Section console.

:::note
API tokens belong to users and share the same permissions as the user they belong to. When a user is removed from a Section account the API tokens associated with that user will not be deleted.
:::

## Create API token

Follow the steps below to create an API token:

1. Log in to the Section console.
2. In the left sidebar, click **API Tokens**.
3. In the **Token Description** field, enter a description for what this API token will be used for and 
4. Copy and Securely store the created API token.
5. then click **Add**.

![Settings](/img/docs/api-token.png)

## Delete API token

Follow the steps below to delete an API token:

1. Log in to the Section console.
2. In the left sidebar, click **API Tokens**.
3. In the API tokens list, find the API token you would like to delete and then click the **X** button in the **Delete** column.
",216
Manage Users,Add Users to Your Account,"Follow the steps below to enable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your Manage Users setting, add the details of the User you would like to invite to your Account.
4. Then click **Invite User**.

![Settings](/img/docs/add-user.png)",74
Manage Users,Learn how to Manage users for your Account.,"---
title: Manage Users
description: Learn how to Manage users for your Account.
keywords:
  - profile
  - Users
sidebar_position: 3
---

## Add Users to Your Account

Follow the steps below to enable two-factor authentication (2FA):

1. Log in to your Section account.
2. Select the **My Account** Tab
3. In your Manage Users setting, add the details of the User you would like to invite to your Account.
4. Then click **Invite User**.

![Settings](/img/docs/add-user.png)

",115
Single Sign On (SSO),Single Sign On (SSO) Enable SSO for your Section account.,"SSO and RBAC are available to Enterprise Customers.

To enable SSO for your Enterprise Section Account please submit a [support request](https://support.section.io/hc/en-us/requests/new).",40
Single Sign On (SSO),Enable SSO for your Section account.,"
SSO and RBAC are available to Enterprise Customers.

To enable SSO for your Enterprise Section Account please submit a [support request](https://support.section.io/hc/en-us/requests/new).
",41
Tunnel Client on Section,Section Client,"This example demonstrate the essence of:
* Having a MySQL client in k8s that connects to a k8s ssh-tunnel client and communicate locally as if it was the MySQL server.

It relies on:
* Existing tunnel server & MySQL server.
* `kubectl` configured to connect to your Section Project.",62
Tunnel Client on Section,Script,"```bash
 # Produces TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY
 generate_tunnel_keys() {
    while read -r line; do
        [ ""${line}"" == ""SERVER (PUBLIC) KEY:"" ] && read -r TUNNEL_SERVER_KEY
        [ ""${line}"" == ""CLIENT (PRIVATE) KEY:"" ] && read -r TUNNEL_CLIENT_KEY
    done < <(docker run --rm ghcr.io/section/section-secure-tunnel:sha-05d9f6a keygen)
    export TUNNEL_CLIENT_KEY
    export TUNNEL_SERVER_KEY
    echo ""Exported TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY""
}

 # Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS
 setup_k8s_client(){
    kubectl delete secret ssh-tunnel || true
    kubectl create secret generic ssh-tunnel \
        --from-literal=TUNNEL_ADDRESS=""${TUNNEL_ADDRESS}"" \
        --from-literal=TUNNEL_CLIENT_KEY=""${TUNNEL_CLIENT_KEY}"" \
        --from-literal=REMOTE_SERVICE_ADDRESS=""${REMOTE_SERVICE_ADDRESS}""

    kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ssh-tunnel
  labels:
    app: ssh-tunnel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ssh-tunnel
  template:
    metadata:
      labels:
        app: ssh-tunnel
    spec:
      containers:
        - name: ssh-tunnel
          image: 'ghcr.io/section/section-secure-tunnel:sha-05d9f6a'
          env:
          - name: REMOTE_SERVICE_ADDRESS
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: REMOTE_SERVICE_ADDRESS
                optional: false
          - name: TUNNEL_ADDRESS
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: TUNNEL_ADDRESS
                optional: false
          - name: TUNNEL_CLIENT_KEY
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: TUNNEL_CLIENT_KEY
                optional: false
          args: [
            ""client"",
            ""$(TUNNEL_ADDRESS)"",
            ""key"",
            ""remote_user_name"",
            ""$(TUNNEL_CLIENT_KEY)"",
            ""3306:$(REMOTE_SERVICE_ADDRESS):3306""
            ]
          resources:
            requests:
              cpu: 0.5
              memory: 512Mi
            limits:
              cpu: 0.5
              memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ssh-tunnel
  name: ssh-tunnel
spec:
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    app: ssh-tunnel
EOF
}

 setup_workload(){
    kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-client
  labels:
    app: mysql-client
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql-client
  template:
    metadata:
      labels:
        app: mysql-client
    spec:
      containers:
        - name: mysql-client
          image: 'mysql:8.0'
          imagePullPolicy: Always
          env:
          - name: MYSQL_PWD
            value: masteruserpassword
          command: [""/bin/sh""]
          args: [
            ""-c"",
            ""/usr/bin/mysql --silent --host=ssh-tunnel --user=masterusername --execute='SELECT VERSION()' && tail -f /dev/null""
            ]
          resources:
            requests:
              cpu: 0.5
              memory: 512Mi
            limits:
              cpu: 0.5
              memory: 512Mi
EOF
}

setup_k8s_client
setup_workload
```

Resulting workload pods:
```
$ kubectl logs mysql-client-db67b59ff-ktzvz
8.0.28
```",900
Tunnel Client on Section,...,"
# Section Client
This example demonstrate the essence of:
* Having a MySQL client in k8s that connects to a k8s ssh-tunnel client and communicate locally as if it was the MySQL server.

It relies on:
* Existing tunnel server & MySQL server.
* `kubectl` configured to connect to your Section Project.

# Script
```bash
 # Produces TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY
 generate_tunnel_keys() {
    while read -r line; do
        [ ""${line}"" == ""SERVER (PUBLIC) KEY:"" ] && read -r TUNNEL_SERVER_KEY
        [ ""${line}"" == ""CLIENT (PRIVATE) KEY:"" ] && read -r TUNNEL_CLIENT_KEY
    done < <(docker run --rm ghcr.io/section/section-secure-tunnel:sha-05d9f6a keygen)
    export TUNNEL_CLIENT_KEY
    export TUNNEL_SERVER_KEY
    echo ""Exported TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY""
}

 # Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS
 setup_k8s_client(){
    kubectl delete secret ssh-tunnel || true
    kubectl create secret generic ssh-tunnel \
        --from-literal=TUNNEL_ADDRESS=""${TUNNEL_ADDRESS}"" \
        --from-literal=TUNNEL_CLIENT_KEY=""${TUNNEL_CLIENT_KEY}"" \
        --from-literal=REMOTE_SERVICE_ADDRESS=""${REMOTE_SERVICE_ADDRESS}""

    kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ssh-tunnel
  labels:
    app: ssh-tunnel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ssh-tunnel
  template:
    metadata:
      labels:
        app: ssh-tunnel
    spec:
      containers:
        - name: ssh-tunnel
          image: 'ghcr.io/section/section-secure-tunnel:sha-05d9f6a'
          env:
          - name: REMOTE_SERVICE_ADDRESS
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: REMOTE_SERVICE_ADDRESS
                optional: false
          - name: TUNNEL_ADDRESS
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: TUNNEL_ADDRESS
                optional: false
          - name: TUNNEL_CLIENT_KEY
            valueFrom:
              secretKeyRef:
                name: ssh-tunnel
                key: TUNNEL_CLIENT_KEY
                optional: false
          args: [
            ""client"",
            ""$(TUNNEL_ADDRESS)"",
            ""key"",
            ""remote_user_name"",
            ""$(TUNNEL_CLIENT_KEY)"",
            ""3306:$(REMOTE_SERVICE_ADDRESS):3306""
            ]
          resources:
            requests:
              cpu: 0.5
              memory: 512Mi
            limits:
              cpu: 0.5
              memory: 512Mi
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ssh-tunnel
  name: ssh-tunnel
spec:
  ports:
  - name: mysql
    port: 3306
    protocol: TCP
    targetPort: 3306
  selector:
    app: ssh-tunnel
EOF
}

 setup_workload(){
    kubectl apply -f - <<'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-client
  labels:
    app: mysql-client
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mysql-client
  template:
    metadata:
      labels:
        app: mysql-client
    spec:
      containers:
        - name: mysql-client
          image: 'mysql:8.0'
          imagePullPolicy: Always
          env:
          - name: MYSQL_PWD
            value: masteruserpassword
          command: [""/bin/sh""]
          args: [
            ""-c"",
            ""/usr/bin/mysql --silent --host=ssh-tunnel --user=masterusername --execute='SELECT VERSION()' && tail -f /dev/null""
            ]
          resources:
            requests:
              cpu: 0.5
              memory: 512Mi
            limits:
              cpu: 0.5
              memory: 512Mi
EOF
}

setup_k8s_client
setup_workload
```

Resulting workload pods:
```
$ kubectl logs mysql-client-db67b59ff-ktzvz
8.0.28
```",970
Tunnel Server using Docker and AWS,Script,```bash,2
Tunnel Server using Docker and AWS,!/bin/bash,"# Produces REMOTE_SERVICE_ADDRESS
 create_database(){
    local db_instance_name=""demo-privatedb-$(date +%s)""

    aws rds create-db-instance \
        --db-instance-identifier ""${db_instance_name}"" \
        --db-instance-class db.t4g.micro \
        --engine mysql \
        --master-username masterusername \
        --master-user-password masteruserpassword \
        --allocated-storage 20 \
        --no-publicly-accessible \
        >/dev/null

    aws rds wait db-instance-available \
        --db-instance-identifier ""${db_instance_name}"" 

    REMOTE_SERVICE_ADDRESS=$(aws rds describe-db-instances \
         --db-instance-identifier ""${db_instance_name}"" \
         --query ""DBInstances[0].Endpoint.Address"" \
         --output text)
    export REMOTE_SERVICE_ADDRESS
    echo ""Exported REMOTE_SERVICE_ADDRESS=${REMOTE_SERVICE_ADDRESS}""
}

 # Produces TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY
 generate_tunnel_keys() {
    while read -r line; do
        [ ""${line}"" == ""SERVER (PUBLIC) KEY:"" ] && read -r TUNNEL_SERVER_KEY
        [ ""${line}"" == ""CLIENT (PRIVATE) KEY:"" ] && read -r TUNNEL_CLIENT_KEY
    done < <(docker run --rm ghcr.io/section/section-secure-tunnel:sha-05d9f6a keygen)
    export TUNNEL_CLIENT_KEY
    export TUNNEL_SERVER_KEY
    echo ""Exported TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY""
}

 # Consumes TUNNEL_SERVER_KEY
 # Produces TUNNEL_ADDRESS
 setup_remote_server(){
    local cluster_name=default
    local tunnel_server_service_name=""demo-ssh-tunnel-$(date +%s)""

    # ensure cluster
    aws ecs create-cluster \
        --cluster-name ""${cluster_name}"" \
        >/dev/null

    # crete task definition
    local task_definition_arn=$(aws ecs register-task-definition \
        --requires-compatibilities FARGATE \
        --family ""${tunnel_server_service_name}"" \
        --network-mode awsvpc \
        --cpu=256 \
        --memory=512 \
        --container-definitions ""[{\""name\"":\""ssh-tunnel\"",\""image\"":\""ghcr.io/section/section-secure-tunnel:sha-05d9f6a\"",\""cpu\"":256,\""command\"":[\""server\"",\""key\"",\""remote_user_name\"",\""${TUNNEL_SERVER_KEY}\"",\""any\""],\""memory\"":512,\""essential\"":true}]"" \
        --query ""taskDefinition.taskDefinitionArn"" \
        --output text \
        )

    # create security group
    local group_id=$(aws ec2 create-security-group \
        --group-name ""${tunnel_server_service_name}"" \
        --description ""ssh-tunnel inbound ${tunnel_server_service_name}"" \
        --query ""GroupId"" \
        --output text)

    # allow all to port 2022
    aws ec2 authorize-security-group-ingress \
        --group-id ""${group_id}"" \
        --protocol tcp \
        --port 2022 \
        --cidr ""0.0.0.0/0"" \
        >/dev/null

    # allow to talk to default SG

    # get all subnets
    local allsubnet=$(aws ec2 describe-subnets \
        --query ""join(',',Subnets[].SubnetId)"" \
        --output text \
        )

    # default_group_id
    local default_group_id=$(aws ec2 describe-security-groups \
        --group-names default \
        --query ""SecurityGroups[0].GroupId"" \
        --output text \
        )

    # create service
    aws ecs create-service \
        --no-cli-pager \
        --service-name ""${tunnel_server_service_name}"" \
        --task-definition ""${task_definition_arn}"" \
        --desired-count 1 \
        --launch-type ""FARGATE"" \
        --network-configuration ""awsvpcConfiguration={subnets=[${allsubnet}],assignPublicIp=ENABLED,securityGroups=[${default_group_id},${group_id}]}"" \
        > /dev/null

    # wait for service to start
    aws ecs wait services-stable --services ""${tunnel_server_service_name}""

    # get the task
    local taskArn=$(aws ecs list-tasks \
        --service-name ""${tunnel_server_service_name}"" \
        --query 'taskArns[0]' \
        --output text) 
 
    # get the network interface 
    local networkInterfaceId=$(aws ecs describe-tasks \
        --tasks ""${taskArn}"" \
        --query ""tasks[0].attachments[0].details[?name=='networkInterfaceId'].value"" \
        --output text)

    # get the public IP
    TUNNEL_ADDRESS=$(aws ec2 describe-network-interfaces \
        --network-interface-ids ${networkInterfaceId} \
        --query ""NetworkInterfaces[0].Association.PublicIp"" \
        --output text)

    export TUNNEL_ADDRESS
    echo ""Exported TUNNEL_ADDRESS: ${TUNNEL_ADDRESS}""
}",1072
Tunnel Server using Docker and AWS,"Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS","test_local_client(){
    # start local ssh-tunnel client
    docker run \
        --name ssh-tunnel-client \
        -d --restart unless-stopped \
        -e TUNNEL_CLIENT_KEY \
        ghcr.io/section/section-secure-tunnel:sha-05d9f6a \
            client \
            ""${TUNNEL_ADDRESS}"" \
            key \
            remote_user_name \
            '${TUNNEL_CLIENT_KEY}' \
            ""3306:${REMOTE_SERVICE_ADDRESS}:3306"" \
        > /dev/null

    # get IP address of client
    local client_ip=$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' ssh-tunnel-client)

    # give  tunnel chance to connect
    sleep 1
    
    # Connect and use!
    echo -n ""Talking to remote db via tunnel to get server version: ""
    MYSQL_PWD=masteruserpassword docker run -it --rm -e MYSQL_PWD --entrypoint ""/bin/sh"" mysql:8.0 ""-c"" ""/usr/bin/mysql --silent --host=${client_ip} --user=masterusername --execute='SELECT VERSION()' && read""

    # clean up
    docker rm -f ssh-tunnel-client > /dev/null
}

create_database
generate_tunnel_keys
setup_remote_server
test_local_client
```

Running the above script should result in:
```
Running with Tunnel Public IP: n.n.n.n
Talking to remote db via tunnel to get server version: 8.0.28
```",321
Tunnel Server using Docker and AWS,...,"
This example demonstrates the essence of:
* an AWS RDS MySQL server that is not publicly accessible 
* an AWS ECS Fargate Task ssh-tunnel server that is publicly accessible 
* running a local docker ssh-tunnel client
* Using MySQL client (via docker) to connect to the local ssh-tunnel client and communicate as if it was the MySQL server

It relies on:
* ""default"" security group allowing all communication between each other
* functional aws-cli and docker on the local machine

When the script is finished it will leave the RDS instance available for testing with the [Secure Tunnel on Section](/guides/secure-tunnel/secure-tunnel-section/) guide.

# Script
```bash
#!/bin/bash

 # Produces REMOTE_SERVICE_ADDRESS
 create_database(){
    local db_instance_name=""demo-privatedb-$(date +%s)""

    aws rds create-db-instance \
        --db-instance-identifier ""${db_instance_name}"" \
        --db-instance-class db.t4g.micro \
        --engine mysql \
        --master-username masterusername \
        --master-user-password masteruserpassword \
        --allocated-storage 20 \
        --no-publicly-accessible \
        >/dev/null

    aws rds wait db-instance-available \
        --db-instance-identifier ""${db_instance_name}"" 

    REMOTE_SERVICE_ADDRESS=$(aws rds describe-db-instances \
         --db-instance-identifier ""${db_instance_name}"" \
         --query ""DBInstances[0].Endpoint.Address"" \
         --output text)
    export REMOTE_SERVICE_ADDRESS
    echo ""Exported REMOTE_SERVICE_ADDRESS=${REMOTE_SERVICE_ADDRESS}""
}

 # Produces TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY
 generate_tunnel_keys() {
    while read -r line; do
        [ ""${line}"" == ""SERVER (PUBLIC) KEY:"" ] && read -r TUNNEL_SERVER_KEY
        [ ""${line}"" == ""CLIENT (PRIVATE) KEY:"" ] && read -r TUNNEL_CLIENT_KEY
    done < <(docker run --rm ghcr.io/section/section-secure-tunnel:sha-05d9f6a keygen)
    export TUNNEL_CLIENT_KEY
    export TUNNEL_SERVER_KEY
    echo ""Exported TUNNEL_CLIENT_KEY, TUNNEL_SERVER_KEY""
}

 # Consumes TUNNEL_SERVER_KEY
 # Produces TUNNEL_ADDRESS
 setup_remote_server(){
    local cluster_name=default
    local tunnel_server_service_name=""demo-ssh-tunnel-$(date +%s)""

    # ensure cluster
    aws ecs create-cluster \
        --cluster-name ""${cluster_name}"" \
        >/dev/null

    # crete task definition
    local task_definition_arn=$(aws ecs register-task-definition \
        --requires-compatibilities FARGATE \
        --family ""${tunnel_server_service_name}"" \
        --network-mode awsvpc \
        --cpu=256 \
        --memory=512 \
        --container-definitions ""[{\""name\"":\""ssh-tunnel\"",\""image\"":\""ghcr.io/section/section-secure-tunnel:sha-05d9f6a\"",\""cpu\"":256,\""command\"":[\""server\"",\""key\"",\""remote_user_name\"",\""${TUNNEL_SERVER_KEY}\"",\""any\""],\""memory\"":512,\""essential\"":true}]"" \
        --query ""taskDefinition.taskDefinitionArn"" \
        --output text \
        )

    # create security group
    local group_id=$(aws ec2 create-security-group \
        --group-name ""${tunnel_server_service_name}"" \
        --description ""ssh-tunnel inbound ${tunnel_server_service_name}"" \
        --query ""GroupId"" \
        --output text)

    # allow all to port 2022
    aws ec2 authorize-security-group-ingress \
        --group-id ""${group_id}"" \
        --protocol tcp \
        --port 2022 \
        --cidr ""0.0.0.0/0"" \
        >/dev/null

    # allow to talk to default SG

    # get all subnets
    local allsubnet=$(aws ec2 describe-subnets \
        --query ""join(',',Subnets[].SubnetId)"" \
        --output text \
        )

    # default_group_id
    local default_group_id=$(aws ec2 describe-security-groups \
        --group-names default \
        --query ""SecurityGroups[0].GroupId"" \
        --output text \
        )

    # create service
    aws ecs create-service \
        --no-cli-pager \
        --service-name ""${tunnel_server_service_name}"" \
        --task-definition ""${task_definition_arn}"" \
        --desired-count 1 \
        --launch-type ""FARGATE"" \
        --network-configuration ""awsvpcConfiguration={subnets=[${allsubnet}],assignPublicIp=ENABLED,securityGroups=[${default_group_id},${group_id}]}"" \
        > /dev/null

    # wait for service to start
    aws ecs wait services-stable --services ""${tunnel_server_service_name}""

    # get the task
    local taskArn=$(aws ecs list-tasks \
        --service-name ""${tunnel_server_service_name}"" \
        --query 'taskArns[0]' \
        --output text) 
 
    # get the network interface 
    local networkInterfaceId=$(aws ecs describe-tasks \
        --tasks ""${taskArn}"" \
        --query ""tasks[0].attachments[0].details[?name=='networkInterfaceId'].value"" \
        --output text)

    # get the public IP
    TUNNEL_ADDRESS=$(aws ec2 describe-network-interfaces \
        --network-interface-ids ${networkInterfaceId} \
        --query ""NetworkInterfaces[0].Association.PublicIp"" \
        --output text)

    export TUNNEL_ADDRESS
    echo ""Exported TUNNEL_ADDRESS: ${TUNNEL_ADDRESS}""
}

# Consumes TUNNEL_ADDRESS, TUNNEL_CLIENT_KEY, REMOTE_SERVICE_ADDRESS
 test_local_client(){
    # start local ssh-tunnel client
    docker run \
        --name ssh-tunnel-client \
        -d --restart unless-stopped \
        -e TUNNEL_CLIENT_KEY \
        ghcr.io/section/section-secure-tunnel:sha-05d9f6a \
            client \
            ""${TUNNEL_ADDRESS}"" \
            key \
            remote_user_name \
            '${TUNNEL_CLIENT_KEY}' \
            ""3306:${REMOTE_SERVICE_ADDRESS}:3306"" \
        > /dev/null

    # get IP address of client
    local client_ip=$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' ssh-tunnel-client)

    # give  tunnel chance to connect
    sleep 1
    
    # Connect and use!
    echo -n ""Talking to remote db via tunnel to get server version: ""
    MYSQL_PWD=masteruserpassword docker run -it --rm -e MYSQL_PWD --entrypoint ""/bin/sh"" mysql:8.0 ""-c"" ""/usr/bin/mysql --silent --host=${client_ip} --user=masterusername --execute='SELECT VERSION()' && read""

    # clean up
    docker rm -f ssh-tunnel-client > /dev/null
}

create_database
generate_tunnel_keys
setup_remote_server
test_local_client
```

Running the above script should result in:
```
Running with Tunnel Public IP: n.n.n.n
Talking to remote db via tunnel to get server version: 8.0.28
```",1563
Kubernetes Dashboard,Kubernetes Dashboard Using Kubernetes Dashboard on Section,"We provide you with a Kubernetes Dashboard for every project you launch on Section.

![K8s Dashboard](/img/docs/ui-dashboard.png)

While the underlying number and location of pods running your application may be adjusted by our AEE from time to time based on your [location preferences](/docs/guides/projects/set-edge-locations/) the Kubernetes Dashboard we provide will always represent the running pods as a single pane of glass; **regardless of how many pods are running and where they may be running at any particular moment.**

You can use this Dashboard as described in the [Kubernetes Documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/) 

See also our [Guide for use of the Kubernetes Dashboard](/docs/guides/projects/manage-resources/) to manage some Section specific items.


To launch the Kubernetes Dashboard simply click on the project and choose Launch Dashboard.

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)",194
Kubernetes Dashboard,Using Kubernetes Dashboard on Section,"
We provide you with a Kubernetes Dashboard for every project you launch on Section.

![K8s Dashboard](/img/docs/ui-dashboard.png)

While the underlying number and location of pods running your application may be adjusted by our AEE from time to time based on your [location preferences](/docs/guides/projects/set-edge-locations/) the Kubernetes Dashboard we provide will always represent the running pods as a single pane of glass; **regardless of how many pods are running and where they may be running at any particular moment.**

You can use this Dashboard as described in the [Kubernetes Documentation](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/) 

See also our [Guide for use of the Kubernetes Dashboard](/docs/guides/projects/manage-resources/) to manage some Section specific items.


To launch the Kubernetes Dashboard simply click on the project and choose Launch Dashboard.

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

",195
Setup Ingress for a Project,Setup Ingress to the Section Edge for HTTP workload,"To setup ingress you will:

1. Create the necessary Kubernetes Service object to expose the web server to the Internet.
1. Use your DNS provider to direct traffic to your web server with a CNAME record.",42
Setup Ingress for a Project,Create a Service object,"Create a yaml file, such as `my-service.yaml`

```yaml title=""my-service.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
        -   name: 80-8080
            port: 80
            protocol: TCP
            targetPort: 80
    selector:
        app: nginx
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: { }
```

## Use kubectl to create your ingress service

### Deploy your service

```bash
kubectl apply -f my-service.yaml
```

The Service called `ingress-upstream` causes the deployment of special ingress pods into your environment that route
traffic into your app. Read more about the Section [HTTP Ingress](/guides/http-extensions/http-ingress/).",190
Setup Ingress for a Project,See your service running on Section,"```bash
  kubectl get services
  ```

You will see a new `ingress-upstream` service, as in the following example.

```
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes         ClusterIP   10.xx.xx.xx     <none>        443/TCP   6m55s
ingress-upstream   ClusterIP   10.xx.xx.xx     <none>        80/TCP    1s
```

### See the pods running on Section's network

  ```bash
  kubectl get pods -o wide
  ```

Note that you didn't set a [location optization strategy](/guides/projects/set-edge-locations/), so Section will run
your application with the default (2 locations).",172
Setup Ingress for a Project,Setup DNS,"* On the overview page of the [Section Console](https://console.section.io), find and click your environment.
* Under Settings select Domains.
* Confirm that the domain name you provided when you created the environment is listed.
* Visit your DNS provider and create a CNAME record to point to Section as instructed on the Section Console Domains
  page.
* After some time you will be able to click the Verify button to confirm that DNS is engaged.

Read more about [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",122
Setup Ingress for a Project,Congratulations!,"When you visit your hostname in a web browser, you'll see ""Welcome to nginx!"". This tells you that you application is
successfully deployed!

<!--- Your Project Settings page gives access to [Traffic Monitor](/explanations/traffic-monitor/), which is a great way to see the traffic coming into your web server.--->",67
Setup Ingress for a Project,Use the Kubernetes API to setup ingress for a Section Project,"
This doc will walk through how to setup ingress for your Project using the Kubernetes API.

## Setup Ingress to the Section Edge for HTTP workload

To setup ingress you will:

1. Create the necessary Kubernetes Service object to expose the web server to the Internet.
1. Use your DNS provider to direct traffic to your web server with a CNAME record.

## Create a Service object

Create a yaml file, such as `my-service.yaml`

```yaml title=""my-service.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
        -   name: 80-8080
            port: 80
            protocol: TCP
            targetPort: 80
    selector:
        app: nginx
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: { }
```

## Use kubectl to create your ingress service

### Deploy your service

```bash
kubectl apply -f my-service.yaml
```

The Service called `ingress-upstream` causes the deployment of special ingress pods into your environment that route
traffic into your app. Read more about the Section [HTTP Ingress](/guides/http-extensions/http-ingress/).

### See your service running on Section

  ```bash
  kubectl get services
  ```

You will see a new `ingress-upstream` service, as in the following example.

```
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes         ClusterIP   10.xx.xx.xx     <none>        443/TCP   6m55s
ingress-upstream   ClusterIP   10.xx.xx.xx     <none>        80/TCP    1s
```

### See the pods running on Section's network

  ```bash
  kubectl get pods -o wide
  ```

Note that you didn't set a [location optization strategy](/guides/projects/set-edge-locations/), so Section will run
your application with the default (2 locations).

## Setup DNS

* On the overview page of the [Section Console](https://console.section.io), find and click your environment.
* Under Settings select Domains.
* Confirm that the domain name you provided when you created the environment is listed.
* Visit your DNS provider and create a CNAME record to point to Section as instructed on the Section Console Domains
  page.
* After some time you will be able to click the Verify button to confirm that DNS is engaged.

Read more about [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

# Congratulations!

When you visit your hostname in a web browser, you'll see ""Welcome to nginx!"". This tells you that you application is
successfully deployed!

<!--- Your Project Settings page gives access to [Traffic Monitor](/explanations/traffic-monitor/), which is a great way to see the traffic coming into your web server.--->
",646
Kubernetes API Basics,Creating a Project,To get started using the Kuberenetes API you will first need to create an Section Project. For more information on the steps to create a Section Project and get your Kubernetes API follow our [Getting Started Guides](/get-started/create-project/).,50
Kubernetes API Basics,Kubernetes API URL,"Every project you create in Section will automatically generate a Kubernetes API endpoint. This is the URL you will use to communicate with Section when using kubectl. This endpoint is displayed in the Section console in Project Settings > Kubernetes.

![Kubernetes API URL](/img/docs/kube_api_url.gif)",59
Kubernetes API Basics,Use kubectl to Deploy a Container,Once you have created a Project and retrieved your Kubernetes API endpoint you can now deploy containers to the Section platform using the Kubernetes API. To do this please follow the [Get Started with Kubernetes](/docs/guides/kubernetes-ui/kubernetes-api/get-started-k8/) guide.,55
Kubernetes API Basics,Get Logs,"To print the logs for a container running on Section you can use the kubectl logs command:

```kubectl logs [pod name]```

For more information on kubectl commands please see the [Kubernetes Documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs)",60
Kubernetes API Basics,Describe,"To show details of a specific resource or group of resources running on Section you can use the **kubectl describe** command:

 ```kubectl describe [pod name]```


For more information on kubectl commands please see the [Kubernetes Documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe)",65
Kubernetes API Basics,Learn the Basics of Working with the Kubernetes API and Section,"
The Kubernetes API provides a consistent interface to deploy and manage workload on the Section [Composable Edge Cloud](/explanations/cec/). This guide will walk you through the basics.

## Creating a Project

To get started using the Kuberenetes API you will first need to create an Section Project. For more information on the steps to create a Section Project and get your Kubernetes API follow our [Getting Started Guides](/get-started/create-project/).

## Kubernetes API URL

Every project you create in Section will automatically generate a Kubernetes API endpoint. This is the URL you will use to communicate with Section when using kubectl. This endpoint is displayed in the Section console in Project Settings > Kubernetes.

![Kubernetes API URL](/img/docs/kube_api_url.gif)

## Use kubectl to Deploy a Container

Once you have created a Project and retrieved your Kubernetes API endpoint you can now deploy containers to the Section platform using the Kubernetes API. To do this please follow the [Get Started with Kubernetes](/docs/guides/kubernetes-ui/kubernetes-api/get-started-k8/) guide.

## Get Logs

To print the logs for a container running on Section you can use the kubectl logs command:

```kubectl logs [pod name]```

For more information on kubectl commands please see the [Kubernetes Documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs)

## Describe

To show details of a specific resource or group of resources running on Section you can use the **kubectl describe** command:

 ```kubectl describe [pod name]```


For more information on kubectl commands please see the [Kubernetes Documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe)
",354
Get Started with Kubernetes API,Use kubectl config to create a context,"Let's configure `kubectl` to communicate with your environment.

* If you haven't already, obtain your [API token](https://console.section.io/configure/user/tokens).
* Use the Section Console to navigate to the Projects page where you will see your `Kubernetes API` URL on your Prokect.",64
Get Started with Kubernetes API,Define Section as a cluster using your `Kubernetes API URL`:,"* **Ubuntu**
    ```bash
    KUBERNETES_API=""https://0123456789.kube.api.section.io/"" # Retrieve from https://console.section.io environment page
    kubectl config set-cluster section \
      --certificate-authority=/etc/ssl/certs/ca-certificates.crt \
      --server=$KUBERNETES_API_URL/
    ```

* **MacOS**
   ```bash
    KUBERNETES_API=""https://0123456789.kube.api.section.io/"" # Retrieve from https://console.section.io environment page
    kubectl config set-cluster section \
      --certificate-authority=/usr/local/etc/ca-certificates/cert.pem \
      --server=$KUBERNETES_API_URL/
   ```

If you don't have the file `/etc/ssl/certs/ca-certificates.crt` because you're on non-WSL-Windows, you can obtain an equivalent file here: [CA certificates](https://curl.se/docs/caextract.html)",207
Get Started with Kubernetes API,Save your API token,"```bash
  kubectl config set-credentials section-user \
     --token=$SECTION_API_TOKEN
  ```

### Create the execution context
  ```bash
  kubectl config set-context my-section-application \
     --cluster=section \
     --user=section-user \
     --namespace=default
  ```",64
Get Started with Kubernetes API,Switch to the new context,"```bash
  kubectl config use-context my-section-application
  ```

### Validate your setup
  ```bash
    kubectl version
  ```

More information about cluster, credential, and context information can be found in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#define-clusters-users-and-contexts)",79
Get Started with Kubernetes API,Deploy a web server to the Section Edge for HTTP workload,Next we'll place an nginx webserver at the edge using a Kubernetes Deployment object. The nginx webserver container used in this example comes from the official nginx images on the DockerHub registry.,38
Get Started with Kubernetes API,Create a Deployment object,"Create a yaml file, such as `my-first-edge-application.yaml`

```yaml title=""my-first-edge-application.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21.6
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        ports:
        - containerPort: 80
```

:::note
You can also use an image from a private registry as a part of your deployment. You can achieve this by creating a secret object containing the image pull credentials and specifying the same in your deployment object. You can read more about how to do this [here](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
:::

## Use kubectl to apply your Deployment

### Deploy your application

```bash
  kubectl apply -f my-first-edge-application.yaml
```",274
Get Started with Kubernetes API,See your deployment running on Section,"```bash
  kubectl get deployment nginx-deployment
```
### See the pods running on Section's network

```bash
  kubectl get pods -o wide
```

The ```-o wide``` switch shows where the pod is running according to the default AEE location optimization strategy. Ultimately you will have NxM pods running in the Section [Composable Edge Cloud](/explanations/cec/), where N is the number of replicas, and M is the number of edge locations where your workload is present.",108
Get Started with Kubernetes API,Configure kubectl to interact with the Kubernetes API to make ,"
Once you [create a Section Project](/get-started/create-project/) you can deploy your first container using the Kubernetes API.

## Use kubectl config to create a context
Let's configure `kubectl` to communicate with your environment.

* If you haven't already, obtain your [API token](https://console.section.io/configure/user/tokens).
* Use the Section Console to navigate to the Projects page where you will see your `Kubernetes API` URL on your Prokect.

### Define Section as a cluster using your `Kubernetes API URL`:

* **Ubuntu**
    ```bash
    KUBERNETES_API=""https://0123456789.kube.api.section.io/"" # Retrieve from https://console.section.io environment page
    kubectl config set-cluster section \
      --certificate-authority=/etc/ssl/certs/ca-certificates.crt \
      --server=$KUBERNETES_API_URL/
    ```

* **MacOS**
   ```bash
    KUBERNETES_API=""https://0123456789.kube.api.section.io/"" # Retrieve from https://console.section.io environment page
    kubectl config set-cluster section \
      --certificate-authority=/usr/local/etc/ca-certificates/cert.pem \
      --server=$KUBERNETES_API_URL/
   ```

If you don't have the file `/etc/ssl/certs/ca-certificates.crt` because you're on non-WSL-Windows, you can obtain an equivalent file here: [CA certificates](https://curl.se/docs/caextract.html)

### Save your API token
  ```bash
  kubectl config set-credentials section-user \
     --token=$SECTION_API_TOKEN
  ```

### Create the execution context
  ```bash
  kubectl config set-context my-section-application \
     --cluster=section \
     --user=section-user \
     --namespace=default
  ```

### Switch to the new context
  ```bash
  kubectl config use-context my-section-application
  ```

### Validate your setup
  ```bash
    kubectl version
  ```

More information about cluster, credential, and context information can be found in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#define-clusters-users-and-contexts)

## Deploy a web server to the Section Edge for HTTP workload
Next we'll place an nginx webserver at the edge using a Kubernetes Deployment object. The nginx webserver container used in this example comes from the official nginx images on the DockerHub registry.

## Create a Deployment object

Create a yaml file, such as `my-first-edge-application.yaml`

```yaml title=""my-first-edge-application.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.21.6
        imagePullPolicy: Always
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        ports:
        - containerPort: 80
```

:::note
You can also use an image from a private registry as a part of your deployment. You can achieve this by creating a secret object containing the image pull credentials and specifying the same in your deployment object. You can read more about how to do this [here](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).
:::

## Use kubectl to apply your Deployment

### Deploy your application

```bash
  kubectl apply -f my-first-edge-application.yaml
```
### See your deployment running on Section

```bash
  kubectl get deployment nginx-deployment
```
### See the pods running on Section's network

```bash
  kubectl get pods -o wide
```

The ```-o wide``` switch shows where the pod is running according to the default AEE location optimization strategy. Ultimately you will have NxM pods running in the Section [Composable Edge Cloud](/explanations/cec/), where N is the number of replicas, and M is the number of edge locations where your workload is present.",930
Create a Project with Section API,Get Your Account ID,"In addition to your token you will also need to your `SECTION_ACCOUNT_ID` to identify which account the Environment will be added to. Your Account ID can easily be seen in the browser's URL for the [Section Console](https://console.section.io), as in:

`https://console.section.io/overview/account/SECTION_ACCOUNT_ID`.

Alternatively, use the [Account API Call](https://aperture.section.io/api/ui/#!/Account/accountList) to get your Account's ID.

```bash title=""get_account_id.sh""
SECTION_API_TOKEN="""" # Create/retrieve from https://console.section.io/configure/user/tokens

curl \
  --header ""Accept: application/json"" \
  --header ""section-token: $SECTION_API_TOKEN"" \
  -X GET ""https://aperture.section.io/api/v1/account""
```

You will receive a JSON response with an `""id""`, this is your `SECTION_ACCOUNT_ID`.

### Create an Environment
Using the [Create Application](https://aperture.section.io/api/ui/#!/Application/applicationCreate) API call Section will automatically provision your Project.

You will send a JSON object with the following fields:
* The **hostname** This will be the domain name used to initialize the [managed ingress controller](/guides/http-extensions/http-ingress/) if you opt-in to using it. This field will also act as the name of your environment. *The **hostname** is limited to alphanumeric characters and the hyphen, and separated by dots.*
* The **origin** field is optional and not applicable to environments created for KEI
* The **stackName** field must be set to **kei**

The curl command below sends the needed JSON object, embedded right into the command line.
```bash title=""create_section_project.sh""
SECTION_API_TOKEN=""""  # See section above
SECTION_ACCOUNT_ID="""" # See section above
YOUR_ENVIRONMENT_HOSTNAME=""example.domain.io""

curl \
  --header ""section-token: $SECTION_API_TOKEN"" \
  --header ""Content-Type: application/json"" \
  --header ""Accept: application/json"" -d ""{ \
  \""hostname\"": \""$YOUR_ENVIRONMENT_HOSTNAME\"", \
  \""origin\"": \""blank\"", \
  \""stackName\"": \""kei\"" \
}"" ""https://aperture.section.io/api/v1/account/$SECTION_ACCOUNT_ID/application/create""
```

Any empty JSON response means that the command did not work.

You will now see a Project in the [Section Console](https://console.section.io/).",514
Create a Project with Section API,Create a Project using the Section Application Programming Interface (API) before starting with the Kubernetes API.,"
This doc will walk you through creating a Section Project with the Section application programming interface (API).

Be sure to [create an API Token](/guides/iam/api-tokens/) for the steps in this guide that require the `SECTION_API_TOKEN`.

### Get Your Account ID
In addition to your token you will also need to your `SECTION_ACCOUNT_ID` to identify which account the Environment will be added to. Your Account ID can easily be seen in the browser's URL for the [Section Console](https://console.section.io), as in:

`https://console.section.io/overview/account/SECTION_ACCOUNT_ID`.

Alternatively, use the [Account API Call](https://aperture.section.io/api/ui/#!/Account/accountList) to get your Account's ID.

```bash title=""get_account_id.sh""
SECTION_API_TOKEN="""" # Create/retrieve from https://console.section.io/configure/user/tokens

curl \
  --header ""Accept: application/json"" \
  --header ""section-token: $SECTION_API_TOKEN"" \
  -X GET ""https://aperture.section.io/api/v1/account""
```

You will receive a JSON response with an `""id""`, this is your `SECTION_ACCOUNT_ID`.

### Create an Environment
Using the [Create Application](https://aperture.section.io/api/ui/#!/Application/applicationCreate) API call Section will automatically provision your Project.

You will send a JSON object with the following fields:
* The **hostname** This will be the domain name used to initialize the [managed ingress controller](/guides/http-extensions/http-ingress/) if you opt-in to using it. This field will also act as the name of your environment. *The **hostname** is limited to alphanumeric characters and the hyphen, and separated by dots.*
* The **origin** field is optional and not applicable to environments created for KEI
* The **stackName** field must be set to **kei**

The curl command below sends the needed JSON object, embedded right into the command line.
```bash title=""create_section_project.sh""
SECTION_API_TOKEN=""""  # See section above
SECTION_ACCOUNT_ID="""" # See section above
YOUR_ENVIRONMENT_HOSTNAME=""example.domain.io""

curl \
  --header ""section-token: $SECTION_API_TOKEN"" \
  --header ""Content-Type: application/json"" \
  --header ""Accept: application/json"" -d ""{ \
  \""hostname\"": \""$YOUR_ENVIRONMENT_HOSTNAME\"", \
  \""origin\"": \""blank\"", \
  \""stackName\"": \""kei\"" \
}"" ""https://aperture.section.io/api/v1/account/$SECTION_ACCOUNT_ID/application/create""
```

Any empty JSON response means that the command did not work.

You will now see a Project in the [Section Console](https://console.section.io/).
",572
Other Web UIs,Other Web UIs Using Other Dashboards on Section,"Because our platform exposes the standard Kubernetes API you can integrate your favourite Kubernetes Web UI where it already leverages the Kubernetes API. 

Examples include:

 * [Lens](https://k8slens.dev/) - 
 	A powerful Kubernetes UI which has recently been opensourced.
 * [Octant](https://octant.dev/) - 
   Simple to install and a dev friendly interface.
 * [KubeNav](https://kubenav.io/) - 
   A relatively new project that is mobile device friendly.

See our [guide to integration with Lens](/docs/guides/kubernetes-ui/other-ui/lens-integration/).",129
Other Web UIs,Using Other Dashboards on Section,"
Because our platform exposes the standard Kubernetes API you can integrate your favourite Kubernetes Web UI where it already leverages the Kubernetes API. 

Examples include:

 * [Lens](https://k8slens.dev/) - 
 	A powerful Kubernetes UI which has recently been opensourced.
 * [Octant](https://octant.dev/) - 
   Simple to install and a dev friendly interface.
 * [KubeNav](https://kubenav.io/) - 
   A relatively new project that is mobile device friendly.

See our [guide to integration with Lens](/docs/guides/kubernetes-ui/other-ui/lens-integration/).",130
Lens Integration,Introduction,"[Lens](https://k8slens.dev/) is a a really powerful UI for Kubernetes with support for Windows, macOS and Linux. It was recently released as an open source project.

![Lens](/img/docs/lens.png)

After you have downloaded Lens, you will want to add your Section project to your Lens Catalog.

![Lens](/img/docs/lensc1.png)

Choose settings for your project

![Lens](/img/docs/settings.png)

Then copy the Kubernetes Config for your project and save it to a local drive. 

![Lens](/img/docs/kubeconfig.png)

From Lens Dashboard choose your Catalog general preferences

![Lens](/img/docs/prefs.png)

Then sync the file you just downloaded (oth the whole folder if you have several project config files in that folder)

![Lens](/img/docs/sync.png)


And Voila!  Your Section projects are now availble in Lens to manage entirely from the Lens Dashboard.  

Click on the cluster the connect.  You may see a brief warning message from Lens which should be ignored while connecting.  Just hang 5 and Lens will connect.

![Lens](/img/docs/lenslive.png)",242
Lens Integration,Integrating Lens with Section,"

## Introduction

[Lens](https://k8slens.dev/) is a a really powerful UI for Kubernetes with support for Windows, macOS and Linux. It was recently released as an open source project.

![Lens](/img/docs/lens.png)

After you have downloaded Lens, you will want to add your Section project to your Lens Catalog.

![Lens](/img/docs/lensc1.png)

Choose settings for your project

![Lens](/img/docs/settings.png)

Then copy the Kubernetes Config for your project and save it to a local drive. 

![Lens](/img/docs/kubeconfig.png)

From Lens Dashboard choose your Catalog general preferences

![Lens](/img/docs/prefs.png)

Then sync the file you just downloaded (oth the whole folder if you have several project config files in that folder)

![Lens](/img/docs/sync.png)


And Voila!  Your Section projects are now availble in Lens to manage entirely from the Lens Dashboard.  

Click on the cluster the connect.  You may see a brief warning message from Lens which should be ignored while connecting.  Just hang 5 and Lens will connect.

![Lens](/img/docs/lenslive.png)",246
Multidomain and Path Routing with Nginx,Multidomain and Path Routing with Nginx,"In this tutorial we will use Section and Nginx to create microservices with multidomain and multipath routing. We will outline how we can have multiple services and deployments all talking to each other within a Section application. 

You  will create two different deployments - `hello-node-deployment.yaml` and `hello-ruby-deployment.yaml`, as well as a service for each. Each deployment will return “hello world” in either node or ruby based on the routing.

There are many scenarios that can benefit from this architecture. For example Section has three different frontend microservices all serving different applications (repositories) to the same website. These include www.section.io, www.section.io/docs and www.section.io/engineering-education which are all static websites. Our single Section application also includes an application programming interface (API) that we use to fetch data from the Google Analytics API in order to display authors number of page views on the EngEd program. Each of these repositories have their own CI/CD pipelines allowing us to have a combination of public and private repos with added flexibility. 

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",278
Multidomain and Path Routing with Nginx,Add the Domains,Add the domains you’d like to handle the routing for in your Section application. In this example we will add the domains www.example-domain-1.com and www.example-domain-2.com.,38
Multidomain and Path Routing with Nginx,Setup Project Structure,"Follow the Getting Started steps in Docs if you’re just starting out with Section. If you already have a Section application, point your `ingress-service` container to a Nginx container. It may help to use Kustomization if you aren’t already. With the use of kustomization, we can utilize the `configMapGenerator` to volume mount our nginx configuration.

Create the following `/k8s` directory and structure:
- /k8s
/base
hello-node-deployment.yaml
hello-ruby-deployment.yaml
hello-node-service.yaml
hello-ruby-service.yaml
ingress-service.yaml
kustomization.yaml
router.yaml
router.conf
For this example we will create `deployment-app1.yaml` and deployment-app2.yaml` as two nginx pods running different versions. 

```yaml title=""hello-node-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-node-deployment
  labels:
    app: hello-node
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-node
  template:
    metadata:
      labels:
        app: hello-node
    spec:
      containers:
        - name: hello-node
          image: pvermeyden/nodejs-hello-world:a1e8cf1edcc04e6d905078aed9861807f6da0da4
          imagePullPolicy: Always
          ports:
          - containerPort: 80
          resources:
            requests:
              memory: ""1Gi""
              cpu: ""500m""
            limits:
              memory: ""1Gi""
              cpu: ""500m""
 
```

```yaml title=""hello-ruby-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-ruby-deployment
  labels:
    app: hello-ruby
spec:
 replicas: 1
  selector:
    matchLabels:
      app: hello-ruby
  template:
    metadata:
      labels:
        app: hello-ruby
    spec:
      containers:
        - name: hello-ruby
          image: sebp/ruby-hello-world
          imagePullPolicy: Always
          ports:
          - containerPort: 80
          resources:
            requests:
              memory: ""1Gi""
              cpu: ""500m""
            limits:
              memory: ""1Gi""
              cpu: ""500m""
```

```yaml title=""hello-world-service.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello-world-service
  name: hello-world-service
  namespace: default
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hello-world
  sessionAffinity: None
  type: ClusterIP
  
```

```yaml title=""hello-ruby-service.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello-ruby-service
  name: hello-ruby-service
  namespace: default
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hello-ruby
  sessionAffinity: None
  type: ClusterIP

```",712
Multidomain and Path Routing with Nginx,Use `configMapGenerator` to apply the Nginx Configuration,"Create a volume mount and configmap to load a custom nginx configuration for `router.yaml` and define the resources you’d like kustomization to manage.

```yaml title=""kustomization.yaml""
configMapGenerator:
 - name: router-config-mount
   files:
     - ./router.conf
 
resources:
 - hello-world-deployment.yaml
 - hello-world-service.yaml
 - hello-ruby-deployment.yaml
 - hello-ruby-service.yaml
 - router.yml
 - ingress-service.yml

```

The `configMapGenerator` defines the name and location of the file to mount. In this case the file path is `./router.conf` with the name `router-config-mount`.

Apply the `configMap` via `volumes` and `volumeMounts`
In our `router.yaml` below we will add the configMap we just created via `volumes` and `volumeMounts`.

```yaml title=""router.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.21.6
          imagePullPolicy: Always
          volumeMounts:
            - name: router-config
              mountPath: ""/etc/nginx/conf.d""
          resources:
            requests:
              memory: "".5Gi""
              cpu: ""500m""
            limits:
              memory: "".5Gi""
              cpu: ""500m""
          ports:
            - containerPort: 80
      volumes:
        - name: router-config
          configMap:
            name: router-config-mount
```",373
Multidomain and Path Routing with Nginx,Nginx configuration,"Create a `router.conf` file in the `k8s/base` directory. This configuration controls the routing for the two different domains. Both domains return the same applications for each location block. However these can point to different services. The important thing to note here is that we can use `proxy_pass` to point our routing to different services that we’ve created. 

```conf title=""router.conf""
server {
 listen       80;
 listen  [::]:80;
 server_name  www.example-domain-1.com;
 
 location /node {
   proxy_pass http://hello-world-service;
 }
 
 location /ruby {
   proxy_pass http://hello-ruby-service;
 }
}
 
server {
 listen       80;
 listen  [::]:80;
 server_name  www.example-domain-2.com;
 
 location /node {
   proxy_pass http://hello-world-service;
 }
 
 location /ruby {
   proxy_pass http:/hello-ruby-service;
 }
}

```

By adding multiple server blocks, we can create routing for different domains. Each server block can contain multiple location blocks. In each location block we can make use of Nginx Reverse Proxy by using the  `proxy_pass` key we can specify the name of a service we want the location to route to.
`
location / {
   proxy_pass http://hello-world-service;
 }
`

In the above example we are handling the routing for both domains  `www.example-domain-1.com` and `www.example-domain-2.com`. When a request comes from `www.example-domain-1.com/node` we route the request using `proxy_pass` to our service `hello-node-service.yaml`, this returns “hello world” from NodeJS. 

Similarly, a request from `www.example-domain-1.com/ruby` points to our service `hello-ruby-service.yaml`, returning us “hello world” from Ruby. As you can see the code above has the same configuration for both domains. However, in a real world scenario these can all have different configurations with different types of applications.

This architecture allows us to have multiple applications and microservices all in one Section application. With relatively simple configuration we can communicate back and forth between different pods and services.",449
Multidomain and Path Routing with Nginx,Learn to configure multidomain and path routing with Nginx,"
# Multidomain and Path Routing with Nginx 

In this tutorial we will use Section and Nginx to create microservices with multidomain and multipath routing. We will outline how we can have multiple services and deployments all talking to each other within a Section application. 

You  will create two different deployments - `hello-node-deployment.yaml` and `hello-ruby-deployment.yaml`, as well as a service for each. Each deployment will return “hello world” in either node or ruby based on the routing.

There are many scenarios that can benefit from this architecture. For example Section has three different frontend microservices all serving different applications (repositories) to the same website. These include www.section.io, www.section.io/docs and www.section.io/engineering-education which are all static websites. Our single Section application also includes an application programming interface (API) that we use to fetch data from the Google Analytics API in order to display authors number of page views on the EngEd program. Each of these repositories have their own CI/CD pipelines allowing us to have a combination of public and private repos with added flexibility. 

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## Add the Domains
Add the domains you’d like to handle the routing for in your Section application. In this example we will add the domains www.example-domain-1.com and www.example-domain-2.com. 

## Setup Project Structure
Follow the Getting Started steps in Docs if you’re just starting out with Section. If you already have a Section application, point your `ingress-service` container to a Nginx container. It may help to use Kustomization if you aren’t already. With the use of kustomization, we can utilize the `configMapGenerator` to volume mount our nginx configuration.

Create the following `/k8s` directory and structure:
- /k8s
/base
hello-node-deployment.yaml
hello-ruby-deployment.yaml
hello-node-service.yaml
hello-ruby-service.yaml
ingress-service.yaml
kustomization.yaml
router.yaml
router.conf
For this example we will create `deployment-app1.yaml` and deployment-app2.yaml` as two nginx pods running different versions. 

```yaml title=""hello-node-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-node-deployment
  labels:
    app: hello-node
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-node
  template:
    metadata:
      labels:
        app: hello-node
    spec:
      containers:
        - name: hello-node
          image: pvermeyden/nodejs-hello-world:a1e8cf1edcc04e6d905078aed9861807f6da0da4
          imagePullPolicy: Always
          ports:
          - containerPort: 80
          resources:
            requests:
              memory: ""1Gi""
              cpu: ""500m""
            limits:
              memory: ""1Gi""
              cpu: ""500m""
 
```

```yaml title=""hello-ruby-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-ruby-deployment
  labels:
    app: hello-ruby
spec:
 replicas: 1
  selector:
    matchLabels:
      app: hello-ruby
  template:
    metadata:
      labels:
        app: hello-ruby
    spec:
      containers:
        - name: hello-ruby
          image: sebp/ruby-hello-world
          imagePullPolicy: Always
          ports:
          - containerPort: 80
          resources:
            requests:
              memory: ""1Gi""
              cpu: ""500m""
            limits:
              memory: ""1Gi""
              cpu: ""500m""
```

```yaml title=""hello-world-service.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello-world-service
  name: hello-world-service
  namespace: default
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hello-world
  sessionAffinity: None
  type: ClusterIP
  
```

```yaml title=""hello-ruby-service.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: hello-ruby-service
  name: hello-ruby-service
  namespace: default
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: hello-ruby
  sessionAffinity: None
  type: ClusterIP

```

## Use `configMapGenerator` to apply the Nginx Configuration
Create a volume mount and configmap to load a custom nginx configuration for `router.yaml` and define the resources you’d like kustomization to manage.

```yaml title=""kustomization.yaml""
configMapGenerator:
 - name: router-config-mount
   files:
     - ./router.conf
 
resources:
 - hello-world-deployment.yaml
 - hello-world-service.yaml
 - hello-ruby-deployment.yaml
 - hello-ruby-service.yaml
 - router.yml
 - ingress-service.yml

```

The `configMapGenerator` defines the name and location of the file to mount. In this case the file path is `./router.conf` with the name `router-config-mount`.

Apply the `configMap` via `volumes` and `volumeMounts`
In our `router.yaml` below we will add the configMap we just created via `volumes` and `volumeMounts`.

```yaml title=""router.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.21.6
          imagePullPolicy: Always
          volumeMounts:
            - name: router-config
              mountPath: ""/etc/nginx/conf.d""
          resources:
            requests:
              memory: "".5Gi""
              cpu: ""500m""
            limits:
              memory: "".5Gi""
              cpu: ""500m""
          ports:
            - containerPort: 80
      volumes:
        - name: router-config
          configMap:
            name: router-config-mount
```

## Nginx configuration
Create a `router.conf` file in the `k8s/base` directory. This configuration controls the routing for the two different domains. Both domains return the same applications for each location block. However these can point to different services. The important thing to note here is that we can use `proxy_pass` to point our routing to different services that we’ve created. 

```conf title=""router.conf""
server {
 listen       80;
 listen  [::]:80;
 server_name  www.example-domain-1.com;
 
 location /node {
   proxy_pass http://hello-world-service;
 }
 
 location /ruby {
   proxy_pass http://hello-ruby-service;
 }
}
 
server {
 listen       80;
 listen  [::]:80;
 server_name  www.example-domain-2.com;
 
 location /node {
   proxy_pass http://hello-world-service;
 }
 
 location /ruby {
   proxy_pass http:/hello-ruby-service;
 }
}

```

By adding multiple server blocks, we can create routing for different domains. Each server block can contain multiple location blocks. In each location block we can make use of Nginx Reverse Proxy by using the  `proxy_pass` key we can specify the name of a service we want the location to route to.
`
location / {
   proxy_pass http://hello-world-service;
 }
`

In the above example we are handling the routing for both domains  `www.example-domain-1.com` and `www.example-domain-2.com`. When a request comes from `www.example-domain-1.com/node` we route the request using `proxy_pass` to our service `hello-node-service.yaml`, this returns “hello world” from NodeJS. 

Similarly, a request from `www.example-domain-1.com/ruby` points to our service `hello-ruby-service.yaml`, returning us “hello world” from Ruby. As you can see the code above has the same configuration for both domains. However, in a real world scenario these can all have different configurations with different types of applications.

This architecture allows us to have multiple applications and microservices all in one Section application. With relatively simple configuration we can communicate back and forth between different pods and services.
",1898
Image Manipulation,Image Manipulation with imgproxy,"[imgproxy](https://imgproxy.net/) is a fast and secure standalone server for resizing and converting remote images. With this tutorial you'll deploy the open-source imgproxy container [from DockerHub](https://hub.docker.com/r/darthsim/imgproxy) to enable low-latency image manipulation close to your end users.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",127
Image Manipulation,Create a Deployment for imgproxy,"Create the deployment for imgproxy as `imgproxy-deployment.yaml`.  This will direct Section to run the imgproxy open source container.

```yaml title=""imgproxy-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: imgproxy
  name: imgproxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: imgproxy
  template:
    metadata:
      labels:
        app: imgproxy
    spec:
      containers:
      - image: darthsim/imgproxy:latest
        imagePullPolicy: Always
        name: imgproxy
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f imgproxy-deployment.yaml`.

## Expose imgproxy on the Internet

We want to expose the imgproxy on the Internet so that it can serve image manipulation requests. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: imgproxy
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

![imgproxy pods](/img/docs/imgproxy-pods.png)",473
Image Manipulation,Experiment with imgproxy,"Now, you can start using imgproxy. To resize the following image:
```
https://m.media-amazon.com/images/M/MV5BMmQ3ZmY4NzYtY2VmYi00ZDRmLTgyODAtZWYzZjhlNzk1NzU2XkEyXkFqcGdeQXVyNTc3MjUzNTI@.jpg
```

You use the following URL:

```
http://YOUR_ENVIRONMENT_HOSTNAME.section.app/insecure/rs:fill:300:400/g:sm/aHR0cHM6Ly9tLm1l/ZGlhLWFtYXpvbi5j/b20vaW1hZ2VzL00v/TVY1Qk1tUTNabVk0/TnpZdFkyVm1ZaTAw/WkRSbUxUZ3lPREF0/WldZelpqaGxOemsx/TnpVMlhrRXlYa0Zx/Y0dkZVFYVnlOVGMz/TWpVek5USUAuanBn.jpg
```

Using the URL above, imgproxy is instructed to resize it to fill an area of 300x400 size with “smart” gravity. “Smart” means that imgproxy chooses the most “interesting” part of the image.",292
Image Manipulation,Original,![Original](https://m.media-amazon.com/images/M/MV5BMmQ3ZmY4NzYtY2VmYi00ZDRmLTgyODAtZWYzZjhlNzk1NzU2XkEyXkFqcGdeQXVyNTc3MjUzNTI@.jpg),78
Image Manipulation,imgproxy result,![imgproxy result](/img/docs/imgproxy-result.jpg),13
Image Manipulation,Learn to deploy an imgproxy service for resizing and converting remote images,"
# Image Manipulation with imgproxy

[imgproxy](https://imgproxy.net/) is a fast and secure standalone server for resizing and converting remote images. With this tutorial you'll deploy the open-source imgproxy container [from DockerHub](https://hub.docker.com/r/darthsim/imgproxy) to enable low-latency image manipulation close to your end users.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## Create a Deployment for imgproxy
Create the deployment for imgproxy as `imgproxy-deployment.yaml`.  This will direct Section to run the imgproxy open source container.

```yaml title=""imgproxy-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: imgproxy
  name: imgproxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: imgproxy
  template:
    metadata:
      labels:
        app: imgproxy
    spec:
      containers:
      - image: darthsim/imgproxy:latest
        imagePullPolicy: Always
        name: imgproxy
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f imgproxy-deployment.yaml`.

## Expose imgproxy on the Internet

We want to expose the imgproxy on the Internet so that it can serve image manipulation requests. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: imgproxy
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

![imgproxy pods](/img/docs/imgproxy-pods.png)

## Experiment with imgproxy
Now, you can start using imgproxy. To resize the following image:
```
https://m.media-amazon.com/images/M/MV5BMmQ3ZmY4NzYtY2VmYi00ZDRmLTgyODAtZWYzZjhlNzk1NzU2XkEyXkFqcGdeQXVyNTc3MjUzNTI@.jpg
```

You use the following URL:

```
http://YOUR_ENVIRONMENT_HOSTNAME.section.app/insecure/rs:fill:300:400/g:sm/aHR0cHM6Ly9tLm1l/ZGlhLWFtYXpvbi5j/b20vaW1hZ2VzL00v/TVY1Qk1tUTNabVk0/TnpZdFkyVm1ZaTAw/WkRSbUxUZ3lPREF0/WldZelpqaGxOemsx/TnpVMlhrRXlYa0Zx/Y0dkZVFYVnlOVGMz/TWpVek5USUAuanBn.jpg
```

Using the URL above, imgproxy is instructed to resize it to fill an area of 300x400 size with “smart” gravity. “Smart” means that imgproxy chooses the most “interesting” part of the image.

## Original
![Original](https://m.media-amazon.com/images/M/MV5BMmQ3ZmY4NzYtY2VmYi00ZDRmLTgyODAtZWYzZjhlNzk1NzU2XkEyXkFqcGdeQXVyNTc3MjUzNTI@.jpg)

## imgproxy result
![imgproxy result](/img/docs/imgproxy-result.jpg)",1014
Varnish Cache,Web App Caching with Varnish,"You've decided to distribute your web app closer to your users for increased performance. Why not make it even faster by leveraging caching? In this tutorial we will use [Varnish](https://varnish-cache.org/) to enable caching for your container running a Webapp. You can learn all about Varnish from their own [documentation](https://varnish-cache.org/docs) or from our comprehensive [guide](https://www.section.io/blog/varnish-cache-tutorial-vcl/).

The pattern we'll demonstrate is beneficial for use cases such as the following:
- The web app uses a [Server Side Rendering](https://www.section.io/blog/server-side-rendering-edge-nodejs/) framework.
- The web app is a multi-page static site.
- The web app is a [Single Page App](https://www.section.io/blog/five-use-cases-nodejs-edge/). 

In all cases, you'll put less load on the web app container by putting Varnish in front.

![Varnish Cache](/img/docs/varn1.png)

The above diagram illustrates the case where Varnish and the WebApp are deployed together on Section.

The pattern also works when the app is centrally located in a single location, in which case you've instantly built a modern [CDN](https://en.wikipedia.org/wiki/Content_delivery_network) for a web app. If you are using a [static site generator](https://jamstack.org/generators/) like Next.js, Hugo, Gatsby, Jekyll, etc., then you absolutely need to put a CDN in front of it. And by using Section for your CDN, you'll get Section's [enhanced security](/about/security/#ddos-protection), plus you can customize your CDN with components of your choice, such as WAFs, image optimization, bot mitigation, and more.

![Web app CDN](/img/docs/varn3.png)

In addition to [Varnish](https://varnish-cache.org/), this tutorial introduces the use of [Kustomize](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/), a technology for keeping your ""kustomizations"" separate from generic YAML templates, and [NATS](https://nats.io), a cloud and edge native messaging system.",472
Varnish Cache,Traffic flow diagram,"*Client requests example-domain.section.app (when it has not been cached)*
```
example-domain.section.app
└─> Section's managed public ingress
    └─> Varnish
        └─> App
```

*Once the page has been cached, the response will be returned by Varnish, which reduces the load on the app itself*
```
example-domain.section.app
└─> Section's managed public ingress
    └─> Varnish
        └x App
```

We will be deploying the resources via `kubectl`, and leveraging [Kustomize's](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/) `configMapGenerator` to convert configuration files (Varnish's `default.vcl`) into a Kubernetes ConfigMap resource. We're using a demo image from Section for the 'app', but you can swap out the container image for one that you prefer.

This tutorial assumes that the reader has a basic understanding of the Varnish configuration language, and so will simply gloss over the `default.vcl` file. You may refer to the official [Varnish documentation](https://varnish-cache.org/docs/) for more details.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",316
Varnish Cache,Project structure,"We show you how to pull the pieces together here. The entire project is available by cloning from Section's Github repo: [https://github.com/section/varnish-tutorial](https://github.com/section/varnish-tutorial). It uses the Section Varnish container on [Docker Hub](https://hub.docker.com/r/sectionio/varnish-cache-kei).

The folder structure will be as follows:

```
.
├── app
│   ├── kustomization.yaml
│   ├── webapp-deployment.yaml
│   └── webapp-service.yaml
├── varnish
│   ├── default.vcl
│   ├── kustomization.yaml
│   ├── varnish-deployment.yaml
│   └── varnish-service.yaml
└── kustomization.yaml

2 directories, 8 files
```

This tutorial will result in the creation of the following Kubernetes resources in your Section Project:
- 2 Deployments
- 2 Services
- 1 ConfigMap",210
Varnish Cache,Root 'kustomization.yaml',"This lists the resources/folders to be deployed when the `kubectl apply -k .` is run. The following code will deploy the resources declared within the `varnish` and `app` folders.

```yaml title=""kustomization.yaml""
resources:
- varnish/
- app/
```

### Folder 'app' 
This is where you would declare your app that sits after Varnish. Depending on your Varnish configuration, the returned responses will then be cached by Varnish.

```yaml title=""app/kustomization.yaml""
resources:
- webapp-deployment.yaml
- webapp-service.yaml
```",128
Varnish Cache,Folder 'varnish',"This is where you would configure the Varnish deployment, such as resource allocation and number of replicas. Kustomize's `configMapGenerator` takes care of converting the `default.vcl` file into a configmap. If the configmap is updated, the Varnish pods will be restarted to take on the new settings.

```yaml title=""varnish/kustomization.yaml""
resources:
- varnish-deployment.yaml
- varnish-service.yaml

configMapGenerator:
- name: vcl
  files:
  - default.vcl
```

```vcl title=""default.vcl""
vcl 4.1;
import dynamic;
backend default none;

sub vcl_init {
    new d = dynamic.director(port = ""80"", ttl = 60s);
}

## Redirect http to https
sub vcl_recv {
    if (req.http.X-Forwarded-Proto !~ ""(?i)https"") { 
        return (synth(10301, ""Moved Permanently"")); 
    }
}
sub vcl_synth {
    if (resp.status == 10301) {
        set resp.status = 301;
        set resp.http.Location = ""https://"" + req.http.host + req.url;
        return(deliver);
    }
}
## End redirect http to https

sub vcl_recv {
    set req.backend_hint = d.backend(""webapp-service""); # Replace this value with the name of the service linked to your app
    return (hash);
}

sub vcl_backend_response {
    set beresp.ttl = 600s;
    unset beresp.http.set-cookie;
    return (deliver);
}

sub vcl_deliver {
    if (obj.hits > 0) {
        set resp.http.Varnish-Cache = ""HIT"";
    } else {
        set resp.http.Varnish-Cache = ""MISS"";
    }
}

sub vcl_hash {
    hash_data(req.http.X-Forwarded-Proto);
}
```",409
Varnish Cache,Deploying the resources,"Assuming you've configured the folder in accordance to the instructions above, you can deploy all the resources by running the following command from the project root folder:

```bash
kubectl apply -k .
```

You can review the resources after creation by running the following command:

```bash
kubectl get pods,deployment,service,configmap -o wide
```",72
Varnish Cache,Issuing Varnish Cache Bans,"[Banning](https://www.varnish-software.com/developers/tutorials/ban/) is a concept in Varnish that allows expression-based cache invalidation. We give two approaches, one that works for infrequent bans, and the other that works better when bans are the result of automation.",60
Varnish Cache,Low frequency bans,"Bans can be issued to all Varnish pods deployed in your project worldwide by using kubectl exec as shown below. Note that this method is not suitable for more than a few bans per minute.

```bash
BAN_EXPRESSION=""req.url ~ /path/to/ban""

kubectl get pod --selector app=varnish-cache -o name \
  | xargs -t -I{} kubectl exec {} -- varnishadm ban $BAN_EXPRESSION
```

## High frequency bans
For high frequency bans, our Varnish container has been programmed to be able to receive messages from the CNCF [NATS](https://nats.io/) cloud and edge native messaging system. You can provision your own instance of NATS. Or you can leverage a managed offering such as [Synadia](https://synadia.com/), a distributed instance of NATS that will work well with your distributed Section project.

![Varnish Cache with NATS](/img/docs/varn2.png)

The `NATS_URL` and `NATS_SUBJECT` environment variables within the `varnish-cache` deployment ([varnish-deployment.yaml](https://github.com/section/varnish-tutorial/blob/main/simple-example/varnish/varnish-deployment.yaml)) specify a NATS server, and a NATS subject respectively where a Golang-implemented process in the Pod will subscribe to receive Varnish Cache ban messages via NATS. The NATS cli is illustrated, but you can use any mechanism you like to get messages onto the message bus.

The messages in NATS are expected to be JSON-encoded and use the structure from the following example, where banExpression is any valid expression that would be accepted by the `varnishadm` ban command.

```json
{
  ""operation"": ""applyban"",
  ""parameters"": {
    ""banExpression"": ""req.url ~ /foo""
  }
}
```",388
Varnish Cache,Learn to accelerate your web app by deploying it with Varnish cache across your multi-region clusters,"
# Web App Caching with Varnish 

You've decided to distribute your web app closer to your users for increased performance. Why not make it even faster by leveraging caching? In this tutorial we will use [Varnish](https://varnish-cache.org/) to enable caching for your container running a Webapp. You can learn all about Varnish from their own [documentation](https://varnish-cache.org/docs) or from our comprehensive [guide](https://www.section.io/blog/varnish-cache-tutorial-vcl/).

The pattern we'll demonstrate is beneficial for use cases such as the following:
- The web app uses a [Server Side Rendering](https://www.section.io/blog/server-side-rendering-edge-nodejs/) framework.
- The web app is a multi-page static site.
- The web app is a [Single Page App](https://www.section.io/blog/five-use-cases-nodejs-edge/). 

In all cases, you'll put less load on the web app container by putting Varnish in front.

![Varnish Cache](/img/docs/varn1.png)

The above diagram illustrates the case where Varnish and the WebApp are deployed together on Section.

The pattern also works when the app is centrally located in a single location, in which case you've instantly built a modern [CDN](https://en.wikipedia.org/wiki/Content_delivery_network) for a web app. If you are using a [static site generator](https://jamstack.org/generators/) like Next.js, Hugo, Gatsby, Jekyll, etc., then you absolutely need to put a CDN in front of it. And by using Section for your CDN, you'll get Section's [enhanced security](/about/security/#ddos-protection), plus you can customize your CDN with components of your choice, such as WAFs, image optimization, bot mitigation, and more.

![Web app CDN](/img/docs/varn3.png)

In addition to [Varnish](https://varnish-cache.org/), this tutorial introduces the use of [Kustomize](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/), a technology for keeping your ""kustomizations"" separate from generic YAML templates, and [NATS](https://nats.io), a cloud and edge native messaging system.

### Traffic flow diagram
*Client requests example-domain.section.app (when it has not been cached)*
```
example-domain.section.app
└─> Section's managed public ingress
    └─> Varnish
        └─> App
```

*Once the page has been cached, the response will be returned by Varnish, which reduces the load on the app itself*
```
example-domain.section.app
└─> Section's managed public ingress
    └─> Varnish
        └x App
```

We will be deploying the resources via `kubectl`, and leveraging [Kustomize's](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/) `configMapGenerator` to convert configuration files (Varnish's `default.vcl`) into a Kubernetes ConfigMap resource. We're using a demo image from Section for the 'app', but you can swap out the container image for one that you prefer.

This tutorial assumes that the reader has a basic understanding of the Varnish configuration language, and so will simply gloss over the `default.vcl` file. You may refer to the official [Varnish documentation](https://varnish-cache.org/docs/) for more details.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## Project structure

We show you how to pull the pieces together here. The entire project is available by cloning from Section's Github repo: [https://github.com/section/varnish-tutorial](https://github.com/section/varnish-tutorial). It uses the Section Varnish container on [Docker Hub](https://hub.docker.com/r/sectionio/varnish-cache-kei).

The folder structure will be as follows:

```
.
├── app
│   ├── kustomization.yaml
│   ├── webapp-deployment.yaml
│   └── webapp-service.yaml
├── varnish
│   ├── default.vcl
│   ├── kustomization.yaml
│   ├── varnish-deployment.yaml
│   └── varnish-service.yaml
└── kustomization.yaml

2 directories, 8 files
```

This tutorial will result in the creation of the following Kubernetes resources in your Section Project:
- 2 Deployments
- 2 Services
- 1 ConfigMap

### Root 'kustomization.yaml'
This lists the resources/folders to be deployed when the `kubectl apply -k .` is run. The following code will deploy the resources declared within the `varnish` and `app` folders.

```yaml title=""kustomization.yaml""
resources:
- varnish/
- app/
```

### Folder 'app' 
This is where you would declare your app that sits after Varnish. Depending on your Varnish configuration, the returned responses will then be cached by Varnish.

```yaml title=""app/kustomization.yaml""
resources:
- webapp-deployment.yaml
- webapp-service.yaml
```

### Folder 'varnish' 
This is where you would configure the Varnish deployment, such as resource allocation and number of replicas. Kustomize's `configMapGenerator` takes care of converting the `default.vcl` file into a configmap. If the configmap is updated, the Varnish pods will be restarted to take on the new settings.

```yaml title=""varnish/kustomization.yaml""
resources:
- varnish-deployment.yaml
- varnish-service.yaml

configMapGenerator:
- name: vcl
  files:
  - default.vcl
```

```vcl title=""default.vcl""
vcl 4.1;
import dynamic;
backend default none;

sub vcl_init {
    new d = dynamic.director(port = ""80"", ttl = 60s);
}

## Redirect http to https
sub vcl_recv {
    if (req.http.X-Forwarded-Proto !~ ""(?i)https"") { 
        return (synth(10301, ""Moved Permanently"")); 
    }
}
sub vcl_synth {
    if (resp.status == 10301) {
        set resp.status = 301;
        set resp.http.Location = ""https://"" + req.http.host + req.url;
        return(deliver);
    }
}
## End redirect http to https

sub vcl_recv {
    set req.backend_hint = d.backend(""webapp-service""); # Replace this value with the name of the service linked to your app
    return (hash);
}

sub vcl_backend_response {
    set beresp.ttl = 600s;
    unset beresp.http.set-cookie;
    return (deliver);
}

sub vcl_deliver {
    if (obj.hits > 0) {
        set resp.http.Varnish-Cache = ""HIT"";
    } else {
        set resp.http.Varnish-Cache = ""MISS"";
    }
}

sub vcl_hash {
    hash_data(req.http.X-Forwarded-Proto);
}
```

## Deploying the resources
Assuming you've configured the folder in accordance to the instructions above, you can deploy all the resources by running the following command from the project root folder:

```bash
kubectl apply -k .
```

You can review the resources after creation by running the following command:

```bash
kubectl get pods,deployment,service,configmap -o wide
```

# Issuing Varnish Cache Bans
[Banning](https://www.varnish-software.com/developers/tutorials/ban/) is a concept in Varnish that allows expression-based cache invalidation. We give two approaches, one that works for infrequent bans, and the other that works better when bans are the result of automation.

## Low frequency bans
Bans can be issued to all Varnish pods deployed in your project worldwide by using kubectl exec as shown below. Note that this method is not suitable for more than a few bans per minute.

```bash
BAN_EXPRESSION=""req.url ~ /path/to/ban""

kubectl get pod --selector app=varnish-cache -o name \
  | xargs -t -I{} kubectl exec {} -- varnishadm ban $BAN_EXPRESSION
```

## High frequency bans
For high frequency bans, our Varnish container has been programmed to be able to receive messages from the CNCF [NATS](https://nats.io/) cloud and edge native messaging system. You can provision your own instance of NATS. Or you can leverage a managed offering such as [Synadia](https://synadia.com/), a distributed instance of NATS that will work well with your distributed Section project.

![Varnish Cache with NATS](/img/docs/varn2.png)

The `NATS_URL` and `NATS_SUBJECT` environment variables within the `varnish-cache` deployment ([varnish-deployment.yaml](https://github.com/section/varnish-tutorial/blob/main/simple-example/varnish/varnish-deployment.yaml)) specify a NATS server, and a NATS subject respectively where a Golang-implemented process in the Pod will subscribe to receive Varnish Cache ban messages via NATS. The NATS cli is illustrated, but you can use any mechanism you like to get messages onto the message bus.

The messages in NATS are expected to be JSON-encoded and use the structure from the following example, where banExpression is any valid expression that would be accepted by the `varnishadm` ban command.

```json
{
  ""operation"": ""applyban"",
  ""parameters"": {
    ""banExpression"": ""req.url ~ /foo""
  }
}
```",2114
CI/CD Pipeline Deployment,CI/CD Pipeline Deployment,"If you've been manually deploying your app onto Section (e.g. via the Console UI or `kubectl`), there are many benefits to having an automated pipeline deployment that you should consider.

Reasons to have an automated pipeline:
- There's no need to share Auth tokens to access your Kubernetes cluster amongst your dev team
- You can run automated tests prior, mitigating risks of a bad deployment
- Builds always commence from a consistent starting condition, avoiding scenarios of ""it worked on my machine""",101
CI/CD Pipeline Deployment,Examples,"This tutorial provides you with examples for GitHub Actions workflow, and Bitbucket pipelines. If you require assistance with other CI/CD tools, reach out to us via the [support](https://console.section.io/support) channel and we will endeavour to assist.",51
CI/CD Pipeline Deployment,Prerequisites,"You will need to obtain the following values before proceeding:
- `SECTION_K8S_API_URL`: This is the value of your project's [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) 
- `SECTION_API_TOKEN`: Create or use an existing [Section API token](/guides/iam/api-tokens/)

*Optional*:
- A container image repository",85
CI/CD Pipeline Deployment,Create your Kubernetes deployment YAMLs,"For simplicity, we will store our deployment YAMLs within a folder named `k8s` at the root of the repository. You may add additional files to be deployed if desired.

```sh
mkdir -p k8s
touch k8s/deploy.yaml
```

```yaml title=""./k8s/deploy.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cicd-demo
  labels:
    app: cicd-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cicd-demo
  template:
    metadata:
      labels:
        app: cicd-demo
    spec:
      containers:
      - name: cicd-demo
        image: ${IMAGE_NAME}:main
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "".1""
            memory: "".1Gi""
          limits:
            cpu: "".1""
            memory: "".1Gi""
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-upstream
  labels:
    app: ingress-upstream
spec:
  selector:
    app: cicd-demo
  ports:
    - name: 80-to-80
      protocol: TCP
      port: 80
      targetPort: 80

```

---",288
CI/CD Pipeline Deployment,GitHub Actions,"GitHub Actions ([docs](https://docs.github.com/en/actions/using-workflows/about-workflows)) are defined by creating workflow `.yaml` files within the `.github/workflows` directory in the root of your repository. The following is an example folder structure, with a `deploy-to-section.yaml` action defined:

``` title=""GitHub Actions folder structure""
.
├── .github
│   └── workflows
│       ├── < WORKFLOW_NAME >.yaml
│       └── deploy-to-section.yaml
├── k8s
│   └── deploy.yaml
├── Dockerfile
├── README.md
└── ...
```

### Add repository secrets

Before you add and commit the workflows YAML files, you will first need to insert the two values from earlier (`SECTION_K8S_API_URL` & `SECTION_API_TOKEN`) as repository secrets in the GitHub repo. 

This can be added via Settings > Secrets > Actions, use **`New repository secret`** to add the two secrets.

![GitHub Secrets](/img/docs/cicd-pipeline-gh-secrets.png)


### Create the workflow YAML

Create the GitHub Actions workflows `directory` and `YAML` if you haven't done so already

```sh
mkdir -p .github/workflows
touch .github/workflows/deploy-to-section.yaml
```

Within the `deploy-to-section.yaml` file, insert the following contents:

```yaml title=""deploy-to-section.yaml""
name: Deploy to Section

on:
  push:
    branches: [main]

env:
  REGISTRY: ghcr.io
  REPO_NAME: ${{ github.repository }} # Uses the GitHub repository name as image name by default. Override if desired.
  APP_NAME: cicd-demo # k8s deployment's app name (refer to k8s/deploy.yaml)

jobs:
  build-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
          
      - name: downcase REPO
        run: |
          echo ""LOWERCASE_REPO=${REPO_NAME,,}"" >>${GITHUB_ENV}
        
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.LOWERCASE_REPO }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

      - name: Deploy on Section
        env:
          SECTION_K8S_API_URL: ""${{ secrets.SECTION_K8S_API_URL }}""
          SECTION_API_TOKEN: ""${{ secrets.SECTION_API_TOKEN }}""
          CERT_PATH: ""/etc/ssl/certs/ca-certificates.crt""
          IMAGE_NAME: ${{ env.REGISTRY }}/${{ env.LOWERCASE_REPO }}
        run: | 
          #######################################
          # Configure kubectl to talk to Section
          #######################################

          kubectl config set-cluster section-cluster --server=$SECTION_K8S_API_URL --certificate-authority=$CERT_PATH
          kubectl config set-credentials section-user --token=$SECTION_API_TOKEN
          kubectl config set-context section --cluster=section-cluster --user=section-user

          kubectl config use-context section

          ########################
          # Deploy k8s YAML file
          ########################

          envsubst '$IMAGE_NAME' < ./k8s/deploy.yaml > ./k8s/deploy.yaml.temp
          mv ./k8s/deploy.yaml.temp ./k8s/deploy.yaml

          kubectl apply -f ./k8s/

          kubectl rollout restart deployment $APP_NAME

```

Once all files have been committed and pushed, you should see the GitHub Action run immediately.

---

## Bitbucket Pipelines

Bitbucket Pipelines ([docs](https://support.atlassian.com/bitbucket-cloud/docs/configure-bitbucket-pipelinesyml/)) are configured by creating a `bitbucket-pipelines.yml` file in the root of your repository. The following is an example folder structure:

``` title=""Bitbucket Pipelines folder structure""
.
├── k8s
│   └── deploy.yaml
├── bitbucket-pipelines.yml
├── Dockerfile
├── README.md
└── ...
```",996
CI/CD Pipeline Deployment,Add Repository Variables,"You may refer to the [Bitbucket documentation](https://support.atlassian.com/bitbucket-cloud/docs/variables-and-secrets/) on steps to add Secrets and Variables to be used by this pipeline.

For the purposes of this tutorial, we assume that your image will be deployed on Docker Hub, and will require inserting user credentials.

Secrets/Variables needed:
- `DOCKERHUB_USERNAME`: Login credentials for Docker Hub.
- `DOCKERHUB_PASSWORD`: Login credentials for Docker Hub.
- `DOCKERHUB_NAMESPACE`: This may be the same value as the Username. If not, ensure that the user has write permissions to this namespace.
- `SECTION_K8S_API_URL`: This is the value of your project's [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) 
- `SECTION_API_TOKEN`: Create or use an existing [Section API token](/guides/iam/api-tokens/)
- `IMAGE_NAME`: The name for the built container image.",206
CI/CD Pipeline Deployment,Create the pipeline YAML,"Create the `bitbucket-pipelines.yml` file if you haven't done so already

```sh
touch bitbucket-pipelines.yml
```

Within that file, insert the following contents:
```yaml title=""bitbucket-pipelines.yml""
image: node:16.16.0
definitions:
  services:
    docker:
      memory: 2048
      
pipelines:            
  branches:
    main:
      - step:
          name: ""Build and push to Docker Hub""
          services:
            - docker
          caches:
            - docker
          script:
            - docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_PASSWORD
            - docker build -t $DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT .
            - docker push $DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT

      - step:
          name: Deploy to Section.io
          deployment: production
          script:
            # Install Kubectl
            - echo ""Installing Kubectl...""
            - curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl""
            - curl -LO ""https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256""
            - echo ""$(cat kubectl.sha256)  kubectl"" | sha256sum --check
            - install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

            # Configure kubectl to connect to Section's cluster
            - echo ""Configuring Kubectl...""
            - PATH_TO_CERT_AUTHORITY=""/etc/ssl/certs/ca-certificates.crt""
            - kubectl config set-cluster section-cluster --certificate-authority=$PATH_TO_CERT_AUTHORITY --server=$SECTION_K8S_API_URL
            - kubectl config set-credentials section-user --token=$SECTION_API_TOKEN
            - kubectl config set-context section --cluster=section-cluster --user=section-user --namespace=default
            - kubectl config use-context section

            # Deploying to Section
            - echo ""Deploying to Section...""
            - IMAGE_NAME=$DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT
            - envsubst '$IMAGE_NAME' < ./k8s/deploy.yaml > ./k8s/deploy.yaml.temp
            - mv ./k8s/deploy.yaml.temp ./k8s/deploy.yaml
            - kubectl apply -f ./k8s/
            - APP_NAME=""cicd-demo""  # k8s deployment's app name (refer to k8s/deploy.yaml above)
            - kubectl rollout restart deployment $APP_NAME

```

Once all files have been committed and pushed, you should see the Bitbucket pipeline run on any commits on the main branch.

---",610
CI/CD Pipeline Deployment,Need assistance?,"If you require assistance with other CI/CD tools, reach out to us via the [support](https://console.section.io/support) channel and we will endeavour to assist.",35
CI/CD Pipeline Deployment,Learn how to create a pipeline that deploys your code to Section automatically,"
# CI/CD Pipeline Deployment
If you've been manually deploying your app onto Section (e.g. via the Console UI or `kubectl`), there are many benefits to having an automated pipeline deployment that you should consider.

Reasons to have an automated pipeline:
- There's no need to share Auth tokens to access your Kubernetes cluster amongst your dev team
- You can run automated tests prior, mitigating risks of a bad deployment
- Builds always commence from a consistent starting condition, avoiding scenarios of ""it worked on my machine""

# Examples

This tutorial provides you with examples for GitHub Actions workflow, and Bitbucket pipelines. If you require assistance with other CI/CD tools, reach out to us via the [support](https://console.section.io/support) channel and we will endeavour to assist.

## Prerequisites
You will need to obtain the following values before proceeding:
- `SECTION_K8S_API_URL`: This is the value of your project's [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) 
- `SECTION_API_TOKEN`: Create or use an existing [Section API token](/guides/iam/api-tokens/)

*Optional*:
- A container image repository

### Create your Kubernetes deployment YAMLs

For simplicity, we will store our deployment YAMLs within a folder named `k8s` at the root of the repository. You may add additional files to be deployed if desired.

```sh
mkdir -p k8s
touch k8s/deploy.yaml
```

```yaml title=""./k8s/deploy.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cicd-demo
  labels:
    app: cicd-demo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cicd-demo
  template:
    metadata:
      labels:
        app: cicd-demo
    spec:
      containers:
      - name: cicd-demo
        image: ${IMAGE_NAME}:main
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "".1""
            memory: "".1Gi""
          limits:
            cpu: "".1""
            memory: "".1Gi""
        ports:
        - containerPort: 80

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-upstream
  labels:
    app: ingress-upstream
spec:
  selector:
    app: cicd-demo
  ports:
    - name: 80-to-80
      protocol: TCP
      port: 80
      targetPort: 80

```

---

## GitHub Actions

GitHub Actions ([docs](https://docs.github.com/en/actions/using-workflows/about-workflows)) are defined by creating workflow `.yaml` files within the `.github/workflows` directory in the root of your repository. The following is an example folder structure, with a `deploy-to-section.yaml` action defined:

``` title=""GitHub Actions folder structure""
.
├── .github
│   └── workflows
│       ├── < WORKFLOW_NAME >.yaml
│       └── deploy-to-section.yaml
├── k8s
│   └── deploy.yaml
├── Dockerfile
├── README.md
└── ...
```

### Add repository secrets

Before you add and commit the workflows YAML files, you will first need to insert the two values from earlier (`SECTION_K8S_API_URL` & `SECTION_API_TOKEN`) as repository secrets in the GitHub repo. 

This can be added via Settings > Secrets > Actions, use **`New repository secret`** to add the two secrets.

![GitHub Secrets](/img/docs/cicd-pipeline-gh-secrets.png)


### Create the workflow YAML

Create the GitHub Actions workflows `directory` and `YAML` if you haven't done so already

```sh
mkdir -p .github/workflows
touch .github/workflows/deploy-to-section.yaml
```

Within the `deploy-to-section.yaml` file, insert the following contents:

```yaml title=""deploy-to-section.yaml""
name: Deploy to Section

on:
  push:
    branches: [main]

env:
  REGISTRY: ghcr.io
  REPO_NAME: ${{ github.repository }} # Uses the GitHub repository name as image name by default. Override if desired.
  APP_NAME: cicd-demo # k8s deployment's app name (refer to k8s/deploy.yaml)

jobs:
  build-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
          
      - name: downcase REPO
        run: |
          echo ""LOWERCASE_REPO=${REPO_NAME,,}"" >>${GITHUB_ENV}
        
      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.LOWERCASE_REPO }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v3
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}

      - name: Deploy on Section
        env:
          SECTION_K8S_API_URL: ""${{ secrets.SECTION_K8S_API_URL }}""
          SECTION_API_TOKEN: ""${{ secrets.SECTION_API_TOKEN }}""
          CERT_PATH: ""/etc/ssl/certs/ca-certificates.crt""
          IMAGE_NAME: ${{ env.REGISTRY }}/${{ env.LOWERCASE_REPO }}
        run: | 
          #######################################
          # Configure kubectl to talk to Section
          #######################################

          kubectl config set-cluster section-cluster --server=$SECTION_K8S_API_URL --certificate-authority=$CERT_PATH
          kubectl config set-credentials section-user --token=$SECTION_API_TOKEN
          kubectl config set-context section --cluster=section-cluster --user=section-user

          kubectl config use-context section

          ########################
          # Deploy k8s YAML file
          ########################

          envsubst '$IMAGE_NAME' < ./k8s/deploy.yaml > ./k8s/deploy.yaml.temp
          mv ./k8s/deploy.yaml.temp ./k8s/deploy.yaml

          kubectl apply -f ./k8s/

          kubectl rollout restart deployment $APP_NAME

```

Once all files have been committed and pushed, you should see the GitHub Action run immediately.

---

## Bitbucket Pipelines

Bitbucket Pipelines ([docs](https://support.atlassian.com/bitbucket-cloud/docs/configure-bitbucket-pipelinesyml/)) are configured by creating a `bitbucket-pipelines.yml` file in the root of your repository. The following is an example folder structure:

``` title=""Bitbucket Pipelines folder structure""
.
├── k8s
│   └── deploy.yaml
├── bitbucket-pipelines.yml
├── Dockerfile
├── README.md
└── ...
```

### Add Repository Variables

You may refer to the [Bitbucket documentation](https://support.atlassian.com/bitbucket-cloud/docs/variables-and-secrets/) on steps to add Secrets and Variables to be used by this pipeline.

For the purposes of this tutorial, we assume that your image will be deployed on Docker Hub, and will require inserting user credentials.

Secrets/Variables needed:
- `DOCKERHUB_USERNAME`: Login credentials for Docker Hub.
- `DOCKERHUB_PASSWORD`: Login credentials for Docker Hub.
- `DOCKERHUB_NAMESPACE`: This may be the same value as the Username. If not, ensure that the user has write permissions to this namespace.
- `SECTION_K8S_API_URL`: This is the value of your project's [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) 
- `SECTION_API_TOKEN`: Create or use an existing [Section API token](/guides/iam/api-tokens/)
- `IMAGE_NAME`: The name for the built container image.

### Create the pipeline YAML

Create the `bitbucket-pipelines.yml` file if you haven't done so already

```sh
touch bitbucket-pipelines.yml
```

Within that file, insert the following contents:
```yaml title=""bitbucket-pipelines.yml""
image: node:16.16.0
definitions:
  services:
    docker:
      memory: 2048
      
pipelines:            
  branches:
    main:
      - step:
          name: ""Build and push to Docker Hub""
          services:
            - docker
          caches:
            - docker
          script:
            - docker login -u $DOCKERHUB_USERNAME -p $DOCKERHUB_PASSWORD
            - docker build -t $DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT .
            - docker push $DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT

      - step:
          name: Deploy to Section.io
          deployment: production
          script:
            # Install Kubectl
            - echo ""Installing Kubectl...""
            - curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl""
            - curl -LO ""https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256""
            - echo ""$(cat kubectl.sha256)  kubectl"" | sha256sum --check
            - install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

            # Configure kubectl to connect to Section's cluster
            - echo ""Configuring Kubectl...""
            - PATH_TO_CERT_AUTHORITY=""/etc/ssl/certs/ca-certificates.crt""
            - kubectl config set-cluster section-cluster --certificate-authority=$PATH_TO_CERT_AUTHORITY --server=$SECTION_K8S_API_URL
            - kubectl config set-credentials section-user --token=$SECTION_API_TOKEN
            - kubectl config set-context section --cluster=section-cluster --user=section-user --namespace=default
            - kubectl config use-context section

            # Deploying to Section
            - echo ""Deploying to Section...""
            - IMAGE_NAME=$DOCKERHUB_NAMESPACE/$IMAGE_NAME:$BITBUCKET_COMMIT
            - envsubst '$IMAGE_NAME' < ./k8s/deploy.yaml > ./k8s/deploy.yaml.temp
            - mv ./k8s/deploy.yaml.temp ./k8s/deploy.yaml
            - kubectl apply -f ./k8s/
            - APP_NAME=""cicd-demo""  # k8s deployment's app name (refer to k8s/deploy.yaml above)
            - kubectl rollout restart deployment $APP_NAME

```

Once all files have been committed and pushed, you should see the Bitbucket pipeline run on any commits on the main branch.

---

## Need assistance?

If you require assistance with other CI/CD tools, reach out to us via the [support](https://console.section.io/support) channel and we will endeavour to assist.",2416
Migration from Fastly,Introduction,"This is a general guide to porting VCL (Varnish Configuration Language) from your Fastly CDN service to a Varnish container running on Section’s Kubernetes Edge Interface (KEI). 

In general, much of the VCL used on the Fastly platform can directly port over to Varnish (We recommend running Varnish 7.0.2 or later), but there are a few caveats and things to consider.",91
Migration from Fastly,Origin shielding,"*  Origin shielding is a Fastly infrastructure feature that sends edge requests to ‘edge POPs’, which then make the request to a ‘shield POP’, increasing cache hit ratio in certain situations. If you are running Varnish on a large number of POPs and would like to see if Section can help you build a similar system through our Professional Services program, please contact Support.",77
Migration from Fastly,Clustering,"* By default, Fastly uses an algorithm to route requests to the same URL to the same Varnish instance in their POPs. This is known as ‘clustering’ and is not a default Varnish behavior. If you have a large-scale CDN plan in mind, we recommend using a `consistenthash` container to provide similar functionality, increasing cache hit ratio for cacheable assets. Improvements in cache hit ratio via deploying `consistenthash` will be dependent on the number of Varnish containers and endpoints you will be running.",110
Migration from Fastly,Logging,"* Fastly logging format is defined in the VCL itself. On Section, you will define an external logging endpoint as we show in our [Log Streaming guide](https://sites.www.section.io/docs/guides/monitor/logs/log-streaming/). This has the advantage of being able to log all components to your logging system - so any A/B testing, IDS or bot detection components you choose to deploy can also send logs to your storage, enabling full visibility into the full application stack.",99
Migration from Fastly,Dictionaries and ACLs,"* Dictionaries are a proprietary Fastly feature, and there is not yet equivalent functionality in the open source Varnish. 
* Note that Access Control Lists (ACLs) _do_ work in OSS Varnish, so if you are using ACLs on your Fastly service to control access on an IP level, they will be usable.",71
Migration from Fastly,req.url.path,"* This variable contains the request URL without any query strings. This is a proprietary Fastly feature that has no out of the box equivalent in OSS Varnish. If you require this functionality, contact our Support Team to discuss your neesd and a possible implentation.",56
Migration from Fastly,`vcl_fetch`,"* This is Fastly’s name for the `vcl_backend_response` subroutine, which runs immediately after the content is fetched either from the origin or cache, and before it is delivered to the browser (via `vcl_deliver`).  If you would like to modify a response before delivering to the browser, place this in `vcl_backend_response`.",72
Migration from Fastly,`vcl_hit` and `vcl_miss`,"* These functions are available to your Varnish configuration, but are not present in the default config. Feel free to add them if they are needed.",31
Migration from Fastly,VCL example,"Fastly creates a new service with a basic configuration suitable for general caching needs.  In most cases, the default Varnish VCL below is a great starting point.

```
# This is an extension on the default VCL that Section has created to get
# you up and running with Varnish.
#
# Please note: There is an underlying default Varnish behavior that occurs after the VCL logic
# you see below. You can see the builtin code here
# https://github.com/varnishcache/varnish-cache/blob/5.2/bin/varnishd/builtin.vcl
#
# See the VCL chapters in the Users Guide at https://www.varnish-cache.org/docs/
# and http://varnish-cache.org/trac/wiki/VCLExamples for more examples.

# Marker to tell the VCL compiler that this VCL has been adapted to the
# new 4.0 format.
vcl 4.0;

# Tells Varnish the location of the upstream. Do not change .host and .port.
backend default {
    .host = “backend.example.com”;
    .port = ""80"";
    .first_byte_timeout = 125s;
    .between_bytes_timeout = 125s;
}

# The following VMODs are available for use if required:
#import std; # see https://www.varnish-cache.org/docs/5.2/reference/vmod_std.generated.html
#import header; # see https://github.com/varnish/varnish-modules


# Method: vcl_recv
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-recv
# Description: Happens before we check if we have this in cache already.
#
# Purpose: Typically you clean up the request here, removing cookies you don't need,
# rewriting the request, etc.
sub vcl_recv {
    
# Section default code
#
# Purpose: If the request method is not GET, HEAD or PURGE, return pass.
# Documentation: Reference documentation for vcl_recv.
if (req.method != ""GET"" && req.method != ""HEAD"" && req.method != ""PURGE"") {
    return (pass);
}

# Section default code
#
# Purpose: If the request contains auth header return pass.
# Documentation: Reference documentation for vcl_recv.
if (req.http.Authorization) {
    /* Not cacheable by default */
    return (pass);
    }

}


# Method: vcl_backend_fetch
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-backend-fetch
# Description: Called before sending the backend request.
#
# Purpose: Typically you alter the request for the backend here. Overriding to the
# required hostname, upstream Proto matching, etc
sub vcl_backend_fetch {
    # No default Section code for vcl_backend_fetch
}


# Method: vcl_backend_response
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-backend-response
# Description: Happens after reading the response headers from the backend.
#
# Purpose: Here you clean the response headers, removing Set-Cookie headers
# and other mistakes your backend may produce. This is also where you can manually
# set cache TTL periods.
sub vcl_backend_response {

unset beresp.http.Vary;

}


# Method: vcl_deliver
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-deliver
# Description: Happens when we have all the pieces we need, and are about to send the
# response to the client.
#
# Purpose: You can do accounting logic or modify the final object here.
sub vcl_deliver {
    # Section default code
    #
    # Purpose: We are setting 'HIT' or 'MISS' as a custom header for easy debugging.
    if (obj.hits > 0) {
       set resp.http.section-io-cache = ""Hit"";
    } else {
       set resp.http.section-io-cache = ""Miss"";
    }
    
set resp.http.hits = obj.hits;

}

sub vcl_synth {
  
}

# Method: vcl_hash
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-hash
# Description: This method is used to build up a key to look up the object in Varnish.
#
# Purpose: You can specify which headers you want to cache by.
sub vcl_hash {
    # Section default code
    #
    # Purpose: Split cache by HTTP and HTTPS protocol.
    hash_data(req.http.X-Forwarded-Proto);
}
```",993
Migration from Fastly,Learn to migrate your VCL from Fastly Section,"
# Porting your Fastly VCL Configuration To Varnish On Section.io

## Introduction

This is a general guide to porting VCL (Varnish Configuration Language) from your Fastly CDN service to a Varnish container running on Section’s Kubernetes Edge Interface (KEI). 

In general, much of the VCL used on the Fastly platform can directly port over to Varnish (We recommend running Varnish 7.0.2 or later), but there are a few caveats and things to consider.

## Considerations

### Infrastructure

#### Origin shielding

*  Origin shielding is a Fastly infrastructure feature that sends edge requests to ‘edge POPs’, which then make the request to a ‘shield POP’, increasing cache hit ratio in certain situations. If you are running Varnish on a large number of POPs and would like to see if Section can help you build a similar system through our Professional Services program, please contact Support.

#### Clustering

* By default, Fastly uses an algorithm to route requests to the same URL to the same Varnish instance in their POPs. This is known as ‘clustering’ and is not a default Varnish behavior. If you have a large-scale CDN plan in mind, we recommend using a `consistenthash` container to provide similar functionality, increasing cache hit ratio for cacheable assets. Improvements in cache hit ratio via deploying `consistenthash` will be dependent on the number of Varnish containers and endpoints you will be running.

#### Logging

* Fastly logging format is defined in the VCL itself. On Section, you will define an external logging endpoint as we show in our [Log Streaming guide](https://sites.www.section.io/docs/guides/monitor/logs/log-streaming/). This has the advantage of being able to log all components to your logging system - so any A/B testing, IDS or bot detection components you choose to deploy can also send logs to your storage, enabling full visibility into the full application stack.

### Dictionaries and ACLs
  
* Dictionaries are a proprietary Fastly feature, and there is not yet equivalent functionality in the open source Varnish. 
* Note that Access Control Lists (ACLs) _do_ work in OSS Varnish, so if you are using ACLs on your Fastly service to control access on an IP level, they will be usable.

## Code Considerations

### Special variables

#### req.url.path
* This variable contains the request URL without any query strings. This is a proprietary Fastly feature that has no out of the box equivalent in OSS Varnish. If you require this functionality, contact our Support Team to discuss your neesd and a possible implentation.

### Subroutines

#### `vcl_fetch`

* This is Fastly’s name for the `vcl_backend_response` subroutine, which runs immediately after the content is fetched either from the origin or cache, and before it is delivered to the browser (via `vcl_deliver`).  If you would like to modify a response before delivering to the browser, place this in `vcl_backend_response`.

#### `vcl_hit` and `vcl_miss`

* These functions are available to your Varnish configuration, but are not present in the default config. Feel free to add them if they are needed.



## VCL example

Fastly creates a new service with a basic configuration suitable for general caching needs.  In most cases, the default Varnish VCL below is a great starting point.

```
# This is an extension on the default VCL that Section has created to get
# you up and running with Varnish.
#
# Please note: There is an underlying default Varnish behavior that occurs after the VCL logic
# you see below. You can see the builtin code here
# https://github.com/varnishcache/varnish-cache/blob/5.2/bin/varnishd/builtin.vcl
#
# See the VCL chapters in the Users Guide at https://www.varnish-cache.org/docs/
# and http://varnish-cache.org/trac/wiki/VCLExamples for more examples.

# Marker to tell the VCL compiler that this VCL has been adapted to the
# new 4.0 format.
vcl 4.0;

# Tells Varnish the location of the upstream. Do not change .host and .port.
backend default {
    .host = “backend.example.com”;
    .port = ""80"";
    .first_byte_timeout = 125s;
    .between_bytes_timeout = 125s;
}

# The following VMODs are available for use if required:
#import std; # see https://www.varnish-cache.org/docs/5.2/reference/vmod_std.generated.html
#import header; # see https://github.com/varnish/varnish-modules


# Method: vcl_recv
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-recv
# Description: Happens before we check if we have this in cache already.
#
# Purpose: Typically you clean up the request here, removing cookies you don't need,
# rewriting the request, etc.
sub vcl_recv {
    
# Section default code
#
# Purpose: If the request method is not GET, HEAD or PURGE, return pass.
# Documentation: Reference documentation for vcl_recv.
if (req.method != ""GET"" && req.method != ""HEAD"" && req.method != ""PURGE"") {
    return (pass);
}

# Section default code
#
# Purpose: If the request contains auth header return pass.
# Documentation: Reference documentation for vcl_recv.
if (req.http.Authorization) {
    /* Not cacheable by default */
    return (pass);
    }

}


# Method: vcl_backend_fetch
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-backend-fetch
# Description: Called before sending the backend request.
#
# Purpose: Typically you alter the request for the backend here. Overriding to the
# required hostname, upstream Proto matching, etc
sub vcl_backend_fetch {
    # No default Section code for vcl_backend_fetch
}


# Method: vcl_backend_response
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-backend-response
# Description: Happens after reading the response headers from the backend.
#
# Purpose: Here you clean the response headers, removing Set-Cookie headers
# and other mistakes your backend may produce. This is also where you can manually
# set cache TTL periods.
sub vcl_backend_response {

unset beresp.http.Vary;

}


# Method: vcl_deliver
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-deliver
# Description: Happens when we have all the pieces we need, and are about to send the
# response to the client.
#
# Purpose: You can do accounting logic or modify the final object here.
sub vcl_deliver {
    # Section default code
    #
    # Purpose: We are setting 'HIT' or 'MISS' as a custom header for easy debugging.
    if (obj.hits > 0) {
       set resp.http.section-io-cache = ""Hit"";
    } else {
       set resp.http.section-io-cache = ""Miss"";
    }
    
set resp.http.hits = obj.hits;

}

sub vcl_synth {
  
}

# Method: vcl_hash
# Documentation: https://varnish-cache.org/docs/5.2/users-guide/vcl-built-in-subs.html#vcl-hash
# Description: This method is used to build up a key to look up the object in Varnish.
#
# Purpose: You can specify which headers you want to cache by.
sub vcl_hash {
    # Section default code
    #
    # Purpose: Split cache by HTTP and HTTPS protocol.
    hash_data(req.http.X-Forwarded-Proto);
}
```


",1689
Cloudflare Workers on Section,Why run a container instead of Functions-as-a-Service (FaaS)?,"- Cloudflare Workers are resource bound, with 128 MB of memory allocated. 
- Cloudflare Workers are time capped. 10ms, 50ms, up to 30s depending on pricing tier.

If either of the above is causing a limitation for your workload, you should consider moving from a FaaS to a container hosted solution.

With containers, you have the ability to choose your own resource allocation, or define your own timeout duration for long running workloads (_such as reports generation_), leverage other containers from the community, and develop solutions using any language and technology of your choice.

In this tutorial we will show you how to convert your CF Worker to a container image, which you can then host on Section's multi-region, multi-cloud Kubernetes hosting solution.

This example app contains 2 function paths:
- A basic hello world, eg [Return small HTML page](https://developers.cloudflare.com/workers/examples/return-html/)
- A redirector, eg [Respond with another site](https://developers.cloudflare.com/workers/examples/respond-with-another-site/)",215
Cloudflare Workers on Section,Prerequisites,"- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- An ExpressJS app
- (optional) kubectl",31
Cloudflare Workers on Section,Steps,"1. Folder structure
1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section",26
Cloudflare Workers on Section,Folder structure,"You can refer to our example repository here - https://github.com/section/cfworker-tutorial

In our example, we have an existing Express setup, with a subfolder containing each function's code.

```
.
├── functions
│   ├── helloWorld.js
│   ├── respondWithAnotherSite.js
│   └── ...
├── app.js
├── Dockerfile
└── package.json
```

Within `app.js`, you would write the specific paths that correspond with the functions to be run. In our example, the ""`/`"" route returns a basic Hello World html page, and the ""`/section`"" route returns a redirected page.

```js title=""app.js""
const express = require('express');
const helloWorld = require('./functions/helloWorld');
const respondWithAnotherSite = require('./functions/respondWithAnotherSite');

const app = express()
const port = process.env.PORT || 3000

// Return small HTML page - https://developers.cloudflare.com/workers/examples/return-html/
app.get('/', async function(req, res) {
  res.send(await helloWorld())
})

// Respond with another site - https://developers.cloudflare.com/workers/examples/respond-with-another-site/
app.get('/section', async function(req, res) {
  res.send(await respondWithAnotherSite())
})


app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`)
})
```

To test that this works, you simply need to run `node app.js`

### Creating your Dockerfile

Create a `Dockerfile` in the root folder of your app. It should sit alongside package.json.

The [Dockerfile](https://github.com/section/cfworker-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as runner
WORKDIR /app
COPY package*.json ./
RUN npm clean-install
COPY . .
EXPOSE 3000
CMD [""node"", ""app.js""]

```

This script would use Node to run an Express app listening on port 3000 of the container for any incoming requests.",449
Cloudflare Workers on Section,Building the container image,"Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh",20
Cloudflare Workers on Section,Replace these example values,"USER=section
IMAGENAME=cf-worker
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```",181
Cloudflare Workers on Section,Deploy to Section,"Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before, and specify port 3000.",36
Cloudflare Workers on Section,Learn how to move your Cloudflare workers into a container image on Section,"
# Deploy a CF Worker-like App on Section 

## Why run a container instead of Functions-as-a-Service (FaaS)?
- Cloudflare Workers are resource bound, with 128 MB of memory allocated. 
- Cloudflare Workers are time capped. 10ms, 50ms, up to 30s depending on pricing tier.

If either of the above is causing a limitation for your workload, you should consider moving from a FaaS to a container hosted solution.

With containers, you have the ability to choose your own resource allocation, or define your own timeout duration for long running workloads (_such as reports generation_), leverage other containers from the community, and develop solutions using any language and technology of your choice.

In this tutorial we will show you how to convert your CF Worker to a container image, which you can then host on Section's multi-region, multi-cloud Kubernetes hosting solution.

This example app contains 2 function paths:
- A basic hello world, eg [Return small HTML page](https://developers.cloudflare.com/workers/examples/return-html/)
- A redirector, eg [Respond with another site](https://developers.cloudflare.com/workers/examples/respond-with-another-site/)

## Step by Step

### Prerequisites
- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- An ExpressJS app
- (optional) kubectl

### Steps
1. Folder structure
1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section

### Folder structure
You can refer to our example repository here - https://github.com/section/cfworker-tutorial

In our example, we have an existing Express setup, with a subfolder containing each function's code.

```
.
├── functions
│   ├── helloWorld.js
│   ├── respondWithAnotherSite.js
│   └── ...
├── app.js
├── Dockerfile
└── package.json
```

Within `app.js`, you would write the specific paths that correspond with the functions to be run. In our example, the ""`/`"" route returns a basic Hello World html page, and the ""`/section`"" route returns a redirected page.

```js title=""app.js""
const express = require('express');
const helloWorld = require('./functions/helloWorld');
const respondWithAnotherSite = require('./functions/respondWithAnotherSite');

const app = express()
const port = process.env.PORT || 3000

// Return small HTML page - https://developers.cloudflare.com/workers/examples/return-html/
app.get('/', async function(req, res) {
  res.send(await helloWorld())
})

// Respond with another site - https://developers.cloudflare.com/workers/examples/respond-with-another-site/
app.get('/section', async function(req, res) {
  res.send(await respondWithAnotherSite())
})


app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`)
})
```

To test that this works, you simply need to run `node app.js`

### Creating your Dockerfile

Create a `Dockerfile` in the root folder of your app. It should sit alongside package.json.

The [Dockerfile](https://github.com/section/cfworker-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as runner
WORKDIR /app
COPY package*.json ./
RUN npm clean-install
COPY . .
EXPOSE 3000
CMD [""node"", ""app.js""]

```

This script would use Node to run an Express app listening on port 3000 of the container for any incoming requests.


### Building the container image

Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh
# Replace these example values
USER=section
IMAGENAME=cf-worker
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```


## Deploy to Section

Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before, and specify port 3000.
",1023
Mastodon,Mastodon on Section,"Learn how to run a Mastodon server at the edge for low latency and high availability. Perform the steps below using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",119
Mastodon,Prerequisites,"* You need a Postgres database (try using [PolyScale.ai](https://polyscale.ai) to optimize access to a fixed-location Postgres database of your choice). See our [tutorial](/tutorials/data/polyscale-caching.md) on how PolyScale works with Section.
* Redis database (try a managed Redis, such as [DigitalOcean's](https://www.digitalocean.com/products/managed-databases-redis), or from the several listed [here](https://geekflare.com/redis-hosting-platform/)).
* Email delivery service or SMTP server.
* S3 bucket (try a distributed managed object store like [Supabase](https://supabase.com), [Backblaze](https://www.backblaze.com/b2/docs/s3_compatible_api.html), [Synadia Jetstream Obj store](https://synadia.com/ngs), or [Wasabi](https://wasabi.com)).",191
Mastodon,Deploy It,"Create a Section deployment for the Mastodon server with a `mastodon-deployment.yaml` file, substituting the [environment variables](https://docs.joinmastodon.org/admin/config/) accordingly. This will direct Section to distribute the [`linuxserver/mastodon`](https://hub.docker.com/r/linuxserver/mastodon) image.

```yaml title=""mastodon-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mastodon
  labels:
    app: mastodon
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mastodon
  template:
    metadata:
      labels:
        app: mastodon
    spec:
      containers:
      - name: mastodon
        image: linuxserver/mastodon:4.0.2
        imagePullPolicy: Always
        lifecycle:
          postStart:
            exec:
              command:
              - ""/bin/sh""
              - ""-c""
              - >
                sleep 5;
                sed -i -e ""s/\$scheme/'https'/"" /config/nginx/site-confs/default.conf
        resources:
          requests:
            memory: ""1000Mi""
            cpu: ""1000m""
          limits:
            memory: ""1000Mi""
            cpu: ""1000m""
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            port: 80
            httpHeaders:
            - name: ""Host""
              value: ""mastodon.example.com""
          failureThreshold: 15
          initialDelaySeconds: 60
          periodSeconds: 20
        env:
        - name: PUID
          value: ""1000""
        - name: PGID
          value: ""1000""
        - name: TZ
          value: ""America/New_York""
        - name: LOCAL_DOMAIN
          value: ""mastodon.example.com""
        - name: REDIS_HOST
          value: ""redis""
        - name: REDIS_PORT
          value: ""6379""
        - name: DB_HOST
          value: ""db""
        - name: DB_USER
          value: ""mastodon""
        - name: DB_NAME
          value: ""mastodon""
        - name: DB_PASS
          value: ""mastodon""
        - name: DB_PORT
          value: ""5432""
        - name: SECRET_KEY_BASE
          value: """"
        - name: OTP_SECRET
          value: """"
        - name: VAPID_PRIVATE_KEY
          value: """"
        - name: VAPID_PUBLIC_KEY
          value: """"
        - name: SMTP_SERVER
          value: ""mail.example.com""
        - name: SMTP_PORT
          value: ""25""
        - name: SMTP_LOGIN
          value: """"
        - name: SMTP_PASSWORD
          value: """"
        - name: SMTP_FROM_ADDRESS
          value: ""notifications@example.com""
        - name: ES_ENABLED
          value: ""false""
        - name: ES_HOST # optional
          value: ""es""
        - name: ES_PORT # optional
          value: ""9200""
        - name: ES_USER # optional
          value: ""elastic""
        - name: ES_PASS # optional
          value: ""elastic""
        - name: S3_ENABLED
          value: ""false""
        - name: S3_BUCKET # optional
          value: """"
        - name: AWS_ACCESS_KEY_ID # optional
          value: """"
        - name: AWS_SECRET_ACCESS_KEY # optional
          value: """"
        - name: S3_ALIAS_HOST # optional
          value: """"
        - name: WEB_DOMAIN # optional
          value: ""mastodon.example.com""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f mastodon-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::tip
For a production Mastodon server, use [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables) as the values for private environment variables.
:::

### Expose It
Expose it on the internet, mapping the container's port `80`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
        app: mastodon
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`. The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",1167
Mastodon,See What You've Built,"See the Mastodon server you've built by visiting the `https://mastodon.example.com`, substituting `mastodon.example.com` according to your DNS and HTTPS configuration.",35
Mastodon,Learn to deploy a Mastodon server at the edge,"
# Mastodon on Section
Learn how to run a Mastodon server at the edge for low latency and high availability. Perform the steps below using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need a Postgres database (try using [PolyScale.ai](https://polyscale.ai) to optimize access to a fixed-location Postgres database of your choice). See our [tutorial](/tutorials/data/polyscale-caching.md) on how PolyScale works with Section.
* Redis database (try a managed Redis, such as [DigitalOcean's](https://www.digitalocean.com/products/managed-databases-redis), or from the several listed [here](https://geekflare.com/redis-hosting-platform/)).
* Email delivery service or SMTP server.
* S3 bucket (try a distributed managed object store like [Supabase](https://supabase.com), [Backblaze](https://www.backblaze.com/b2/docs/s3_compatible_api.html), [Synadia Jetstream Obj store](https://synadia.com/ngs), or [Wasabi](https://wasabi.com)).

### Deploy It
Create a Section deployment for the Mastodon server with a `mastodon-deployment.yaml` file, substituting the [environment variables](https://docs.joinmastodon.org/admin/config/) accordingly. This will direct Section to distribute the [`linuxserver/mastodon`](https://hub.docker.com/r/linuxserver/mastodon) image.

```yaml title=""mastodon-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mastodon
  labels:
    app: mastodon
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mastodon
  template:
    metadata:
      labels:
        app: mastodon
    spec:
      containers:
      - name: mastodon
        image: linuxserver/mastodon:4.0.2
        imagePullPolicy: Always
        lifecycle:
          postStart:
            exec:
              command:
              - ""/bin/sh""
              - ""-c""
              - >
                sleep 5;
                sed -i -e ""s/\$scheme/'https'/"" /config/nginx/site-confs/default.conf
        resources:
          requests:
            memory: ""1000Mi""
            cpu: ""1000m""
          limits:
            memory: ""1000Mi""
            cpu: ""1000m""
        ports:
        - containerPort: 80
        readinessProbe:
          httpGet:
            port: 80
            httpHeaders:
            - name: ""Host""
              value: ""mastodon.example.com""
          failureThreshold: 15
          initialDelaySeconds: 60
          periodSeconds: 20
        env:
        - name: PUID
          value: ""1000""
        - name: PGID
          value: ""1000""
        - name: TZ
          value: ""America/New_York""
        - name: LOCAL_DOMAIN
          value: ""mastodon.example.com""
        - name: REDIS_HOST
          value: ""redis""
        - name: REDIS_PORT
          value: ""6379""
        - name: DB_HOST
          value: ""db""
        - name: DB_USER
          value: ""mastodon""
        - name: DB_NAME
          value: ""mastodon""
        - name: DB_PASS
          value: ""mastodon""
        - name: DB_PORT
          value: ""5432""
        - name: SECRET_KEY_BASE
          value: """"
        - name: OTP_SECRET
          value: """"
        - name: VAPID_PRIVATE_KEY
          value: """"
        - name: VAPID_PUBLIC_KEY
          value: """"
        - name: SMTP_SERVER
          value: ""mail.example.com""
        - name: SMTP_PORT
          value: ""25""
        - name: SMTP_LOGIN
          value: """"
        - name: SMTP_PASSWORD
          value: """"
        - name: SMTP_FROM_ADDRESS
          value: ""notifications@example.com""
        - name: ES_ENABLED
          value: ""false""
        - name: ES_HOST # optional
          value: ""es""
        - name: ES_PORT # optional
          value: ""9200""
        - name: ES_USER # optional
          value: ""elastic""
        - name: ES_PASS # optional
          value: ""elastic""
        - name: S3_ENABLED
          value: ""false""
        - name: S3_BUCKET # optional
          value: """"
        - name: AWS_ACCESS_KEY_ID # optional
          value: """"
        - name: AWS_SECRET_ACCESS_KEY # optional
          value: """"
        - name: S3_ALIAS_HOST # optional
          value: """"
        - name: WEB_DOMAIN # optional
          value: ""mastodon.example.com""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f mastodon-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::tip
For a production Mastodon server, use [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables) as the values for private environment variables.
:::

### Expose It
Expose it on the internet, mapping the container's port `80`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
        app: mastodon
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`. The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the Mastodon server you've built by visiting the `https://mastodon.example.com`, substituting `mastodon.example.com` according to your DNS and HTTPS configuration.
",1535
Quant WAF,QuantWAF on Section,"Learn how to deploy a QuantWAF instance at the edge for low latency, high availability and to secure your web applications. QuantWAF has been packaged as a [helm chart](https://helm.sh/) so make sure to configure [`kubectl`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::note
Before starting, create a new Section Project, you can then optionally [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment).
:::",100
Quant WAF,Prerequisites,"* You will need a [QuantWAF License](https://quantcdn.io/waf), please contact QuantCDN directly to get a license key.",31
Quant WAF,Create the `values.yml` file,"The QuantWAF helm chart provides a list of configuration options to aid in the deployment of the WAF instance. At minimum; `nextHop.selector` and `quant` keys must be defined in your values.yaml file. Please view [QuantWAF documentation](https://quantcdn.io/waf) for a full list of supported values.

```yaml title=""values.yaml""
nextHop:
  selector:
    app: console-project
quant:
  license: <your quantwaf license key>
  endpoint: <your quant endpoint>
  key: <your quant key>
  organization: <your quant organization>
  project: <your quant project>
```

`NextHop.selector` is an object that matches selector values that have been applied to your applications deployment. The default section application deployment will use `app: console-project` as the selector, however this will need to be updated to match selection criteria that you have defined for your application.

:::note
The configuration values for the `quant` configuration object will be provided to you during onboarding.
:::

### Deploy It

Create a Section deployment for the QuantWAF instance with `helm` and the `values.yaml` file, ensuring that you have updated the values file accordingly. This will create all the required resources and configure the QuantWAF appropriately for your account.

```
helm install quant-waf quant-waf -f values.yaml
```

:::tip
You can review the resources that will be created with `helm install --dry-run` before you apply directly to your cluster
:::


### Expose It
Expose it on the internet, mapping the container's port `80`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-80
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app.kubernetes.io/name: quant-waf
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`. The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",607
Quant WAF,See What You've Built,"See the QuantWAF instance you've deployed by visiting the `https://quantwaf.example.com?q=/etc/hosts`, substituting `quantwaf.example.com` according to your DNS and HTTPS configuration.",43
Quant WAF,Learn how to deploy a Quant web application firewall at the edge,"
# QuantWAF on Section

Learn how to deploy a QuantWAF instance at the edge for low latency, high availability and to secure your web applications. QuantWAF has been packaged as a [helm chart](https://helm.sh/) so make sure to configure [`kubectl`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::note
Before starting, create a new Section Project, you can then optionally [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment).
:::

### Prerequisites
* You will need a [QuantWAF License](https://quantcdn.io/waf), please contact QuantCDN directly to get a license key.

### Create the `values.yml` file

The QuantWAF helm chart provides a list of configuration options to aid in the deployment of the WAF instance. At minimum; `nextHop.selector` and `quant` keys must be defined in your values.yaml file. Please view [QuantWAF documentation](https://quantcdn.io/waf) for a full list of supported values.

```yaml title=""values.yaml""
nextHop:
  selector:
    app: console-project
quant:
  license: <your quantwaf license key>
  endpoint: <your quant endpoint>
  key: <your quant key>
  organization: <your quant organization>
  project: <your quant project>
```

`NextHop.selector` is an object that matches selector values that have been applied to your applications deployment. The default section application deployment will use `app: console-project` as the selector, however this will need to be updated to match selection criteria that you have defined for your application.

:::note
The configuration values for the `quant` configuration object will be provided to you during onboarding.
:::

### Deploy It

Create a Section deployment for the QuantWAF instance with `helm` and the `values.yaml` file, ensuring that you have updated the values file accordingly. This will create all the required resources and configure the QuantWAF appropriately for your account.

```
helm install quant-waf quant-waf -f values.yaml
```

:::tip
You can review the resources that will be created with `helm install --dry-run` before you apply directly to your cluster
:::


### Expose It
Expose it on the internet, mapping the container's port `80`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-80
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app.kubernetes.io/name: quant-waf
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`. The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built

See the QuantWAF instance you've deployed by visiting the `https://quantwaf.example.com?q=/etc/hosts`, substituting `quantwaf.example.com` according to your DNS and HTTPS configuration.",809
GraphQL with Hasura and Supabase,Distributed GraphQL with Hasura and Supabase,"Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Hasura container to multiple datacenters. We will configure it to use Supabase as the Postgres database backend.

The Hasura container we will use is [available on DockerHub](https://hub.docker.com/r/hasura/graphql-engine).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",140
GraphQL with Hasura and Supabase,Prerequisites,* You need an account on [Supabase](https://supabase.com).,18
GraphQL with Hasura and Supabase,Create a Database on Supabase,"Create a new Postgres instance for Hasura to connect to. For this tutorial we will be using Supabase as they provide free, managed Postgres instances. But any Postgres database will work.
1. Visit https://app.supabase.com/ and click ""New project"".
2. Select a name, password, and region for your database. Make sure to save the password, as you will need it later.
3. Click ""Create new project"". Creating the project can take a while, so be patient.
4. Once the project is created, navigate to the ""Database"" tab on the left.

You have just created an empty database on Supabase.  Hasura will populate it with metadata upon first connection.",149
GraphQL with Hasura and Supabase,Get Your Connection String,"Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-SUPABASE-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@db.abcxyzabcxyzabcxyzabcxyz.supabase.co:5432/postgres`.

1. Go to Settings, and then Database.
1. Scroll down to the ""Connection string"" section and copy the connection string from the ""URI"" tab.  (Do not use the connection string in ""Connection pooling"", as Hasura is doing connection pooling of its own.)
1. Insert the password you saved earlier into the string at `[YOUR-PASSWORD]`.",146
GraphQL with Hasura and Supabase,Create a Deployment for Hasura,"Next, create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Substitute `YOUR_CONNECTION_STRING` accordingly. And supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: YOUR_CONNECTION_STRING
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",580
GraphQL with Hasura and Supabase,Experiment with Hasura,"Now, you can start using Hasura. While the main purpose of the Postgres database is to store Hasura metadata, note that you can use the Data section of the Hasura console to create tables of your own in the same Postgress database, and then use Hasura's querying ability to access that data.",64
GraphQL with Hasura and Supabase,"Learn to deploy a multi-datacenter, multi-provider, Hasura service","
# Distributed GraphQL with Hasura and Supabase

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Hasura container to multiple datacenters. We will configure it to use Supabase as the Postgres database backend.

The Hasura container we will use is [available on DockerHub](https://hub.docker.com/r/hasura/graphql-engine).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need an account on [Supabase](https://supabase.com).

## Create a Database on Supabase
Create a new Postgres instance for Hasura to connect to. For this tutorial we will be using Supabase as they provide free, managed Postgres instances. But any Postgres database will work.
1. Visit https://app.supabase.com/ and click ""New project"".
2. Select a name, password, and region for your database. Make sure to save the password, as you will need it later.
3. Click ""Create new project"". Creating the project can take a while, so be patient.
4. Once the project is created, navigate to the ""Database"" tab on the left.

You have just created an empty database on Supabase.  Hasura will populate it with metadata upon first connection.

## Get Your Connection String
Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-SUPABASE-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@db.abcxyzabcxyzabcxyzabcxyz.supabase.co:5432/postgres`.

1. Go to Settings, and then Database.
1. Scroll down to the ""Connection string"" section and copy the connection string from the ""URI"" tab.  (Do not use the connection string in ""Connection pooling"", as Hasura is doing connection pooling of its own.)
1. Insert the password you saved earlier into the string at `[YOUR-PASSWORD]`.


## Create a Deployment for Hasura
Next, create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Substitute `YOUR_CONNECTION_STRING` accordingly. And supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: YOUR_CONNECTION_STRING
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

## Experiment with Hasura
Now, you can start using Hasura. While the main purpose of the Postgres database is to store Hasura metadata, note that you can use the Data section of the Hasura console to create tables of your own in the same Postgress database, and then use Hasura's querying ability to access that data.",1143
GraphQL with Hasura and AWS RDS Aurora Postgres,Distributed GraphQL with Hasura and AWS RDS Aurora Postgres,"Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Hasura container to multiple datacenters. We will configure it to use AWS RDS Aurora Postgres as the Postgres database backend.

The Hasura container we will use is [available on DockerHub](https://hub.docker.com/r/hasura/graphql-engine).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",143
GraphQL with Hasura and AWS RDS Aurora Postgres,Prerequisites,* You need an account on [AWS](https://aws.amazon.com).,15
GraphQL with Hasura and AWS RDS Aurora Postgres,Create a New AWS RDS Aurora Postgress Database,Create a new Postgres instance for Hasura to connect to. For this tutorial we will be using AWS RDS Auroroa Postgres. But any Postgres database will work. Following is guidance on how to complete the various sections of the AWS creation process.,53
GraphQL with Hasura and AWS RDS Aurora Postgres,Create database,"1. In your AWS console, search for RDS.
1. Choose ""Create database"".
1. Choose ""Standard create"".",26
GraphQL with Hasura and AWS RDS Aurora Postgres,Engine options,"1. Engine type is ""Amazon Aurora"".
1. Edition is ""Amazon Aurora PostgresSQL-Compatible Edition"".",22
GraphQL with Hasura and AWS RDS Aurora Postgres,Templates,"1. Use the ""Dev/Test"" Template.",10
GraphQL with Hasura and AWS RDS Aurora Postgres,Settings,"1. Accept the DB cluster identifier of ""database-1"", or whatever it suggests.
1. Login ID for the master username should remain as ""postgres"".
1. Choose a strong password.",39
GraphQL with Hasura and AWS RDS Aurora Postgres,Instance configuration,"1. DB instance class: Serverless.
1. Capacity range: leave the defaults.",18
GraphQL with Hasura and AWS RDS Aurora Postgres,Availability & durability,"1. Multi-AZ deployment ""Don't create an Aurora Replica"".",14
GraphQL with Hasura and AWS RDS Aurora Postgres,Connectivity,"1. Network type: IPv4.
1. VPC: Default.
1. Subject group: default.
1. Public access: Yes.
1. VPC security group: ""Create new"".
1. New VPC security group name: ""hasura"" (or any name of your choosing).
1. Availability Zone: No Preference.
1. Database port (use the default) 5432.",82
GraphQL with Hasura and AWS RDS Aurora Postgres,Finishing Up,"Ignore the remaining sections of the AWS creation process.

Choose ""Create database"", and wait several minutes.",20
GraphQL with Hasura and AWS RDS Aurora Postgres,Expose Your Database to the Internet,"In general, Section deployments use IP addresses that change as the workload moves according to developer requirements. Because of this, it is not possible to allow/block database traffic by using IP address or range. Hence you should allow connections from all IP addresses. Typically this can be achieved by entering an IP address of (0.0.0.0).  While this might be a concern, relying solely on IP for security is generally ineffective and can lead to poor security practices. Be sure to use SSL when connecting to a production database.

When you created your database and made the choice to give public access, AWS gave access only to your own computer's IP address.  You will now need to adjust the VPC security group so that all IP addresses are allowed (0.0.0.0), as the Hasura container running in Section will be contacting the database from many different IP addresses.

1. Select the writer instance.
1. Select the Connectivity & security tab.
1. In the Security section, select the VPC security group, which we called ""hasura"" above.
1. In the Inbound rules tab, you will have one IP address allowed.
1. Choose ""Edit inbound rules""
1. Remove the single IP address and add a new one of ""0.0.0.0/0"" (Anywhere-IPv4).
1. Choose ""Save rules"".
1. Using a different computer (different IP address), test access by telneting to the endpoint, `telnet YOUR-ENDPOINT 5432`.  If it connects, then you're good.",321
GraphQL with Hasura and AWS RDS Aurora Postgres,Get Your Connection String,"Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-CLUSTER-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@database-1.cluster-ckremhbvmdf0.us-east-1.rds.amazonaws.com:5432/postgres`

1. Choose your cluster DB identifier, likely ""database-1"".
1. In the Connectivity & security tab, copy a writer instance endpoint name. Save this string for the Hasura deployment, which is next.

You have just created an empty Postgres database on AWS RDS Aurora.  Hasura will populate it with metadata upon first connection.",146
GraphQL with Hasura and AWS RDS Aurora Postgres,Create a Deployment for Hasura,"Next, create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Substitute `YOUR_CONNECTION_STRING` accordingly. And supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: YOUR_CONNECTION_STRING
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```
Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",580
GraphQL with Hasura and AWS RDS Aurora Postgres,Experiment with Hasura,"Now, you can start using Hasura.  While the main purpose of the Postgres database is to store Hasura metadata, note that you can use the Data section of the Hasura console to create tables of your own in the same Postgress database, and then use Hasura's querying ability to access that data.",65
GraphQL with Hasura and AWS RDS Aurora Postgres,"Learn to deploy a multi-datacenter, multi-provider, Hasura service","
# Distributed GraphQL with Hasura and AWS RDS Aurora Postgres

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Hasura container to multiple datacenters. We will configure it to use AWS RDS Aurora Postgres as the Postgres database backend.

The Hasura container we will use is [available on DockerHub](https://hub.docker.com/r/hasura/graphql-engine).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need an account on [AWS](https://aws.amazon.com).

## Create a New AWS RDS Aurora Postgress Database
Create a new Postgres instance for Hasura to connect to. For this tutorial we will be using AWS RDS Auroroa Postgres. But any Postgres database will work. Following is guidance on how to complete the various sections of the AWS creation process.
### Create database
1. In your AWS console, search for RDS.
1. Choose ""Create database"".
1. Choose ""Standard create"".
### Engine options
1. Engine type is ""Amazon Aurora"".
1. Edition is ""Amazon Aurora PostgresSQL-Compatible Edition"".
### Templates
1. Use the ""Dev/Test"" Template.
### Settings
1. Accept the DB cluster identifier of ""database-1"", or whatever it suggests.
1. Login ID for the master username should remain as ""postgres"".
1. Choose a strong password.
### Instance configuration
1. DB instance class: Serverless.
1. Capacity range: leave the defaults.
### Availability & durability
  1. Multi-AZ deployment ""Don't create an Aurora Replica"".
### Connectivity
1. Network type: IPv4.
1. VPC: Default.
1. Subject group: default.
1. Public access: Yes.
1. VPC security group: ""Create new"".
1. New VPC security group name: ""hasura"" (or any name of your choosing).
1. Availability Zone: No Preference.
1. Database port (use the default) 5432.

### Finishing Up
Ignore the remaining sections of the AWS creation process.

Choose ""Create database"", and wait several minutes.

## Expose Your Database to the Internet
In general, Section deployments use IP addresses that change as the workload moves according to developer requirements. Because of this, it is not possible to allow/block database traffic by using IP address or range. Hence you should allow connections from all IP addresses. Typically this can be achieved by entering an IP address of (0.0.0.0).  While this might be a concern, relying solely on IP for security is generally ineffective and can lead to poor security practices. Be sure to use SSL when connecting to a production database.

When you created your database and made the choice to give public access, AWS gave access only to your own computer's IP address.  You will now need to adjust the VPC security group so that all IP addresses are allowed (0.0.0.0), as the Hasura container running in Section will be contacting the database from many different IP addresses.

1. Select the writer instance.
1. Select the Connectivity & security tab.
1. In the Security section, select the VPC security group, which we called ""hasura"" above.
1. In the Inbound rules tab, you will have one IP address allowed.
1. Choose ""Edit inbound rules""
1. Remove the single IP address and add a new one of ""0.0.0.0/0"" (Anywhere-IPv4).
1. Choose ""Save rules"".
1. Using a different computer (different IP address), test access by telneting to the endpoint, `telnet YOUR-ENDPOINT 5432`.  If it connects, then you're good.

## Get Your Connection String
Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-CLUSTER-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@database-1.cluster-ckremhbvmdf0.us-east-1.rds.amazonaws.com:5432/postgres`

1. Choose your cluster DB identifier, likely ""database-1"".
1. In the Connectivity & security tab, copy a writer instance endpoint name. Save this string for the Hasura deployment, which is next.

You have just created an empty Postgres database on AWS RDS Aurora.  Hasura will populate it with metadata upon first connection.

## Create a Deployment for Hasura
Next, create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Substitute `YOUR_CONNECTION_STRING` accordingly. And supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: YOUR_CONNECTION_STRING
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```
Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

## Experiment with Hasura
Now, you can start using Hasura.  While the main purpose of the Postgres database is to store Hasura metadata, note that you can use the Data section of the Hasura console to create tables of your own in the same Postgress database, and then use Hasura's querying ability to access that data.",1647
GraphQL with Apollo Router,Distributed GraphQL with Apollo Router,"The [Apollo Router](https://www.apollographql.com/docs/router/) is a high-performance GraphQL router that runs a [federated supergraph](https://www.apollographql.com/docs/federation/), which is a single graph composed of multiple underlying services. By distributing the router container across Section's network around the world your apps will be more responsive thanks to faster responses to their API calls. This tutorial shows you how to distribute the Apollo Router on Section.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",159
GraphQL with Apollo Router,Prerequisites,* You need an Apollo Federation 2 project in Apollo Studio. Apollo's [Federation 2 quickstart](https://www.apollographql.com/docs/federation/quickstart/setup) can guide you if you don't have one already.,51
GraphQL with Apollo Router,Get Setup with Apollo,"If you already have an Apollo Federation 2 project, then skip to the next step. Otherwise set on up by following Apollo's [Federation 2 quickstart](https://www.apollographql.com/docs/federation/quickstart/setup). You'll be leveraging sample data provided by Apollo.",61
GraphQL with Apollo Router,Setup a ConfigMap for Router Configuration,"Create a `config.yaml` as defined below. This ConfigMap configures the router to listen on port 80. Additional examples of what can be configured are [here](https://www.apollographql.com/docs/router/containerization/kubernetes/#kubernetes-configuration).

```yaml title=""config.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: apollo
  labels:
    name: apollo
data:
  configuration.yaml: |
    server:
      listen: 0.0.0.0:80
```

Apply this ConfigMap resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f configmap.yaml`.

## Create a Kubernetes Deployment for Apollo Router
Next, create a Section deployment for Apollo Router with a file `apollo-router-deployment.yaml`, substituting `YOUR_APOLLO_KEY` and `YOUR_APOLLO_GRAPH_REF` accordingly. `YOUR_APOLLO_KEY` is your personal API KEY for [Apollo Studio](https://studio.apollographql.com).  And `YOUR_APOLLO_GRAPH_REF` is a string along the lines of `My-Graph-lypak@current`, and is the reference for your federated graph.

This deployment direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""apollo-router-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apollo
  name: apollo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apollo
  template:
    metadata:
      labels:
        app: apollo
    spec:
      containers:
      - image: ghcr.io/apollographql/router:v1.0.0-alpha.3
        imagePullPolicy: Always
        name: apollo
        args:
          - --hot-reload
          - --config
          - /app/configuration.yaml
        env:
          - name: APOLLO_KEY
            value: YOUR_APOLLO_KEY
          - name: APOLLO_GRAPH_REF
            value: YOUR_APOLLO_GRAPH_REF
        volumeMounts:
          - name: router-configuration
            mountPath: /app/configuration.yaml
            subPath: configuration.yaml
            readOnly: true
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
      volumes:
        - name: router-configuration
          configMap:
            name: apollo
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f apollo-router-deployment.yaml`.",602
GraphQL with Apollo Router,Expose the Service on the Internet,"We want to expose the Apollo Server on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: apollo
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network with `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Apollo pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 4000.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

## Browse Your GraphQL API
Visit https://YOUR_ENVIRONMENT_HOSTNAME in your browser to play in the Apollo Sandbox. You may have multiple pods running in multiple locations, but your chosen hostname will route to the one that is physically closest.

Enter the following query:
```GraphQL
query Query {
  locations {
    name
  }
}
```
You'll see the following result:
![Apollo Router Response](/img/docs/apollo-router-response.png)",402
GraphQL with Apollo Router,"Learn to deploy a multi-datacenter, multi-provider, Apollo Router for fast GraphQL performance","
# Distributed GraphQL with Apollo Router

The [Apollo Router](https://www.apollographql.com/docs/router/) is a high-performance GraphQL router that runs a [federated supergraph](https://www.apollographql.com/docs/federation/), which is a single graph composed of multiple underlying services. By distributing the router container across Section's network around the world your apps will be more responsive thanks to faster responses to their API calls. This tutorial shows you how to distribute the Apollo Router on Section.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need an Apollo Federation 2 project in Apollo Studio. Apollo's [Federation 2 quickstart](https://www.apollographql.com/docs/federation/quickstart/setup) can guide you if you don't have one already.

## Get Setup with Apollo
If you already have an Apollo Federation 2 project, then skip to the next step. Otherwise set on up by following Apollo's [Federation 2 quickstart](https://www.apollographql.com/docs/federation/quickstart/setup). You'll be leveraging sample data provided by Apollo.

## Setup a ConfigMap for Router Configuration
Create a `config.yaml` as defined below. This ConfigMap configures the router to listen on port 80. Additional examples of what can be configured are [here](https://www.apollographql.com/docs/router/containerization/kubernetes/#kubernetes-configuration).

```yaml title=""config.yaml""
apiVersion: v1
kind: ConfigMap
metadata:
  name: apollo
  labels:
    name: apollo
data:
  configuration.yaml: |
    server:
      listen: 0.0.0.0:80
```

Apply this ConfigMap resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f configmap.yaml`.

## Create a Kubernetes Deployment for Apollo Router
Next, create a Section deployment for Apollo Router with a file `apollo-router-deployment.yaml`, substituting `YOUR_APOLLO_KEY` and `YOUR_APOLLO_GRAPH_REF` accordingly. `YOUR_APOLLO_KEY` is your personal API KEY for [Apollo Studio](https://studio.apollographql.com).  And `YOUR_APOLLO_GRAPH_REF` is a string along the lines of `My-Graph-lypak@current`, and is the reference for your federated graph.

This deployment direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""apollo-router-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apollo
  name: apollo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apollo
  template:
    metadata:
      labels:
        app: apollo
    spec:
      containers:
      - image: ghcr.io/apollographql/router:v1.0.0-alpha.3
        imagePullPolicy: Always
        name: apollo
        args:
          - --hot-reload
          - --config
          - /app/configuration.yaml
        env:
          - name: APOLLO_KEY
            value: YOUR_APOLLO_KEY
          - name: APOLLO_GRAPH_REF
            value: YOUR_APOLLO_GRAPH_REF
        volumeMounts:
          - name: router-configuration
            mountPath: /app/configuration.yaml
            subPath: configuration.yaml
            readOnly: true
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
      volumes:
        - name: router-configuration
          configMap:
            name: apollo
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f apollo-router-deployment.yaml`.

## Expose the Service on the Internet
We want to expose the Apollo Server on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: apollo
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network with `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Apollo pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 4000.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

## Browse Your GraphQL API
Visit https://YOUR_ENVIRONMENT_HOSTNAME in your browser to play in the Apollo Sandbox. You may have multiple pods running in multiple locations, but your chosen hostname will route to the one that is physically closest.

Enter the following query:
```GraphQL
query Query {
  locations {
    name
  }
}
```
You'll see the following result:
![Apollo Router Response](/img/docs/apollo-router-response.png)",1311
GraphQL with Hasura Quickstart,Distributed GraphQL Quickstart with Hasura,"This is a GraphQL quickstart. Your apps will run faster if the APIs they call are physically located close to your end users. You'll deploy the open-source Hasura [from DockerHub](https://hub.docker.com/r/hasura/graphql-engine), and to make things quick we'll leverage a pre-created data source that Section provides. (Our other GraphQL tutorials show you how to configure various backend databases of your own.)

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",146
GraphQL with Hasura Quickstart,Create a Deployment for Hasura,"Create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Note that we are using a connection string that points to a read-only Section owned/operated database on AWS. Be sure to supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: postgresql://read_only_user:8BiusLd6Z89kjVgS@database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com:5432/postgres
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)",607
GraphQL with Hasura Quickstart,Experiment with Hasura,"Now, you can start using Hasura.  The AWS RDS Aurora database contains a small amount of read-only ""pets"" data that you can use to experiment with Hasura's GraphQL querying ability.

![Pets query](/img/docs/pets.png)",52
GraphQL with Hasura Quickstart,"Learn to deploy a multi-datacenter, multi-provider, Hasura service","
# Distributed GraphQL Quickstart with Hasura

This is a GraphQL quickstart. Your apps will run faster if the APIs they call are physically located close to your end users. You'll deploy the open-source Hasura [from DockerHub](https://hub.docker.com/r/hasura/graphql-engine), and to make things quick we'll leverage a pre-created data source that Section provides. (Our other GraphQL tutorials show you how to configure various backend databases of your own.)

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## Create a Deployment for Hasura
Create the deployment for Hasura as `hasura-deployment.yaml`.  This will direct Section to run the Hasura open source container. Note that we are using a connection string that points to a read-only Section owned/operated database on AWS. Be sure to supply a Hasura console password in `YOUR_ADMIN_PASSWORD`.

```yaml title=""hasura-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: hasura
  name: hasura
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hasura
  template:
    metadata:
      labels:
        app: hasura
    spec:
      containers:
      - image: hasura/graphql-engine
        imagePullPolicy: Always
        name: hasura
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: HASURA_GRAPHQL_DATABASE_URL
          value: postgresql://read_only_user:8BiusLd6Z89kjVgS@database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com:5432/postgres
        - name: HASURA_GRAPHQL_ENABLE_CONSOLE
          value: ""true""
        - name: HASURA_GRAPHQL_ADMIN_SECRET
          value: YOUR_ADMIN_PASSWORD
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f hasura-deployment.yaml`.

## Expose the Hasura Console on the Internet

We want to expose the Hasura console on the Internet. Create `ingress-upstream.yaml` as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: hasura
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

![Hasura pods](/img/docs/hasura-pods.png)

## Experiment with Hasura
Now, you can start using Hasura.  The AWS RDS Aurora database contains a small amount of read-only ""pets"" data that you can use to experiment with Hasura's GraphQL querying ability.

![Pets query](/img/docs/pets.png)",829
GraphQL with Postgraphile and Supabase,Distributed GraphQL with Postgraphile and Supabase,"Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Postgraphile container to multiple datacenters. We will configure it to use Supabase as the Postgres database backend.

The Postgraphile container we will use is [available on DockerHub](https://hub.docker.com/r/graphile/postgraphile).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",142
GraphQL with Postgraphile and Supabase,Prerequisites,* You need an account on [Supabase](https://supabase.com).,18
GraphQL with Postgraphile and Supabase,Create a Database on Supabase,"Create a new Postgres instance for Postgraphile to connect to. For this tutorial we will be using Supabase as they provide free, managed Postgres instances. But any Postgres database will work.
1. Visit https://app.supabase.com/ and click ""New project"".
2. Select a name, password, and region for your database. Make sure to save the password, as you will need it later.
3. Click ""Create new project"". Creating the project can take a while, so be patient.
4. Once the project is created, navigate to the ""Database"" tab on the left.

You have just created an empty database on Supabase.  Postgraphile will populate it with metadata upon first connection.",151
GraphQL with Postgraphile and Supabase,Get Your Connection String,"Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-SUPABASE-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@db.abcxyzabcxyzabcxyzabcxyz.supabase.co:5432/postgres`.

1. Go to Settings, and then Database.
1. Scroll down to the ""Connection string"" section and copy the connection string from the ""URI"" tab.  (Do not use the connection string in ""Connection pooling"", as Postgraphile is doing connection pooling of its own.)
1. Insert the password you saved earlier into the string at `[YOUR-PASSWORD]`.",147
GraphQL with Postgraphile and Supabase,Create a Deployment for Postgraphile,"Next, create the deployment for Postgraphile as `postgraphile-deployment.yaml`.  This will direct Section to run the Postgraphile open source container. Substitute `YOUR_CONNECTION_STRING` accordingly.

```yaml title=""postgraphile-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: postgraphile
  name: postgraphile
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgraphile
  template:
    metadata:
      labels:
        app: postgraphile
    spec:
      containers:
      - image: graphile/postgraphile:latest
        imagePullPolicy: Always
        name: postgraphile
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: DATABASE_URL
          value: YOUR_CONNECTION_STRING
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f postgraphile-deployment.yaml`.

## Expose the Postgraphile Console on the Internet

We want to expose the Postgraphile console on the Internet. Create `ingress-upstream.yaml` as defined below.  Note that it directs port 80 traffic to port 5000 on the Postgraphile container.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 5000
  selector:
    app: postgraphile
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",550
GraphQL with Postgraphile and Supabase,Experiment with Postgraphile,"Now, you can start using the GraphiQL interface of Postgraphile by visiting the `https://YOUR.DOMAIN.COM/graphiql`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",45
GraphQL with Postgraphile and Supabase,"Learn to deploy a multi-datacenter, multi-provider, Postgraphile service","
# Distributed GraphQL with Postgraphile and Supabase

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will use Section to deploy the open-source Postgraphile container to multiple datacenters. We will configure it to use Supabase as the Postgres database backend.

The Postgraphile container we will use is [available on DockerHub](https://hub.docker.com/r/graphile/postgraphile).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need an account on [Supabase](https://supabase.com).

## Create a Database on Supabase
Create a new Postgres instance for Postgraphile to connect to. For this tutorial we will be using Supabase as they provide free, managed Postgres instances. But any Postgres database will work.
1. Visit https://app.supabase.com/ and click ""New project"".
2. Select a name, password, and region for your database. Make sure to save the password, as you will need it later.
3. Click ""Create new project"". Creating the project can take a while, so be patient.
4. Once the project is created, navigate to the ""Database"" tab on the left.

You have just created an empty database on Supabase.  Postgraphile will populate it with metadata upon first connection.

## Get Your Connection String
Your connection string will be of the form `postgresql://postgres:[YOUR-PASSWORD]@[YOUR-SUPABASE-ENDPOINT]:5432/postgres`. An example (with mock credentials) would look like, `postgresql://postgres:abc1234@db.abcxyzabcxyzabcxyzabcxyz.supabase.co:5432/postgres`.

1. Go to Settings, and then Database.
1. Scroll down to the ""Connection string"" section and copy the connection string from the ""URI"" tab.  (Do not use the connection string in ""Connection pooling"", as Postgraphile is doing connection pooling of its own.)
1. Insert the password you saved earlier into the string at `[YOUR-PASSWORD]`.


## Create a Deployment for Postgraphile
Next, create the deployment for Postgraphile as `postgraphile-deployment.yaml`.  This will direct Section to run the Postgraphile open source container. Substitute `YOUR_CONNECTION_STRING` accordingly.

```yaml title=""postgraphile-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: postgraphile
  name: postgraphile
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgraphile
  template:
    metadata:
      labels:
        app: postgraphile
    spec:
      containers:
      - image: graphile/postgraphile:latest
        imagePullPolicy: Always
        name: postgraphile
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: DATABASE_URL
          value: YOUR_CONNECTION_STRING
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f postgraphile-deployment.yaml`.

## Expose the Postgraphile Console on the Internet

We want to expose the Postgraphile console on the Internet. Create `ingress-upstream.yaml` as defined below.  Note that it directs port 80 traffic to port 5000 on the Postgraphile container.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 5000
  selector:
    app: postgraphile
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

## Experiment with Postgraphile
Now, you can start using the GraphiQL interface of Postgraphile by visiting the `https://YOUR.DOMAIN.COM/graphiql`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1102
GraphQL with Apollo Server,Distributed GraphQL with Apollo Server,"Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will show you how to distribute an Apollo Server in multiple locations around the world by using the Section.

You'll first build a small Apollo Server container, push it to Docker Hub, and then deploy it to Section. This tutorial was inspired by an [example](https://www.preciouschicken.com/blog/posts/apollo-server-docker-container/) from PreciousChicken.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",157
GraphQL with Apollo Server,Prerequisites,"* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.",57
GraphQL with Apollo Server,Get Setup with Apollo,"Install the relevant packages.

```bash
mkdir apollo
cd apollo
npm init -y
npm install apollo-server graphql
```

## Create Your Container Image
The container image is Apollo code plus a small amount of static data, built upon node. For a real Apollo Server you'd have it pointing to a database or REST API. But static data is all we want for the minute. Let's start by saving the following code into `index.js`.

```javascript title=""index.js""
const { ApolloServer, gql } = require('apollo-server');

const data = {
  ""beasts"": [
    {
	    ""id"": ""md"",
	    ""legs"": 6,
	    ""binomial"": ""Musca domestica"",
	    ""commonName"": ""housefly"",
    },
    {
	    ""id"": ""nr"",
	    ""legs"": 8,
	    ""binomial"": ""Neriene radiata"",
	    ""commonName"": ""filmy dome spider"",
    },
    {
	    ""id"": ""cc"",
	    ""legs"": 2,
	    ""binomial"": ""Corvus corone"",
	    ""commonName"": ""carrion crow"",
    },
    {
	    ""id"": ""fc"",
	    ""legs"": 4,
	    ""binomial"": ""Felis catus"",
	    ""commonName"": ""cat"",
    }
  ]
};

const typeDefs = gql`
	type Beast {
		id: ID
		legs: Int
		binomial: String
		commonName: String
	}

	type Query {
		beasts: [Beast]
	}
`;

const resolvers = {
	Query: {
		// Returns array of all beasts.
		beasts: () => data.beasts
	}
};

const server = new ApolloServer({ typeDefs, resolvers });

// The `listen` method launches a web server.
server.listen(4000).then(({ url }) => {
    console.log(`🚀  Server ready at ${url}`);
});
```

Now let's define the Dockerfile.

```dockerfile title=""Dockerfile""",434
GraphQL with Apollo Server,Uses the node base image with the latest LTS version,FROM node:14.16.0,8
GraphQL with Apollo Server,specified network ports at runtime,EXPOSE 4000,5
GraphQL with Apollo Server,directory to a new app directory on the container,COPY index.js package.json package-lock.json  app/,11
GraphQL with Apollo Server,Changes working directory to the new directory just created,WORKDIR /app,4
GraphQL with Apollo Server,Installs npm dependencies on container,RUN npm ci,3
GraphQL with Apollo Server,Command container will actually run when called,"CMD [""node"", ""index.js""]
```

## Build and Publish the Image
Build the Docker image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.

```bash
docker build -t my-apollo-server-image .
docker tag my-apollo-server-image YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
docker push YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
```",92
GraphQL with Apollo Server,Create a Kubernetes Deployment for Apollo Server,"Next, create a Section deployment for Apollo Server with a file `apollo-deployment.yaml`, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""apollo-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apollo
  name: apollo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apollo
  template:
    metadata:
      labels:
        app: apollo
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
        imagePullPolicy: Always
        name: apollo
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f apollo-deployment.yaml`.

## Expose the Service on the Internet
We want to expose the Apollo Server on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 4000
  selector:
    app: apollo
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network with `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Apollo pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 4000.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",559
GraphQL with Apollo Server,Browse Your GraphQL API,"Visit https://YOUR_ENVIRONMENT_HOSTNAME in your browser to play in the Apollo Sandbox. You may have multiple pods running in multiple locations, but your chosen hostname will route to the one that is physically closest.

Enter the following query:
```GraphQL
{
  beasts {
    commonName
    legs
  }
}
```
You'll see the following result:
![Apollo Sandbox Beasts](/img/docs/apollo-sandbox-beasts.png)",90
GraphQL with Apollo Server,"Learn to deploy a multi-datacenter, multi-provider, Apollo Server for fast GraphQL performance","
# Distributed GraphQL with Apollo Server

Your apps will run faster if the APIs they call are physically located close to your end users. This tutorial will show you how to distribute an Apollo Server in multiple locations around the world by using the Section.

You'll first build a small Apollo Server container, push it to Docker Hub, and then deploy it to Section. This tutorial was inspired by an [example](https://www.preciouschicken.com/blog/posts/apollo-server-docker-container/) from PreciousChicken.

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.

## Get Setup with Apollo
Install the relevant packages.

```bash
mkdir apollo
cd apollo
npm init -y
npm install apollo-server graphql
```

## Create Your Container Image
The container image is Apollo code plus a small amount of static data, built upon node. For a real Apollo Server you'd have it pointing to a database or REST API. But static data is all we want for the minute. Let's start by saving the following code into `index.js`.

```javascript title=""index.js""
const { ApolloServer, gql } = require('apollo-server');

const data = {
  ""beasts"": [
    {
	    ""id"": ""md"",
	    ""legs"": 6,
	    ""binomial"": ""Musca domestica"",
	    ""commonName"": ""housefly"",
    },
    {
	    ""id"": ""nr"",
	    ""legs"": 8,
	    ""binomial"": ""Neriene radiata"",
	    ""commonName"": ""filmy dome spider"",
    },
    {
	    ""id"": ""cc"",
	    ""legs"": 2,
	    ""binomial"": ""Corvus corone"",
	    ""commonName"": ""carrion crow"",
    },
    {
	    ""id"": ""fc"",
	    ""legs"": 4,
	    ""binomial"": ""Felis catus"",
	    ""commonName"": ""cat"",
    }
  ]
};

const typeDefs = gql`
	type Beast {
		id: ID
		legs: Int
		binomial: String
		commonName: String
	}

	type Query {
		beasts: [Beast]
	}
`;

const resolvers = {
	Query: {
		// Returns array of all beasts.
		beasts: () => data.beasts
	}
};

const server = new ApolloServer({ typeDefs, resolvers });

// The `listen` method launches a web server.
server.listen(4000).then(({ url }) => {
    console.log(`🚀  Server ready at ${url}`);
});
```

Now let's define the Dockerfile.

```dockerfile title=""Dockerfile""
# Uses the node base image with the latest LTS version
FROM node:14.16.0
# Informs Docker that the container listens on the
# specified network ports at runtime
EXPOSE 4000
# Copies index.js and the two package files from the local
# directory to a new app directory on the container
COPY index.js package.json package-lock.json  app/
# Changes working directory to the new directory just created
WORKDIR /app
# Installs npm dependencies on container
RUN npm ci
# Command container will actually run when called
CMD [""node"", ""index.js""]
```

## Build and Publish the Image
Build the Docker image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.

```bash
docker build -t my-apollo-server-image .
docker tag my-apollo-server-image YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
docker push YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
```

## Create a Kubernetes Deployment for Apollo Server
Next, create a Section deployment for Apollo Server with a file `apollo-deployment.yaml`, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""apollo-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apollo
  name: apollo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: apollo
  template:
    metadata:
      labels:
        app: apollo
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/my-apollo-server:latest
        imagePullPolicy: Always
        name: apollo
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f apollo-deployment.yaml`.

## Expose the Service on the Internet
We want to expose the Apollo Server on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 4000
  selector:
    app: apollo
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network with `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your GraphQL API is running according to the default [AEE location optimization](/explanations/aee) strategy. Your GraphQL API will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Apollo pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 4000.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

## Browse Your GraphQL API
Visit https://YOUR_ENVIRONMENT_HOSTNAME in your browser to play in the Apollo Sandbox. You may have multiple pods running in multiple locations, but your chosen hostname will route to the one that is physically closest.

Enter the following query:
```GraphQL
{
  beasts {
    commonName
    legs
  }
}
```
You'll see the following result:
![Apollo Sandbox Beasts](/img/docs/apollo-sandbox-beasts.png)",1540
Go,Go App on Section,"Learn how to run a ""Hello World"" <a href=""https://go.dev/"">Go app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",83
Go,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://aged-rain-2087.section.app/"">https://aged-rain-2087.section.app/</a></strong> to see what you'll be building.</p>",54
Go,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/go-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/go-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.py` and watch your changes go live.",206
Go,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a Go ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",33
Go,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.,26
Go,Create the Go App,"Create a new directory for your app.

```bash
mkdir my-go-app
cd my-go-app
```

Create `helloworld.go` with the following code.

```go title=""helloworld.go""
package main

import (
    ""fmt""
    ""net/http""
)

func main() {
    http.HandleFunc(""/"", HelloServer)
    http.ListenAndServe("":8080"", nil)
}

func HelloServer(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, ""Hello World from Go on Section!"")
}
```",107
Go,Dockerize It,"Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM golang:1.19
WORKDIR /usr/src/my-go-app
RUN go mod init my-go-app
RUN go mod download && go mod verify
COPY . .
RUN go build -v -o /usr/local/bin/my-go-app ./...
CMD [""my-go-app""]
EXPOSE 8080
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",248
Go,Deploy It,"Next, use the Create Project command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod` with port 8080.

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",208
Go,See What You've Built,"See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",37
Go,Learn to deploy a Go app at the edge,"
# Go App on Section
Learn how to run a ""Hello World"" <a href=""https://go.dev/"">Go app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://aged-rain-2087.section.app/"">https://aged-rain-2087.section.app/</a></strong> to see what you'll be building.</p>

## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/go-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/go-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.py` and watch your changes go live.

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a Go ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.

### Create the Go App
Create a new directory for your app.

```bash
mkdir my-go-app
cd my-go-app
```

Create `helloworld.go` with the following code.

```go title=""helloworld.go""
package main

import (
    ""fmt""
    ""net/http""
)

func main() {
    http.HandleFunc(""/"", HelloServer)
    http.ListenAndServe("":8080"", nil)
}

func HelloServer(w http.ResponseWriter, r *http.Request) {
    fmt.Fprintf(w, ""Hello World from Go on Section!"")
}
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM golang:1.19
WORKDIR /usr/src/my-go-app
RUN go mod init my-go-app
RUN go mod download && go mod verify
COPY . .
RUN go build -v -o /usr/local/bin/my-go-app ./...
CMD [""my-go-app""]
EXPOSE 8080
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the Create Project command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod` with port 8080.

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1061
Nim,Nim App on Section,"Learn how to run a ""Hello World"" <a href=""https://nim-lang.org/"">Nim app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",85
Nim,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://fragrant-smoke-2968.section.app/"">https://fragrant-smoke-2968.section.app/</a></strong> to see what you'll be building.</p>",56
Nim,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/nim-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/nim-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.nim` and watch your changes go live.",209
Nim,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a Nim ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",33
Nim,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.,26
Nim,Create the Nim App,"Create a new directory for your app.

```bash
mkdir my-nim-app
cd my-nim-app
```

Create `helloworld.nim` with the following code.

```nim title=""helloworld.nim""
import std/asynchttpserver
import std/asyncdispatch

proc main {.async.} =
  var server = newAsyncHttpServer()
  proc cb(req: Request) {.async.} =
    echo (req.reqMethod, req.url, req.headers)
    let headers = {""Content-type"": ""text/plain; charset=utf-8""}
    await req.respond(Http200, ""Hello World from Nim on Section!"", headers.newHttpHeaders())

  server.listen(Port(8080))
  let port = server.getPort
  echo ""Web server is now running on port "" & $port.uint16
  while true:
    if server.shouldAcceptRequest():
      await server.acceptRequest(cb)
    else:
      # too many concurrent connections, `maxFDs` exceeded
      # wait 500ms for FDs to be closed
      await sleepAsync(500)

waitFor main()
```",229
Nim,Dockerize It,"Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM nimlang/nim
COPY . ./my-nim-app
WORKDIR /my-nim-app
EXPOSE 8080
CMD [ ""nim"", ""c"", ""--run"", ""./helloworld.nim"" ]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",232
Nim,Deploy It,"Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`Web server is now running on port 8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",208
Nim,See What You've Built,"See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",37
Nim,Learn to deploy a Nim app at the edge,"
# Nim App on Section
Learn how to run a ""Hello World"" <a href=""https://nim-lang.org/"">Nim app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://fragrant-smoke-2968.section.app/"">https://fragrant-smoke-2968.section.app/</a></strong> to see what you'll be building.</p>

## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/nim-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/nim-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.nim` and watch your changes go live.

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a Nim ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.

### Create the Nim App
Create a new directory for your app.

```bash
mkdir my-nim-app
cd my-nim-app
```

Create `helloworld.nim` with the following code.

```nim title=""helloworld.nim""
import std/asynchttpserver
import std/asyncdispatch

proc main {.async.} =
  var server = newAsyncHttpServer()
  proc cb(req: Request) {.async.} =
    echo (req.reqMethod, req.url, req.headers)
    let headers = {""Content-type"": ""text/plain; charset=utf-8""}
    await req.respond(Http200, ""Hello World from Nim on Section!"", headers.newHttpHeaders())

  server.listen(Port(8080))
  let port = server.getPort
  echo ""Web server is now running on port "" & $port.uint16
  while true:
    if server.shouldAcceptRequest():
      await server.acceptRequest(cb)
    else:
      # too many concurrent connections, `maxFDs` exceeded
      # wait 500ms for FDs to be closed
      await sleepAsync(500)

waitFor main()
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM nimlang/nim
COPY . ./my-nim-app
WORKDIR /my-nim-app
EXPOSE 8080
CMD [ ""nim"", ""c"", ""--run"", ""./helloworld.nim"" ]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-nim-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`Web server is now running on port 8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1174
ASP.NET Core,ASP.NET Core on Section,"Learn how to run a <a href=""https://github.com/dotnet/dotnet-docker/tree/main/samples/aspnetapp"">sample</a> <a href=""https://learn.microsoft.com/en-us/aspnet/core/?view=aspnetcore-7.0"">ASP.NET Core app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",127
ASP.NET Core,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://spring-sea-8563.section.app/"">https://spring-sea-8563.section.app/</a></strong> to see what you'll be building.</p>",54
ASP.NET Core,Step by Step,"Following are step-by-step instructions to deploy a ASP.NET Core <a href=""https://github.com/dotnet/dotnet-docker/tree/main/samples/aspnetapp"">sample</a> application to the edge on Section. We'll Dockerize it, and deploy it on Section.",60
ASP.NET Core,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.,26
ASP.NET Core,Create the ASP.NET Core App,"Create a new directory for your app.

```bash
mkdir my-aspnetcore-app
cd my-aspnetcore-app
```

Download or clone the Microsoft ASP.NET Core <a href=""https://github.com/dotnet/dotnet-docker"">dotnet/dotnet-docker</a> repository from Github.

Then place the contents of the `aspnetapp` directory in your `my-aspnetcore-app` directory.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
# https://hub.docker.com/_/microsoft-dotnet
FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build
WORKDIR /source

# copy csproj and restore as distinct layers
COPY aspnetapp/*.csproj .
RUN dotnet restore --use-current-runtime  

# copy everything else and build app
COPY aspnetapp/. .
RUN dotnet publish -c Release -o /app --use-current-runtime --self-contained false --no-restore

# final stage/image
FROM mcr.microsoft.com/dotnet/aspnet:7.0
WORKDIR /app
COPY --from=build /app .
EXPOSE 80
ENTRYPOINT [""dotnet"", ""aspnetapp.dll""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8000:80 ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
curl http://localhost:8000
```",366
ASP.NET Core,Push It,"Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-aspnetcore-app:prod` with port 8000.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8000:
```bash
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://[::]:80
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Production
info: Microsoft.Hosting.Lifetime[0]
      Content root path: /app
```

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",349
ASP.NET Core,See What You've Built,"See the sample app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",34
ASP.NET Core,Learn to deploy a ASP.NET Core app at the edge,"
# ASP.NET Core on Section
Learn how to run a <a href=""https://github.com/dotnet/dotnet-docker/tree/main/samples/aspnetapp"">sample</a> <a href=""https://learn.microsoft.com/en-us/aspnet/core/?view=aspnetcore-7.0"">ASP.NET Core app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://spring-sea-8563.section.app/"">https://spring-sea-8563.section.app/</a></strong> to see what you'll be building.</p>

## Step by Step

Following are step-by-step instructions to deploy a ASP.NET Core <a href=""https://github.com/dotnet/dotnet-docker/tree/main/samples/aspnetapp"">sample</a> application to the edge on Section. We'll Dockerize it, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.

### Create the ASP.NET Core App
Create a new directory for your app.

```bash
mkdir my-aspnetcore-app
cd my-aspnetcore-app
```

Download or clone the Microsoft ASP.NET Core <a href=""https://github.com/dotnet/dotnet-docker"">dotnet/dotnet-docker</a> repository from Github.

Then place the contents of the `aspnetapp` directory in your `my-aspnetcore-app` directory.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
# https://hub.docker.com/_/microsoft-dotnet
FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build
WORKDIR /source

# copy csproj and restore as distinct layers
COPY aspnetapp/*.csproj .
RUN dotnet restore --use-current-runtime  

# copy everything else and build app
COPY aspnetapp/. .
RUN dotnet publish -c Release -o /app --use-current-runtime --self-contained false --no-restore

# final stage/image
FROM mcr.microsoft.com/dotnet/aspnet:7.0
WORKDIR /app
COPY --from=build /app .
EXPOSE 80
ENTRYPOINT [""dotnet"", ""aspnetapp.dll""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8000:80 ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
curl http://localhost:8000
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-aspcorenet-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-aspnetcore-app:prod` with port 8000.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8000:
```bash
info: Microsoft.Hosting.Lifetime[14]
      Now listening on: http://[::]:80
info: Microsoft.Hosting.Lifetime[0]
      Application started. Press Ctrl+C to shut down.
info: Microsoft.Hosting.Lifetime[0]
      Hosting environment: Production
info: Microsoft.Hosting.Lifetime[0]
      Content root path: /app
```

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the sample app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1059
Ruby on Rails,Ruby on Rails on Section,"Learn how to run a <a href=""https://rubyonrails.org/"">Ruby on Rails app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",83
Ruby on Rails,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://dry-dew-3573.section.app/"">https://dry-dew-3573.section.app/</a></strong> to see what you'll be building.</p>",54
Ruby on Rails,Step by Step,"Following are step-by-step instructions to deploy a Ruby on Rails application to the edge on Section. We'll Dockerize it, and deploy it on Section.",31
Ruby on Rails,Prerequisites,"* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image, and [Ruby](https://www.ruby-lang.org/en/documentation/installation/), [Rails](https://guides.rubyonrails.org/getting_started.html#creating-a-new-rails-project-installing-rails-installing-rails), and [SQLite](https://www.sqlite.org/download.html) installed for testing locally.",92
Ruby on Rails,Create the Ruby on Rails App,"Create a new directory for your app.

```bash
mkdir my-rubyonrails-app
cd my-rubyonrails-app
```

Initialize the Ruby on Rails application with the following commands:

```bash
rails new hello_world
cd hello_world
```

Run the Ruby on Rails application locally with the following command:

```bash",68
Ruby on Rails,On Windows,ruby bin/rails server,5
Ruby on Rails,On Linux/Mac,"bin/rails server
```

Now navigate to `http://localhost:3000` in your browser to see the Ruby on Rails app running.

:::note
After deploying on Section and using your own domain name(s) you will need to edit the `config.hosts` setting in `my_rubyonrails_app/hello_world/config/environments/development.rb` to be your domain name(s). For example, if your domain name is `example.com` (or including subdomains such as `www.example.com`), you would change it to the following line:
```ruby title=""my_rubyonrails_app/hello_world/config/environments/development.rb""
  # Allow hosts
  config.hosts << /[a-z0-9-.]+\.example\.com/
```
:::",164
Ruby on Rails,Dockerize It,"Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your `hello_world` directory with the following content.

```dockerfile title=""Dockerfile""",42
Ruby on Rails,Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby),"FROM ruby:latest

WORKDIR /hello_world
COPY . /hello_world",16
Ruby on Rails,Run bundle install to install the Ruby dependencies.,RUN bundle install,3
Ruby on Rails,Install Yarn.,"RUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -
RUN echo ""deb https://dl.yarnpkg.com/debian/ stable main"" | tee /etc/apt/sources.list.d/yarn.list
RUN apt-get update && apt-get install -y yarn",63
Ruby on Rails,Run yarn install to install JavaScript dependencies.,RUN yarn install --check-files,6
Ruby on Rails,Export port 3000,"EXPOSE 3000

CMD [""rails"", ""server"", ""-b"", ""0.0.0.0""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
curl http://localhost:3000
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",179
Ruby on Rails,Deploy It,"Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod` with port 3000.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000:
```cmd
=> Booting Puma
=> Rails 7.0.4 application starting in development
=> Run `bin/rails server --help` for more startup options
Puma starting in single mode...
* Puma version: 5.6.5 (ruby 3.1.3-p185) (""Birdie's Version"")
*  Min threads: 5
*  Max threads: 5
*  Environment: development
*          PID: 1
* Listening on http://0.0.0.0:3000
```

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the sample app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",358
Ruby on Rails,Learn to deploy a Ruby on Rails app at the edge,"
# Ruby on Rails on Section
Learn how to run a <a href=""https://rubyonrails.org/"">Ruby on Rails app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://dry-dew-3573.section.app/"">https://dry-dew-3573.section.app/</a></strong> to see what you'll be building.</p>

## Step by Step

Following are step-by-step instructions to deploy a Ruby on Rails application to the edge on Section. We'll Dockerize it, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image, and [Ruby](https://www.ruby-lang.org/en/documentation/installation/), [Rails](https://guides.rubyonrails.org/getting_started.html#creating-a-new-rails-project-installing-rails-installing-rails), and [SQLite](https://www.sqlite.org/download.html) installed for testing locally.

### Create the Ruby on Rails App
Create a new directory for your app.

```bash
mkdir my-rubyonrails-app
cd my-rubyonrails-app
```

Initialize the Ruby on Rails application with the following commands:

```bash
rails new hello_world
cd hello_world
```

Run the Ruby on Rails application locally with the following command:

```bash
# On Windows
ruby bin/rails server
# On Linux/Mac
bin/rails server
```

Now navigate to `http://localhost:3000` in your browser to see the Ruby on Rails app running.

:::note
After deploying on Section and using your own domain name(s) you will need to edit the `config.hosts` setting in `my_rubyonrails_app/hello_world/config/environments/development.rb` to be your domain name(s). For example, if your domain name is `example.com` (or including subdomains such as `www.example.com`), you would change it to the following line:
```ruby title=""my_rubyonrails_app/hello_world/config/environments/development.rb""
  # Allow hosts
  config.hosts << /[a-z0-9-.]+\.example\.com/
```
:::

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your `hello_world` directory with the following content.

```dockerfile title=""Dockerfile""
# Use the latest Ruby image from Docker Hub (https://hub.docker.com/_/ruby)
FROM ruby:latest

WORKDIR /hello_world
COPY . /hello_world

# Run bundle install to install the Ruby dependencies.
RUN bundle install

# Install Yarn.
RUN curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -
RUN echo ""deb https://dl.yarnpkg.com/debian/ stable main"" | tee /etc/apt/sources.list.d/yarn.list
RUN apt-get update && apt-get install -y yarn

# Run yarn install to install JavaScript dependencies.
RUN yarn install --check-files

# Export port 3000
EXPOSE 3000

CMD [""rails"", ""server"", ""-b"", ""0.0.0.0""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
curl http://localhost:3000
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-rubyonrails-app:prod` with port 3000.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000:
```cmd
=> Booting Puma
=> Rails 7.0.4 application starting in development
=> Run `bin/rails server --help` for more startup options
Puma starting in single mode...
* Puma version: 5.6.5 (ruby 3.1.3-p185) (""Birdie's Version"")
*  Min threads: 5
*  Max threads: 5
*  Environment: development
*          PID: 1
* Listening on http://0.0.0.0:3000
```

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the sample app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1270
Next.js,Next.js App on Section,"Learn how to run a default <a href=""https://nextjs.org/"">Next.js app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",82
Next.js,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://crimson-resonance-8094.section.app/"">https://crimson-resonance-8094.section.app/</a></strong> to see what you'll be building.</p>

---",59
Next.js,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/nextjs-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/nextjs-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `pages/index.tsx` and watch your changes go live.

---",211
Next.js,Prerequisites,"- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- (optional) An existing Next.js app
- (optional) kubectl",35
Next.js,Steps,"1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section",21
Next.js,Creating your Dockerfile,"We're assuming you are either using the Section tutorial example, you have your own Next.js app that compiles successfully, or a newly created Next.js app eg:
```sh
npx create-next-app@latest
```

Create a `Dockerfile` in the root folder of your app. It should sit alongside `package.json`, eg the following

```
.
├── pages
│   ├── index.tsx
│   └── ...
├── public
│   └── ...
├── styles
│   └── ...
├── Dockerfile
├── next.config.js
└── package.json
```

The [Dockerfile](https://github.com/section/nextjs-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as dependencies
WORKDIR /app
COPY package.json yarn.lock* package-lock.json* ./
RUN yarn install --frozen-lockfile

FROM node:alpine as builder
WORKDIR /app
COPY . .
COPY --from=dependencies /app/node_modules ./node_modules
RUN yarn build

FROM node:alpine as runner
WORKDIR /app
ENV NODE_ENV production
# If you are using a custom next.config.js file, uncomment this line.
# COPY --from=builder /app/next.config.js ./
COPY --from=builder /app/public ./public
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json

EXPOSE 3000
CMD [""yarn"", ""start""]

```

This script would create a clean build of your Next.js app, and host it on port 3000 when the container is run.",384
Next.js,Building the container image,"Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh",20
Next.js,Replace these example values,"USER=section
IMAGENAME=next
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```",180
Next.js,Deploy to Section,"Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before (eg: `ghcr.io/section/nextdemo:0.0.1`), and specify port 3000.

That's it! Navigate to your project URL and you'll see your Next.js page live, similar to our <a href=""https://crimson-resonance-8094.section.app/"">example</a>.

![Next.js demo splash page](/img/docs/nextjs-tutorial.png)",115
Next.js,Learn to deploy a Next.js app at the edge,"
# Next.js App on Section 
Learn how to run a default <a href=""https://nextjs.org/"">Next.js app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://crimson-resonance-8094.section.app/"">https://crimson-resonance-8094.section.app/</a></strong> to see what you'll be building.</p>

---
## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/nextjs-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/nextjs-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `pages/index.tsx` and watch your changes go live.

---
## Option 2 - Step by Step

### Prerequisites
- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- (optional) An existing Next.js app
- (optional) kubectl

### Steps
1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section


### Creating your Dockerfile
We're assuming you are either using the Section tutorial example, you have your own Next.js app that compiles successfully, or a newly created Next.js app eg:
```sh
npx create-next-app@latest
```

Create a `Dockerfile` in the root folder of your app. It should sit alongside `package.json`, eg the following

```
.
├── pages
│   ├── index.tsx
│   └── ...
├── public
│   └── ...
├── styles
│   └── ...
├── Dockerfile
├── next.config.js
└── package.json
```

The [Dockerfile](https://github.com/section/nextjs-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as dependencies
WORKDIR /app
COPY package.json yarn.lock* package-lock.json* ./
RUN yarn install --frozen-lockfile

FROM node:alpine as builder
WORKDIR /app
COPY . .
COPY --from=dependencies /app/node_modules ./node_modules
RUN yarn build

FROM node:alpine as runner
WORKDIR /app
ENV NODE_ENV production
# If you are using a custom next.config.js file, uncomment this line.
# COPY --from=builder /app/next.config.js ./
COPY --from=builder /app/public ./public
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/package.json ./package.json

EXPOSE 3000
CMD [""yarn"", ""start""]

```

This script would create a clean build of your Next.js app, and host it on port 3000 when the container is run.


### Building the container image

Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh
# Replace these example values
USER=section
IMAGENAME=next
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```


## Deploy to Section

Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before (eg: `ghcr.io/section/nextdemo:0.0.1`), and specify port 3000.

That's it! Navigate to your project URL and you'll see your Next.js page live, similar to our <a href=""https://crimson-resonance-8094.section.app/"">example</a>.

![Next.js demo splash page](/img/docs/nextjs-tutorial.png)",1174
Rust,Rust on Section,"Learn how to run a default <a href=""https://www.rust-lang.org"">Rust app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",84
Rust,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://white-flower-3215.section.app"">https://white-flower-3215.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",114
Rust,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/rust-tutorial/actions/workflows/workflows.yaml/badge.svg)

1. Make a new repo from our template: in your browser visit https://github.com/section/rust-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the message in `src/main.rs` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",243
Rust,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a Rust app to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",35
Rust,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) and [Rust](https://www.rust-lang.org/tools/install) installed so that you can build a Docker image.,41
Rust,Create the Rust App,"Create a Rust app via the Rust `cargo` command:

```bash
cargo new rust-tutorial
```

After the Rust app has been created, we need to set up the web server. We'll be using the [Actix Web](https://actix.rs) framework to handle this. Add this dependency to your Rust app in the `Cargo.toml` file:

```rust title=""Cargo.toml""
[package]
name = ""rust-tutorial""
version = ""0.1.0""
edition = ""2021""

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

// highlight-start
[dependencies]
actix-web = ""4""
// highlight-end
```

Next, update the `src/main.rs` file to the following:

```rust title=""src/main.rs""
use actix_web::{get, App, HttpResponse, HttpServer, Responder};",191
Rust,"[get(""/"")]","async fn hello() -> impl Responder {
    HttpResponse::Ok().body(""Hello World from Rust on Section!"")
}",24
Rust,[actix_web::main],"async fn main() -> std::io::Result<()> {
    HttpServer::new(|| {
        App::new()
            .service(hello)
    })
    .bind(""0.0.0.0:8080"")?
    .run()
    .await
}
```

Lastly, we'll want to build and run the Rust app by running the following:

```bash
cd rust-tutorial

cargo build --release

cargo run --release
```

Test it by running `curl http://localhost:8080` in your terminal or by visiting `http://localhost:8080` in your browser. You should get a ""Hello World from Rust on Section!"" message.",141
Rust,Dockerize It,"Let's build the container image that we'll deploy to Section. First, make a `Dockerfile` in your directory with the following content:

```dockerfile title=""Dockerfile""
FROM rust:latest as build

RUN cargo new --bin rust-tutorial
WORKDIR /rust-tutorial

COPY ./Cargo.lock ./Cargo.lock
COPY ./Cargo.toml ./Cargo.toml

RUN cargo build --release
RUN rm src/*.rs

COPY ./src ./src

RUN rm ./target/release/deps/rust_tutorial*
RUN cargo build --release

FROM debian:buster-slim

COPY --from=build /rust-tutorial/target/release/rust-tutorial .

CMD [""./rust-tutorial""]
```

Create a `.dockerignore` file from the `.gitignore` file:

```bash
cp .gitignore .dockerignore
```

Build and tag the Docker image:

```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`.",279
Rust,Deploy It,"Next, create a Section deployment for the Rust app with a `rust-deployment.yaml` file, substituting `YOUR_GITHUB_USERNAME` and the environment variables accordingly. This will direct Section to distribute the container you've pushed to GitHub Packages.

```yaml title=""rust-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rust
  labels:
    app: rust
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rust
  template:
    metadata:
      labels:
        app: rust
    spec:
      containers:
      - name: rust
        image: ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "".1""
            memory: "".1Gi""
          limits:
            cpu: "".1""
            memory: "".1Gi""
        ports:
        - containerPort: 8080
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f rust-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

### Expose It
Expose it on the internet, mapping the container's port `8080`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  name: ingress-upstream
  labels:
    app: ingress-upstream
spec:
  selector:
    app: rust
  ports:
    - name: 80-to-8080
      protocol: TCP
      port: 80
      targetPort: 8080
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",521
Rust,See What You've Built,"See the Rust app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",34
Rust,Learn to deploy a Rust app at the edge,"
# Rust on Section
Learn how to run a default <a href=""https://www.rust-lang.org"">Rust app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://white-flower-3215.section.app"">https://white-flower-3215.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/rust-tutorial/actions/workflows/workflows.yaml/badge.svg)

1. Make a new repo from our template: in your browser visit https://github.com/section/rust-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the message in `src/main.rs` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a Rust app to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) and [Rust](https://www.rust-lang.org/tools/install) installed so that you can build a Docker image.

### Create the Rust App
Create a Rust app via the Rust `cargo` command:

```bash
cargo new rust-tutorial
```

After the Rust app has been created, we need to set up the web server. We'll be using the [Actix Web](https://actix.rs) framework to handle this. Add this dependency to your Rust app in the `Cargo.toml` file:

```rust title=""Cargo.toml""
[package]
name = ""rust-tutorial""
version = ""0.1.0""
edition = ""2021""

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

// highlight-start
[dependencies]
actix-web = ""4""
// highlight-end
```

Next, update the `src/main.rs` file to the following:

```rust title=""src/main.rs""
use actix_web::{get, App, HttpResponse, HttpServer, Responder};

#[get(""/"")]
async fn hello() -> impl Responder {
    HttpResponse::Ok().body(""Hello World from Rust on Section!"")
}

#[actix_web::main]
async fn main() -> std::io::Result<()> {
    HttpServer::new(|| {
        App::new()
            .service(hello)
    })
    .bind(""0.0.0.0:8080"")?
    .run()
    .await
}
```

Lastly, we'll want to build and run the Rust app by running the following:

```bash
cd rust-tutorial

cargo build --release

cargo run --release
```

Test it by running `curl http://localhost:8080` in your terminal or by visiting `http://localhost:8080` in your browser. You should get a ""Hello World from Rust on Section!"" message.

### Dockerize It
Let's build the container image that we'll deploy to Section. First, make a `Dockerfile` in your directory with the following content:

```dockerfile title=""Dockerfile""
FROM rust:latest as build

RUN cargo new --bin rust-tutorial
WORKDIR /rust-tutorial

COPY ./Cargo.lock ./Cargo.lock
COPY ./Cargo.toml ./Cargo.toml

RUN cargo build --release
RUN rm src/*.rs

COPY ./src ./src

RUN rm ./target/release/deps/rust_tutorial*
RUN cargo build --release

FROM debian:buster-slim

COPY --from=build /rust-tutorial/target/release/rust-tutorial .

CMD [""./rust-tutorial""]
```

Create a `.dockerignore` file from the `.gitignore` file:

```bash
cp .gitignore .dockerignore
```

Build and tag the Docker image:

```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`.

### Deploy It
Next, create a Section deployment for the Rust app with a `rust-deployment.yaml` file, substituting `YOUR_GITHUB_USERNAME` and the environment variables accordingly. This will direct Section to distribute the container you've pushed to GitHub Packages.

```yaml title=""rust-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rust
  labels:
    app: rust
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rust
  template:
    metadata:
      labels:
        app: rust
    spec:
      containers:
      - name: rust
        image: ghcr.io/YOUR_GITHUB_USERNAME/rust-tutorial:main
        imagePullPolicy: Always
        resources:
          requests:
            cpu: "".1""
            memory: "".1Gi""
          limits:
            cpu: "".1""
            memory: "".1Gi""
        ports:
        - containerPort: 8080
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f rust-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

### Expose It
Expose it on the internet, mapping the container's port `8080`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  name: ingress-upstream
  labels:
    app: ingress-upstream
spec:
  selector:
    app: rust
  ports:
    - name: 80-to-8080
      protocol: TCP
      port: 80
      targetPort: 8080
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the Rust app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.
",1775
Deno,Deno on Section,"Learn how to run a ""Hello World"" <a href=""https://deno.land/"">Deno app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",85
Deno,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://old-glade-8722.section.app/"">https://old-glade-8722.section.app/</a></strong> to see what you'll be building.</p>",54
Deno,Step by Step,"Following are step-by-step instructions to deploy a Deno ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",40
Deno,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) and [Deno](https://deno.land/manual@v1.28.3/getting_started/installation) installed so that you can build a docker image.,50
Deno,Create the Deno App,"Create a new directory for your app.

```bash
mkdir my-deno-app
cd my-deno-app
```

Create `main.ts` with the following code.

```typescript title=""main.ts""
import { serve } from ""https://deno.land/std@0.167.0/http/server.ts"";
serve((_req) => new Response(""Hello World from Deno on Section!""), { port: 1993 });
```

Test it using `run --allow-net main.ts` and visit it using `curl http://localhost:1993`. You'll get the ""Hello World from Deno on Section!"" response.",129
Deno,Dockerize It,"Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM denoland/deno:latest",45
Deno,The port that your application listens to.,"EXPOSE 1993

WORKDIR /my-deno-app",13
Deno,Prefer not to run as root.,USER deno,3
Deno,These steps will be re-run upon each file change in your working directory:,ADD . .,3
Deno,Compile the main app so that it doesn't need to be compiled each startup/entry.,"RUN deno cache main.ts

CMD [""run"", ""--allow-net"", ""main.ts""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
```

Launch it locally to test it.

```bash
docker run -p 1993:1993 ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
curl http://localhost:1993
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",167
Deno,Deploy It,"Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 1993 (`Listening on http://localhost:1993/`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",207
Deno,See What You've Built,"See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",37
Deno,Learn to deploy a Deno app at the edge,"
# Deno on Section
Learn how to run a ""Hello World"" <a href=""https://deno.land/"">Deno app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://old-glade-8722.section.app/"">https://old-glade-8722.section.app/</a></strong> to see what you'll be building.</p>

## Step by Step

Following are step-by-step instructions to deploy a Deno ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) and [Deno](https://deno.land/manual@v1.28.3/getting_started/installation) installed so that you can build a docker image.

### Create the Deno App
Create a new directory for your app.

```bash
mkdir my-deno-app
cd my-deno-app
```

Create `main.ts` with the following code.

```typescript title=""main.ts""
import { serve } from ""https://deno.land/std@0.167.0/http/server.ts"";
serve((_req) => new Response(""Hello World from Deno on Section!""), { port: 1993 });
```

Test it using `run --allow-net main.ts` and visit it using `curl http://localhost:1993`. You'll get the ""Hello World from Deno on Section!"" response.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM denoland/deno:latest

# The port that your application listens to.
EXPOSE 1993

WORKDIR /my-deno-app

# Prefer not to run as root.
USER deno

# These steps will be re-run upon each file change in your working directory:
ADD . .
# Compile the main app so that it doesn't need to be compiled each startup/entry.
RUN deno cache main.ts

CMD [""run"", ""--allow-net"", ""main.ts""]
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
```

Launch it locally to test it.

```bash
docker run -p 1993:1993 ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
curl http://localhost:1993
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-deno-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 1993 (`Listening on http://localhost:1993/`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",933
Laravel,Laravel on Section,"Learn how to run a default <a href=""https://laravel.com/"">Laravel app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",82
Laravel,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://snowy-thunder-5268.section.app"">https://snowy-thunder-5268.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",116
Laravel,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/laravel-tutorial/actions/workflows/workflows.yaml/badge.svg)

1. Make a new repo from our template: in your browser visit https://github.com/section/laravel-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the HTML text in `resources/views/welcome.blade.php` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",247
Laravel,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a default Laravel app to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",36
Laravel,Prerequisites,"* You need [Docker](https://docs.docker.com/get-docker), PHP, and [Composer](https://getcomposer.org) installed so that you can build a Docker image.",38
Laravel,Create the Laravel App,"Create a Laravel app via the Composer `create-project` command:

```bash
composer create-project laravel/laravel laravel
```

After the Laravel app has been created, test it locally using the Laravel's Artisan CLI `serve` command:

```bash
cd laravel

php artisan serve
```

Access it by running `curl http://localhost:8000` in your terminal or by visiting `http://localhost:8000` in your browser. You should get the default Laravel welcome page.",104
Laravel,Dockerize It,"Let's build the container image that we'll deploy to Section. First, make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM lorisleiva/laravel-docker:8.1

COPY . .

RUN composer install

CMD php artisan serve --host=0.0.0.0
```

Create a `.dockerignore` file from the `.gitignore` file:

```bash
cp .gitignore .dockerignore
```

Build and tag the Docker image.

```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`.",203
Laravel,Deploy It,"Next, create a Section deployment for the Laravel app with a `laravel-deployment.yaml` file, substituting `YOUR_GITHUB_USERNAME` and the environment variables accordingly. This will direct Section to distribute the container you've pushed to GitHub Packages.

```yaml title=""laravel-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: laravel
  labels:
    app: laravel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: laravel
  template:
    metadata:
      labels:
        app: laravel
    spec:
      containers:
      - name: laravel
        image: ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
        imagePullPolicy: Always
        resources:
          requests:
            memory: ""200Mi""
            cpu: ""200m""
          limits:
            memory: ""200Mi""
            cpu: ""200m""
        ports:
        - containerPort: 80
        env:
        - name: APP_NAME
          value: ""Laravel""
        - name: APP_ENV
          value: ""production""
        - name: APP_KEY
          value: ""YOUR_APP_KEY""
        - name: APP_DEBUG
          value: ""false""
        - name: APP_URL
          value: ""https://YOUR.DOMAIN.COM""

        - name: LOG_CHANNEL
          value: ""stack""
        - name: LOG_DEPRECATIONS_CHANNEL
          value: ""null""
        - name: LOG_LEVEL
          value: ""debug""

        - name: BROADCAST_DRIVER
          value: ""log""
        - name: CACHE_DRIVER
          value: ""file""
        - name: FILESYSTEM_DISK
          value: ""local""
        - name: QUEUE_CONNECTION
          value: ""sync""
        - name: SESSION_DRIVER
          value: ""cookie""
        - name: SESSION_LIFETIME
          value: ""120""

        - name: MAIL_MAILER
          value: ""smtp""
        - name: MAIL_HOST
          value: ""mailhog""
        - name: MAIL_PORT
          value: ""1025""
        - name: MAIL_USERNAME
          value: ""null""
        - name: MAIL_PASSWORD
          value: ""null""
        - name: MAIL_ENCRYPTION
          value: ""null""
        - name: MAIL_FROM_ADDRESS
          value: ""hello@example.com""
        - name: MAIL_FROM_NAME
          value: ""Laravel""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f laravel-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::tip
For a production Laravel app, use [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables) as the values for private environment variables.
:::

### Expose It
Expose it on the internet, mapping the container's port `8000`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 8000
    selector:
        app: laravel
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",909
Laravel,See What You've Built,"See the Laravel app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",34
Laravel,Learn to deploy a Laravel app at the edge,"
# Laravel on Section
Learn how to run a default <a href=""https://laravel.com/"">Laravel app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://snowy-thunder-5268.section.app"">https://snowy-thunder-5268.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/laravel-tutorial/actions/workflows/workflows.yaml/badge.svg)

1. Make a new repo from our template: in your browser visit https://github.com/section/laravel-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the HTML text in `resources/views/welcome.blade.php` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a default Laravel app to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker), PHP, and [Composer](https://getcomposer.org) installed so that you can build a Docker image.

### Create the Laravel App
Create a Laravel app via the Composer `create-project` command:

```bash
composer create-project laravel/laravel laravel
```

After the Laravel app has been created, test it locally using the Laravel's Artisan CLI `serve` command:

```bash
cd laravel

php artisan serve
```

Access it by running `curl http://localhost:8000` in your terminal or by visiting `http://localhost:8000` in your browser. You should get the default Laravel welcome page.

### Dockerize It
Let's build the container image that we'll deploy to Section. First, make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM lorisleiva/laravel-docker:8.1

COPY . .

RUN composer install

CMD php artisan serve --host=0.0.0.0
```

Create a `.dockerignore` file from the `.gitignore` file:

```bash
cp .gitignore .dockerignore
```

Build and tag the Docker image.

```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`.

### Deploy It
Next, create a Section deployment for the Laravel app with a `laravel-deployment.yaml` file, substituting `YOUR_GITHUB_USERNAME` and the environment variables accordingly. This will direct Section to distribute the container you've pushed to GitHub Packages.

```yaml title=""laravel-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: laravel
  labels:
    app: laravel
spec:
  replicas: 1
  selector:
    matchLabels:
      app: laravel
  template:
    metadata:
      labels:
        app: laravel
    spec:
      containers:
      - name: laravel
        image: ghcr.io/YOUR_GITHUB_USERNAME/laravel:main
        imagePullPolicy: Always
        resources:
          requests:
            memory: ""200Mi""
            cpu: ""200m""
          limits:
            memory: ""200Mi""
            cpu: ""200m""
        ports:
        - containerPort: 80
        env:
        - name: APP_NAME
          value: ""Laravel""
        - name: APP_ENV
          value: ""production""
        - name: APP_KEY
          value: ""YOUR_APP_KEY""
        - name: APP_DEBUG
          value: ""false""
        - name: APP_URL
          value: ""https://YOUR.DOMAIN.COM""

        - name: LOG_CHANNEL
          value: ""stack""
        - name: LOG_DEPRECATIONS_CHANNEL
          value: ""null""
        - name: LOG_LEVEL
          value: ""debug""

        - name: BROADCAST_DRIVER
          value: ""log""
        - name: CACHE_DRIVER
          value: ""file""
        - name: FILESYSTEM_DISK
          value: ""local""
        - name: QUEUE_CONNECTION
          value: ""sync""
        - name: SESSION_DRIVER
          value: ""cookie""
        - name: SESSION_LIFETIME
          value: ""120""

        - name: MAIL_MAILER
          value: ""smtp""
        - name: MAIL_HOST
          value: ""mailhog""
        - name: MAIL_PORT
          value: ""1025""
        - name: MAIL_USERNAME
          value: ""null""
        - name: MAIL_PASSWORD
          value: ""null""
        - name: MAIL_ENCRYPTION
          value: ""null""
        - name: MAIL_FROM_ADDRESS
          value: ""hello@example.com""
        - name: MAIL_FROM_NAME
          value: ""Laravel""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f laravel-deployment.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

:::tip
For a production Laravel app, use [Kubernetes Secrets](https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables) as the values for private environment variables.
:::

### Expose It
Expose it on the internet, mapping the container's port `8000`.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
    labels:
        app: ingress-upstream
    name: ingress-upstream
spec:
    ports:
    - name: 80-8080
      port: 80
      protocol: TCP
      targetPort: 8000
    selector:
        app: laravel
    sessionAffinity: None
    type: ClusterIP
status:
    loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [`kubectl apply -f ingress-upstream.yaml`](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The `-o wide` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the Laravel app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.
",1826
Node Express,Node Express on Section,"Learn how to run a ""Hello World"" <a href=""https://expressjs.com/"">Node Express app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",85
Node Express,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://floral-smoke-4415.section.app"">https://floral-smoke-4415.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",116
Node Express,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/node-express/actions/workflows/workflows.yaml/badge.svg)


1. Make a new repo from our template: in your browser visit https://github.com/section/node-express and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the HTML text in `views/pages/index.ejs` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",244
Node Express,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a Node.js Express ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",41
Node Express,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.,40
Node Express,Create the Node.js App,"Create a new directory for your app.

```bash
mkdir node-express
cd node-express
```

Initialize the `package.json` file, which handles metadata for Node.js. Answer `yes` in response.

```bash
npm init
... 
Is this OK? (yes)
```

Now install Express with `npm install express --save`.

And then we add our application code. Create `app.js` with the following code.

```javascript title=""app.js""
const express = require('express')
const app = express()
const port = process.env.PORT || 3000

app.get('/', (req, res) => {
  res.send( ""<h1>Hello World from Node Express on Section!</h1>"" );
})

app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`)
})
```

Test it using `node app.js` and visit it using `curl http://localhost:3000`. You'll get the ""Hello World from Node Express on Section!"" response.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM node:lts as runner
WORKDIR /node-express
ENV NODE_ENV production
ARG COMMIT_ID
ENV COMMIT_ID=${COMMIT_ID}
COPY . .
RUN npm ci --only=production
EXPOSE 3000
CMD [""node"", ""app.js""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
curl http://localhost:3000
```",383
Node Express,Push It,"Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](/get-started/create-project) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod` with port 80.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Node Express pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000 (`App listening at http://localhost:3000`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",328
Node Express,Learn to deploy a Node Express app at the edge,"
# Node Express on Section
Learn how to run a ""Hello World"" <a href=""https://expressjs.com/"">Node Express app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://floral-smoke-4415.section.app"">https://floral-smoke-4415.section.app</a></strong> to see what you'll be building.</p>

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/node-express/actions/workflows/workflows.yaml/badge.svg)


1. Make a new repo from our template: in your browser visit https://github.com/section/node-express and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the HTML text in `views/pages/index.ejs` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions). 

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a Node.js Express ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.

### Create the Node.js App
Create a new directory for your app.

```bash
mkdir node-express
cd node-express
```

Initialize the `package.json` file, which handles metadata for Node.js. Answer `yes` in response.

```bash
npm init
... 
Is this OK? (yes)
```

Now install Express with `npm install express --save`.

And then we add our application code. Create `app.js` with the following code.

```javascript title=""app.js""
const express = require('express')
const app = express()
const port = process.env.PORT || 3000

app.get('/', (req, res) => {
  res.send( ""<h1>Hello World from Node Express on Section!</h1>"" );
})

app.listen(port, () => {
  console.log(`App listening at http://localhost:${port}`)
})
```

Test it using `node app.js` and visit it using `curl http://localhost:3000`. You'll get the ""Hello World from Node Express on Section!"" response.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM node:lts as runner
WORKDIR /node-express
ENV NODE_ENV production
ARG COMMIT_ID
ENV COMMIT_ID=${COMMIT_ID}
COPY . .
RUN npm ci --only=production
EXPOSE 3000
CMD [""node"", ""app.js""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
curl http://localhost:3000
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](/get-started/create-project) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/node-express:prod` with port 80.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

![Node Express pods](/img/docs/apollo-pods.png)

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000 (`App listening at http://localhost:3000`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1286
Python Django,Python Django App on Section,"Learn how to run a <a href=""https://www.djangoproject.com/"">Python Django</a> app at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",81
Python Django,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://muddy-butterfly-8163.section.app/"">https://muddy-butterfly-8163.section.app/</a></strong> to see what you'll be building.</p>",58
Python Django,Option 1 - Copy Our GitHub Repo,"[![workflow status](https://github.com/section/python-django-tutorial/actions/workflows/workflows.yaml/badge.svg)](https://github.com/section/python-django-tutorial/actions)

Make a new repo from our template: in your browser visit https://github.com/section/python-django-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to the files within `./my_django_app/` and watch your changes go live.",230
Python Django,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a Python Django application to the edge on Section. We'll Dockerize it, and deploy it on Section.",30
Python Django,Prerequisites,"* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image, [Python](https://www.python.org/) installed so you can test it locally (which comes with `pip` the Python package manager since Python 3.4).",60
Python Django,Create the Python Django App,"Create a new directory for your app.

```bash
mkdir my-django-app
cd my-django-app
```

Initialize the Django application with the following commands:

```bash
pip install django
django-admin startproject my_django_app
```

Run the Django application locally with the following commands:

```bash
python manage.py runserver 8080
```

Now navigate to `http://localhost:8080` in your browser to see the Django app running.

:::note
After deploying on Section and using your own domain name(s) you will need to edit the `ALLOWED_HOSTS` setting in `my_django_app/settings.py` to include your domain name(s). For example, if your domain name is `example.com`, you would add the following line to `my_django_app/settings.py`:
```python
ALLOWED_HOSTS = ['example.com']
```
:::",186
Python Django,Dockerize It,"Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM python:3.7-alpine
EXPOSE 8080
WORKDIR /my-django-app 
RUN pip install django
COPY . /my-django-app
ENTRYPOINT [""python3""] 
CMD [""manage.py"", ""runserver"", ""0.0.0.0:8080""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`",250
Python Django,Deploy It,"Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).",173
Python Django,See What You've Built,"See the Python Django app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",35
Python Django,Learn to deploy a Python Django app at the edge,"
# Python Django App on Section
Learn how to run a <a href=""https://www.djangoproject.com/"">Python Django</a> app at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://muddy-butterfly-8163.section.app/"">https://muddy-butterfly-8163.section.app/</a></strong> to see what you'll be building.</p>

## Option 1 - Copy Our GitHub Repo
[![workflow status](https://github.com/section/python-django-tutorial/actions/workflows/workflows.yaml/badge.svg)](https://github.com/section/python-django-tutorial/actions)

Make a new repo from our template: in your browser visit https://github.com/section/python-django-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to the files within `./my_django_app/` and watch your changes go live.

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a Python Django application to the edge on Section. We'll Dockerize it, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image, [Python](https://www.python.org/) installed so you can test it locally (which comes with `pip` the Python package manager since Python 3.4).

### Create the Python Django App
Create a new directory for your app.

```bash
mkdir my-django-app
cd my-django-app
```

Initialize the Django application with the following commands:

```bash
pip install django
django-admin startproject my_django_app
```

Run the Django application locally with the following commands:

```bash
python manage.py runserver 8080
```

Now navigate to `http://localhost:8080` in your browser to see the Django app running.

:::note
After deploying on Section and using your own domain name(s) you will need to edit the `ALLOWED_HOSTS` setting in `my_django_app/settings.py` to include your domain name(s). For example, if your domain name is `example.com`, you would add the following line to `my_django_app/settings.py`:
```python
ALLOWED_HOSTS = ['example.com']
```
:::

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM python:3.7-alpine
EXPOSE 8080
WORKDIR /my-django-app 
RUN pip install django
COPY . /my-django-app
ENTRYPOINT [""python3""] 
CMD [""manage.py"", ""runserver"", ""0.0.0.0:8080""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-django-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the Python Django app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1163
React,React App on Section,"Learn how to run a <a href=""https://reactjs.org/"">React app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",80
React,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://icy-tire-4479.section.app/"">https://icy-tire-4479.section.app/</a></strong> to see what you'll be building.</p>

---",55
React,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/react-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/react-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `src/app.js` and watch your changes go live.

---",206
React,Prerequisites,"- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- (optional) An existing React app
- (optional) kubectl",34
React,Steps,"1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section",21
React,Creating your Dockerfile,"We're assuming you are either using the Section tutorial example, you have your own React app that compiles successfully, or a newly created React app eg:
```sh
npx create-react-app $NAME
```

Create a `Dockerfile` in the root folder of your app. It should sit alongside package.json, eg the following

```
.
├── public
│   ├── favicon.ico
│   ├── index.html
│   └── ...
├── src
│   ├── App.js
│   ├── index.js
│   └── ...
├── Dockerfile
├── package-lock.json
└── package.json
```

The [Dockerfile](https://github.com/section/react-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as build
WORKDIR /app
ENV PATH /app/node_modules/.bin:$PATH
COPY package.json ./
COPY package-lock.json ./
RUN npm clean-install
RUN npm install react-scripts -g
COPY . ./
RUN npm run build

# production environment
FROM nginx:alpine
COPY --from=build /app/build /usr/share/nginx/html
EXPOSE 80
CMD [""nginx"", ""-g"", ""daemon off;""]

```

This script would create a clean build of your React app, and host it on port 80 of a lightweight nginx image when it's run.",306
React,Building the container image,"Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh",20
React,Replace these example values,"USER=section
IMAGENAME=react
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```",180
React,Deploy to Section,"Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before, and specify port 80.

That's it! Navigate to your project URL and you'll see your React page live, similar to our <a href=""https://icy-tire-4479.section.app/"">example</a>.

![React demo splash page](/img/docs/react-tutorial.png)",89
React,Learn to deploy a React app at the edge,"
# React App on Section 
Learn how to run a <a href=""https://reactjs.org/"">React app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://icy-tire-4479.section.app/"">https://icy-tire-4479.section.app/</a></strong> to see what you'll be building.</p>

---
## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/react-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/react-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `src/app.js` and watch your changes go live.

---
## Option 2 - Step by Step

### Prerequisites
- Docker or equivalent installed
- A public container repository account (eg GitHub or Docker Hub)
- (optional) An existing React app
- (optional) kubectl

### Steps
1. Create a Dockerfile
1. Build & push your container image
1. Deploy to Section


### Creating your Dockerfile
We're assuming you are either using the Section tutorial example, you have your own React app that compiles successfully, or a newly created React app eg:
```sh
npx create-react-app $NAME
```

Create a `Dockerfile` in the root folder of your app. It should sit alongside package.json, eg the following

```
.
├── public
│   ├── favicon.ico
│   ├── index.html
│   └── ...
├── src
│   ├── App.js
│   ├── index.js
│   └── ...
├── Dockerfile
├── package-lock.json
└── package.json
```

The [Dockerfile](https://github.com/section/react-tutorial/blob/main/Dockerfile)'s contents should be the following:

```dockerfile title=""Dockerfile""
FROM node:alpine as build
WORKDIR /app
ENV PATH /app/node_modules/.bin:$PATH
COPY package.json ./
COPY package-lock.json ./
RUN npm clean-install
RUN npm install react-scripts -g
COPY . ./
RUN npm run build

# production environment
FROM nginx:alpine
COPY --from=build /app/build /usr/share/nginx/html
EXPOSE 80
CMD [""nginx"", ""-g"", ""daemon off;""]

```

This script would create a clean build of your React app, and host it on port 80 of a lightweight nginx image when it's run.


### Building the container image

Simply run the following command from the Dockerfile's directory to build and tag your image.

```sh
# Replace these example values
USER=section
IMAGENAME=react
TAG=0.0.1

docker build . --tag ghcr.io/$USER/$IMAGENAME:$TAG
```


### Push your image to a repository

We will be pushing the container image to GitHub for this example. Follow the [instructions](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry#authenticating-to-the-container-registry) to do a `docker login` on your terminal before running the next command.

```sh
GITHUB_TOKEN="""" # https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token

echo $GITHUB_TOKEN | docker login ghcr.io -u $GITHUB_USER --password-stdin
docker push ghcr.io/$USER/$IMAGENAME:$TAG
```


## Deploy to Section

Follow the steps in this doc - [Deploy a Project](/get-started/create-project/) - and simply insert your image name from before, and specify port 80.

That's it! Navigate to your project URL and you'll see your React page live, similar to our <a href=""https://icy-tire-4479.section.app/"">example</a>.

![React demo splash page](/img/docs/react-tutorial.png)",1057
Dart,Dart App on Section,"Learn how to run a ""Hello World"" <a href=""https://dart.dev/"">Dart app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",84
Dart,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://snowy-morning-9265.section.app/"">https://snowy-morning-9265.section.app/</a></strong> to see what you'll be building.</p>",56
Dart,Step by Step,"Following are step-by-step instructions to deploy a Dart ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",33
Dart,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.,26
Dart,Create the Dart App,"Create a new directory for your app.

```bash
mkdir my-dart-app
cd my-dart-app
```

Create directory `bin` with a new file `server.dart` with the following code.

```dart title=""server.dart""
import 'dart:convert';
import 'dart:io';

import 'package:shelf/shelf.dart';
import 'package:shelf/shelf_io.dart' as shelf_io;
import 'package:shelf_router/shelf_router.dart' as shelf_router;
import 'package:shelf_static/shelf_static.dart' as shelf_static;

Future<void> main() async {
  // If the ""PORT"" environment variable is set, listen to it. Otherwise, 8080.
  // https://cloud.google.com/run/docs/reference/container-contract#port
  final port = int.parse(Platform.environment['PORT'] ?? '8080');

  // See https://pub.dev/documentation/shelf/latest/shelf/Cascade-class.html
  final cascade = Cascade()
      // First, serve files from the 'public' directory
      .add(_staticHandler)
      // If a corresponding file is not found, send requests to a `Router`
      .add(_router);

  // See https://pub.dev/documentation/shelf/latest/shelf_io/serve.html
  final server = await shelf_io.serve(
    // See https://pub.dev/documentation/shelf/latest/shelf/logRequests.html
    logRequests()
        // See https://pub.dev/documentation/shelf/latest/shelf/MiddlewareExtensions/addHandler.html
        .addHandler(cascade.handler),
    InternetAddress.anyIPv4, // Allows external connections
    port,
  );

  print('Serving at http://${server.address.host}:${server.port}');
}

// Serve files from the file system.
final _staticHandler =
    shelf_static.createStaticHandler('public', defaultDocument: 'index.html');

// Router instance to handler requests.
final _router = shelf_router.Router()
  ..get('/helloworld', _helloWorldHandler);

Response _helloWorldHandler(Request request) => Response.ok('Hello World!');
```

Create a directory called `public` in the root directory, and create a new file inside the `public` directory called `index.html` with the following content.

```html title=""index.html""
<html5>
    <head>
        <title>Dart Example on Section</title>
    </head>
    <body>
        <h1>Hello World from Dart on Section!</h1>
    </body>
</html5>
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your root directory with the following content.

```dockerfile title=""Dockerfile""
# Official Dart image: https://hub.docker.com/_/dart
# Specify the Dart SDK base image version using dart:<version> (ex: dart:2.12)
FROM dart:stable AS build

# Resolve app dependencies.
WORKDIR /app
COPY pubspec.* ./
RUN dart pub get

# Copy app source code and AOT compile it.
COPY . .
# Ensure packages are still up-to-date if anything has changed
RUN dart pub get --offline
RUN dart compile exe bin/server.dart -o bin/server

# Build minimal serving image from AOT-compiled `/server` and required system
# libraries and configuration files stored in `/runtime/` from the build stage.
FROM scratch
COPY --from=build /runtime/ /
COPY --from=build /app/bin/server /app/bin/

# Include files in the /public directory to enable static asset handling
COPY --from=build /app/public/ /public

# Start server.
EXPOSE 8080
CMD [""/app/bin/server""]
```

Now create a file called `pubspec.yaml` in the root directory, this is where we will define our dependencies.

```yaml title=""pubspec.yaml""
name: helloworld
publish_to: none

environment:
  sdk: "">=2.12.0 <3.0.0""

dependencies:
  shelf: ^1.2.0
  shelf_router: ^1.0.0
  shelf_static: ^1.0.0

dev_dependencies:
  http: ^0.13.0
  lints: ^2.0.0
  test: ^1.15.0
  test_process: ^2.0.0
```

Finally, create a file called `.dockerignore` in the root directory, this is where we will define files that we don't want to include in our container image.

```text title="".dockerignore""
.dockerignore
Dockerfile
build/
.dart_tool/
.git/
.github/
.gitignore
.packages
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
curl http://localhost:8080
```",1054
Dart,Push It,"Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`Serving at http://0.0.0.0:8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",328
Dart,Learn to deploy a Dart app at the edge,"
# Dart App on Section
Learn how to run a ""Hello World"" <a href=""https://dart.dev/"">Dart app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://snowy-morning-9265.section.app/"">https://snowy-morning-9265.section.app/</a></strong> to see what you'll be building.</p>

## Step by Step

Following are step-by-step instructions to deploy a Dart ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.

### Create the Dart App
Create a new directory for your app.

```bash
mkdir my-dart-app
cd my-dart-app
```

Create directory `bin` with a new file `server.dart` with the following code.

```dart title=""server.dart""
import 'dart:convert';
import 'dart:io';

import 'package:shelf/shelf.dart';
import 'package:shelf/shelf_io.dart' as shelf_io;
import 'package:shelf_router/shelf_router.dart' as shelf_router;
import 'package:shelf_static/shelf_static.dart' as shelf_static;

Future<void> main() async {
  // If the ""PORT"" environment variable is set, listen to it. Otherwise, 8080.
  // https://cloud.google.com/run/docs/reference/container-contract#port
  final port = int.parse(Platform.environment['PORT'] ?? '8080');

  // See https://pub.dev/documentation/shelf/latest/shelf/Cascade-class.html
  final cascade = Cascade()
      // First, serve files from the 'public' directory
      .add(_staticHandler)
      // If a corresponding file is not found, send requests to a `Router`
      .add(_router);

  // See https://pub.dev/documentation/shelf/latest/shelf_io/serve.html
  final server = await shelf_io.serve(
    // See https://pub.dev/documentation/shelf/latest/shelf/logRequests.html
    logRequests()
        // See https://pub.dev/documentation/shelf/latest/shelf/MiddlewareExtensions/addHandler.html
        .addHandler(cascade.handler),
    InternetAddress.anyIPv4, // Allows external connections
    port,
  );

  print('Serving at http://${server.address.host}:${server.port}');
}

// Serve files from the file system.
final _staticHandler =
    shelf_static.createStaticHandler('public', defaultDocument: 'index.html');

// Router instance to handler requests.
final _router = shelf_router.Router()
  ..get('/helloworld', _helloWorldHandler);

Response _helloWorldHandler(Request request) => Response.ok('Hello World!');
```

Create a directory called `public` in the root directory, and create a new file inside the `public` directory called `index.html` with the following content.

```html title=""index.html""
<html5>
    <head>
        <title>Dart Example on Section</title>
    </head>
    <body>
        <h1>Hello World from Dart on Section!</h1>
    </body>
</html5>
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your root directory with the following content.

```dockerfile title=""Dockerfile""
# Official Dart image: https://hub.docker.com/_/dart
# Specify the Dart SDK base image version using dart:<version> (ex: dart:2.12)
FROM dart:stable AS build

# Resolve app dependencies.
WORKDIR /app
COPY pubspec.* ./
RUN dart pub get

# Copy app source code and AOT compile it.
COPY . .
# Ensure packages are still up-to-date if anything has changed
RUN dart pub get --offline
RUN dart compile exe bin/server.dart -o bin/server

# Build minimal serving image from AOT-compiled `/server` and required system
# libraries and configuration files stored in `/runtime/` from the build stage.
FROM scratch
COPY --from=build /runtime/ /
COPY --from=build /app/bin/server /app/bin/

# Include files in the /public directory to enable static asset handling
COPY --from=build /app/public/ /public

# Start server.
EXPOSE 8080
CMD [""/app/bin/server""]
```

Now create a file called `pubspec.yaml` in the root directory, this is where we will define our dependencies.

```yaml title=""pubspec.yaml""
name: helloworld
publish_to: none

environment:
  sdk: "">=2.12.0 <3.0.0""

dependencies:
  shelf: ^1.2.0
  shelf_router: ^1.0.0
  shelf_static: ^1.0.0

dev_dependencies:
  http: ^0.13.0
  lints: ^2.0.0
  test: ^1.15.0
  test_process: ^2.0.0
```

Finally, create a file called `.dockerignore` in the root directory, this is where we will define files that we don't want to include in our container image.

```text title="".dockerignore""
.dockerignore
Dockerfile
build/
.dart_tool/
.git/
.github/
.gitignore
.packages
```

Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-dart-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`Serving at http://0.0.0.0:8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1614
KoaJS,KoaJS on Section,"Learn how to run a ""Hello World"" <a href=""https://koajs.com/"">KoaJS app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",86
KoaJS,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://little-snowflake-1628.section.app"">https://little-snowflake-1628.section.app</a></strong> to see what you'll be building.</p>",55
KoaJS,Option 1 - Copy Our GitHub Repo,"1. Make a new repo from our template: in your browser visit <a href=""https://github.com/section/koajs-tutorial"">https://github.com/section/koajs-tutorial</a> and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the value of `ctx.body` on line 5 of `app.js` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions).",247
KoaJS,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a KoaJS ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.",41
KoaJS,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.,40
KoaJS,Create the KoaJS App,"Create a new directory for your app.

```bash
mkdir node-koajs
cd node-koajs
```

Install the KoaJS application.

```bash
npm i koa
```

And then we add our application code. Create `app.js` with the following code.

```javascript title=""app.js""
const Koa = require('koa');
const app = new Koa();

app.use(async ctx => {
  ctx.body = 'Hello World from KoaJS on Section!';
});

app.listen(3000);
```

Test it using `node app.js` and visit it using `curl http://localhost:3000`. You'll get the ""Hello World from KoaJS on Section!"" response.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM node:lts as runner
WORKDIR /node-koajs
ENV NODE_ENV production
ARG COMMIT_ID
ENV COMMIT_ID=${COMMIT_ID}
COPY . .
RUN npm i koajs
EXPOSE 3000
CMD [""node"", ""app.js""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
curl http://localhost:3000
```",322
KoaJS,Push It,"Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-koajs-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000 (`App listening at http://localhost:3000`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",323
KoaJS,Learn to deploy a KoaJS app at the edge,"
# KoaJS on Section
Learn how to run a ""Hello World"" <a href=""https://koajs.com/"">KoaJS app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://little-snowflake-1628.section.app"">https://little-snowflake-1628.section.app</a></strong> to see what you'll be building.</p>

## Option 1 - Copy Our GitHub Repo
1. Make a new repo from our template: in your browser visit <a href=""https://github.com/section/koajs-tutorial"">https://github.com/section/koajs-tutorial</a> and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).
1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make a simple change to the value of `ctx.body` on line 5 of `app.js` and watch your changes go live.

Every time you push to the repo your project will be built and deployed to Section automatically using [GitHub Actions](https://docs.github.com/en/actions). 

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a KoaJS ""Hello World"" application to the edge on Section. We'll Dockerize it, push it to GitHub Packages, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) and [Node](https://nodejs.org/en/download/package-manager/) installed so that you can build a docker image.

### Create the KoaJS App
Create a new directory for your app.

```bash
mkdir node-koajs
cd node-koajs
```

Install the KoaJS application.

```bash
npm i koa
```

And then we add our application code. Create `app.js` with the following code.

```javascript title=""app.js""
const Koa = require('koa');
const app = new Koa();

app.use(async ctx => {
  ctx.body = 'Hello World from KoaJS on Section!';
});

app.listen(3000);
```

Test it using `node app.js` and visit it using `curl http://localhost:3000`. You'll get the ""Hello World from KoaJS on Section!"" response.

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM node:lts as runner
WORKDIR /node-koajs
ENV NODE_ENV production
ARG COMMIT_ID
ENV COMMIT_ID=${COMMIT_ID}
COPY . .
RUN npm i koajs
EXPOSE 3000
CMD [""node"", ""app.js""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
```

Launch it locally to test it.

```bash
docker run -p 3000:3000 ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
curl http://localhost:3000
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/node-koajs:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-koajs-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 3000 (`App listening at http://localhost:3000`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1165
Gatsby,Gatsby.js on Section,"Learn how to deploy a <a href=""https://www.gatsbyjs.com/"">Gatsby.js app</a> on Section. At the end of this tutorial, you'll have a Gatsby.js app deployed with the following:

* Publicly accessible URL for your app
* Automatic SSL
* Automatic HTTP/2
* Multi-Region hosting
* Multi-Cloud hosting
* Layer 3 and 4 DDoS protection",89
Gatsby,Step by Step,"Follow the step-by-step instructions to deploy a Gatsby.js ""Hello World"" application on Section. We'll Dockerize it, push it to Dockerhub, and deploy it on Section.",38
Gatsby,Prerequisites,"You'll need the following to complete this tutorial:

* A Docker Hub account
* A Section account
* Node.js installed on your local machine
  * You can install it from <a href=""https://nodejs.org/en/download/"">here</a>
* Gatsby.js installed on your local machine
  * You can install it from <a href=""https://www.gatsbyjs.com/docs/tutorial/part-0/"">here</a>
* Docker installed on your local machine",98
Gatsby,Create the Gatsby.js App,"Create a new directory for your app.

```bash
mkdir gatsby-app
cd gatsby-app
```

Initialise a Gatsby.js project by using the Gatsby CLI.

```bash
gatsby new .
```

Make a simple change to the HTML in `src/pages/index.js` and test it locally.

```bash
gatsby develop
```

### Create a Dockerfile
Create a file called `Dockerfile` in the root of your project with the following contents:

```dockerfile title=""Dockerfile""
FROM node:18

# set the working directory
WORKDIR /app
# copy the repository files to it
COPY . /app

RUN npm install
RUN npm install -g gatsby-cli

RUN gatsby build
EXPOSE 80

CMD gatsby serve --port 80 --host 0.0.0.0
```

Then build the image with your docker daemon.

```bash
docker build -t gatsby-app:v1 .
```

### Push to Docker Hub
Authenticate your local docker with Docker Hub.

```bash
docker login
```

Ensure you have a repository on Docker Hub. If you don't, create one. E.g. `myusername/gatsby-app`

Tag the image and push it to Docker Hub. Replace `myusername` with your Docker Hub username/organization. *Note: We are assuming the repository you've created is public.*

```bash
docker tag gatsby-app:v1 myusername/gatsby-app:latest
docker push myusername/gatsby-app:latest
```

### Deploy to Section
Login to Section and [create a new Section project](https://www.section.io/docs/get-started/create-project/).

Use the following values for the project settings:

* **Plan**: Free
* **Image**: myusername/gatsby-app:latest
* **Port**: 80

Click **Create Project**.

### Test the Deployment
Once the deployment is complete, you can test it by visiting the `section.app` URL provided in the project details. Your traffic will be routed to the closest Point of Presence based on your geographical location.",435
Gatsby,Learn to deploy a Gatsby app,"
# Gatsby.js on Section
Learn how to deploy a <a href=""https://www.gatsbyjs.com/"">Gatsby.js app</a> on Section. At the end of this tutorial, you'll have a Gatsby.js app deployed with the following:

* Publicly accessible URL for your app
* Automatic SSL
* Automatic HTTP/2
* Multi-Region hosting
* Multi-Cloud hosting
* Layer 3 and 4 DDoS protection

## Step by Step
Follow the step-by-step instructions to deploy a Gatsby.js ""Hello World"" application on Section. We'll Dockerize it, push it to Dockerhub, and deploy it on Section.

### Prerequisites
You'll need the following to complete this tutorial:

* A Docker Hub account
* A Section account
* Node.js installed on your local machine
  * You can install it from <a href=""https://nodejs.org/en/download/"">here</a>
* Gatsby.js installed on your local machine
  * You can install it from <a href=""https://www.gatsbyjs.com/docs/tutorial/part-0/"">here</a>
* Docker installed on your local machine

### Create the Gatsby.js App
Create a new directory for your app.

```bash
mkdir gatsby-app
cd gatsby-app
```

Initialise a Gatsby.js project by using the Gatsby CLI.

```bash
gatsby new .
```

Make a simple change to the HTML in `src/pages/index.js` and test it locally.

```bash
gatsby develop
```

### Create a Dockerfile
Create a file called `Dockerfile` in the root of your project with the following contents:

```dockerfile title=""Dockerfile""
FROM node:18

# set the working directory
WORKDIR /app
# copy the repository files to it
COPY . /app

RUN npm install
RUN npm install -g gatsby-cli

RUN gatsby build
EXPOSE 80

CMD gatsby serve --port 80 --host 0.0.0.0
```

Then build the image with your docker daemon.

```bash
docker build -t gatsby-app:v1 .
```

### Push to Docker Hub
Authenticate your local docker with Docker Hub.

```bash
docker login
```

Ensure you have a repository on Docker Hub. If you don't, create one. E.g. `myusername/gatsby-app`

Tag the image and push it to Docker Hub. Replace `myusername` with your Docker Hub username/organization. *Note: We are assuming the repository you've created is public.*

```bash
docker tag gatsby-app:v1 myusername/gatsby-app:latest
docker push myusername/gatsby-app:latest
```

### Deploy to Section
Login to Section and [create a new Section project](https://www.section.io/docs/get-started/create-project/).

Use the following values for the project settings:

* **Plan**: Free
* **Image**: myusername/gatsby-app:latest
* **Port**: 80

Click **Create Project**.

### Test the Deployment
Once the deployment is complete, you can test it by visiting the `section.app` URL provided in the project details. Your traffic will be routed to the closest Point of Presence based on your geographical location.
",687
Python Flask,Python Flask App on Section,"Learn how to run a ""Hello World"" <a href=""https://flask.palletsprojects.com/en/2.2.x/"">Python Flask app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).",95
Python Flask,What You'll Build,"<p><mark>Short on time?</mark> Visit <strong><a href=""https://twilight-fire-9701.section.app/"">https://twilight-fire-9701.section.app/</a></strong> to see what you'll be building.</p>",54
Python Flask,Option 1 - Copy Our GitHub Repo,"![workflow status](https://github.com/section/python-flask-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/python-flask-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.py` and watch your changes go live.",210
Python Flask,Option 2 - Step by Step,"Following are step-by-step instructions to deploy a Python Flask ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.",34
Python Flask,Prerequisites,* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.,26
Python Flask,Create the Python Flask App,"Create a new directory for your app.

```bash
mkdir my-flask-app
cd my-flask-app
```

Create `helloworld.py` with the following code.

```python title=""helloworld.py""
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello World from Python Flask on Section!'

if __name__ == ""__main__"":
    app.run(debug=True, host='0.0.0.0', port=8080)
```

Create `requirements.txt` with the following content to specify the version of Flask you are using.

```text title=""requirements.txt""
Flask==2.2.2
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM python:3-alpine
WORKDIR /my-flask-app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8080
CMD [""python"", ""helloworld.py""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod
curl http://localhost:8080
```",306
Python Flask,Push It,"Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`App listening at http://localhost:8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",321
Python Flask,Learn to deploy a Python Flask app at the edge,"
# Python Flask App on Section
Learn how to run a ""Hello World"" <a href=""https://flask.palletsprojects.com/en/2.2.x/"">Python Flask app</a> at the edge for low latency and high availability. You can use our repo as a template, or perform the steps yourself using the [Kubernetes dashboard](/guides/projects/manage-resources/#add-resource) or [kubectl commands](/guides/kubernetes-ui/kubernetes-api/get-started-k8/).

## What You'll Build
<p><mark>Short on time?</mark> Visit <strong><a href=""https://twilight-fire-9701.section.app/"">https://twilight-fire-9701.section.app/</a></strong> to see what you'll be building.</p>

## Option 1 - Copy Our GitHub Repo
![workflow status](https://github.com/section/python-flask-tutorial/actions/workflows/workflows.yaml/badge.svg)

Make a new repo from our template: in your browser visit https://github.com/section/python-flask-tutorial and select **`Use this template`** (don't clone, don't fork, but use the template). Choose yourself as an owner, give it a name of your choice, and make it be <mark>Public</mark> (not Private).

1. In your new GitHub repo, under Settings > Secrets > Actions, use **`New repository secret`** to add these two:
   - `SECTION_K8S_API_URL`: this is the [Kubernetes API endpoint](/guides/kubernetes-ui/kubernetes-api/basics/#kubernetes-api-url) for your new project
   - `SECTION_API_TOKEN`: this is a [Section API token](/guides/iam/api-tokens/)
1. Make any change to `./helloworld.py` and watch your changes go live.

## Option 2 - Step by Step

Following are step-by-step instructions to deploy a Python Flask ""Hello World"" application to the edge on Section. We'll Dockerize it, and deploy it on Section.

### Prerequisites
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.

### Create the Python Flask App
Create a new directory for your app.

```bash
mkdir my-flask-app
cd my-flask-app
```

Create `helloworld.py` with the following code.

```python title=""helloworld.py""
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello World from Python Flask on Section!'

if __name__ == ""__main__"":
    app.run(debug=True, host='0.0.0.0', port=8080)
```

Create `requirements.txt` with the following content to specify the version of Flask you are using.

```text title=""requirements.txt""
Flask==2.2.2
```

### Dockerize It
Let's build the container image that we'll deploy to Section. First make a `Dockerfile` in your directory with the following content.

```dockerfile title=""Dockerfile""
FROM python:3-alpine
WORKDIR /my-flask-app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
EXPOSE 8080
CMD [""python"", ""helloworld.py""]
```
Build and tag it.
```bash
docker build . -t ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod
```

Launch it locally to test it.

```bash
docker run -p 8080:8080 ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod
curl http://localhost:8080
```

### Push It
Push it to GitHub Packages. This makes it available to Section.

```bash
docker push ghcr.io/YOUR_GITHUB_USERNAME/my-go-app:prod
```

Be sure to make it public. To see your packages and make this change, visit `https://github.com/YOUR_GITHUB_USERNAME?tab=packages`

### Deploy It
Next, use the [Create Project](https://www.section.io/docs/get-started/create-project/) command in the Section Console in order to deploy your new container. Use the image name `ghcr.io/YOUR_GITHUB_USERNAME/my-flask-app:prod` with port 8080.

See the pods running on Section's network with either the Kubernetes dashboard or `kubectl get pods -o wide`.  The ```-o wide``` switch shows where your app is running according to the default [AEE location optimization](/explanations/aee) strategy. Your app will be optimally deployed according to traffic. In lieu of significant traffic, your deployment will be made to default locations.

Try `kubectl logs POD` to see the log message reporting that the server is listening on port 8080 (`App listening at http://localhost:8080`)

Finally, follow the instructions that configure [DNS](/docs/guides/projects/dns/) and [TLS](/docs/guides/projects/ssl/).

### See What You've Built
See the ""Hello World!"" app you've built by visiting the `https://YOUR.DOMAIN.COM`, substituting `YOUR.DOMAIN.COM` according to your DNS and HTTPS configuration.",1095
PyTorch,Distributed Machine Learning Predictions Using PyTorch,"Achieve faster ML model serving for your users at the edge by running a distributed ML model server. This tutorial will use  Section to deploy [PyTorch TorchServe](https://github.com/pytorch/serve) with an example pretrained model.

The PyTorch container we will use is [available on DockerHub](https://hub.docker.com/r/pytorch/torchserve). This tutorial was inspired by a [GCP example](https://cloud.google.com/ai-platform/prediction/docs/getting-started-pytorch-container).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",170
PyTorch,Prerequisites,"* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",43
PyTorch,Pull Down the Pretrained Model,"Pull down the PyTorch example models from GitHub so that we can build one of them into the container image. We'll be using one that recognizes digits from PNG images.

```bash
mkdir my-pytorch-example
cd my-pytorch-example
git clone https://github.com/pytorch/serve.git \
  --branch=v0.3.0 \
  --depth=1
```

## Create a Dockerfile for Your Container Image
The container image you'll build relies upon [PyTorch Serve](https://hub.docker.com/r/pytorch/torchserve) on Docker Hub. We'll be using the MNIST model, which will allow us to do image classification of digits from PNG images.

```dockerfile title=""Dockerfile""
FROM pytorch/torchserve:0.3.0-cpu

COPY serve/examples/image_classifier/mnist/mnist.py \
    serve/examples/image_classifier/mnist/mnist_cnn.pt \
    serve/examples/image_classifier/mnist/mnist_handler.py \
    /home/model-server/

USER root
RUN printf ""\nservice_envelope=json"" >> /home/model-server/config.properties
USER model-server

RUN torch-model-archiver \
  --model-name=mnist \
  --version=1.0 \
  --model-file=/home/model-server/mnist.py \
  --serialized-file=/home/model-server/mnist_cnn.pt \
  --handler=/home/model-server/mnist_handler.py \
  --export-path=/home/model-server/model-store

CMD [""torchserve"", \
     ""--start"", \
     ""--ts-config=/home/model-server/config.properties"", \
     ""--models"", \
     ""mnist=mnist.mar""]
```",345
PyTorch,Build and Publish the Image,"Build the docker image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.
```bash
docker build -t my-pytorch-image .
docker tag my-pytorch-image YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
docker push YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
```

## Create a Kubernetes Deployment for PyTorch
Next, create the deployment for PyTorch as pytorch-deployment.yaml substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""pytorch-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pytorch
  name: pytorch
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pytorch
  template:
    metadata:
      labels:
        app: pytorch
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
        imagePullPolicy: Always
        name: pytorch
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f pytorch-upstream.yaml`.",316
PyTorch,Expose the Service on the Internet,"We want to expose the PyTorch service on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: pytorch
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

## Create a File with an Image
You'll send an image of a '3' to the prediction engine to see if it can figure it out. Place the following JSON into a file called `png-image-of-a-3.json`.

```json title=""png-image-of-a-3.json""
{
  ""instances"": [
    {
      ""data"": {
        ""b64"": ""iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAv0lEQVR4nGNgGKSA03faPyDwxibHu/7vvwfnzz/5tsgRU3LW33uukgwMCi1PdmBKOr7dAmEsuiiIKSssDpX8q4fbYYv/4ZZk3YTNWCg48HcGTrnOf39dcUgpzPv97+/b56LY5PKBIfTi+bt//7ptMSV7Py6NYWCQirn17zymJK8R1PRVd4RxuoqhG6erCEmevoBbbsqvUkxBXWMQabzk+wksOhZ9vHDh4oWPf1d6YZFUuff377+/9zp5cNtIHQAAtP5OgKw1m4AAAAAASUVORK5CYII=""
      }
    }
  ]
}
```

The image looks like this:

![Image of 3](/img/docs/3.png)",547
PyTorch,Start Making Predictions at the Edge,"Exercise the ML model server substituting `YOUR_ENVIRONMENT_HOSTNAME` accordingly.
```bash
curl -X POST \
  -H ""Content-Type: application/json; charset=utf-8"" \
  -d @png-image-of-a-3.json \
  YOUR_ENVIRONMENT_HOSTNAME/predictions/mnist
```

The result you'll get:
```
{ ""predictions"": [3] }
```",83
PyTorch,"Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon PyTorch","
# Distributed Machine Learning Predictions Using PyTorch

Achieve faster ML model serving for your users at the edge by running a distributed ML model server. This tutorial will use  Section to deploy [PyTorch TorchServe](https://github.com/pytorch/serve) with an example pretrained model.

The PyTorch container we will use is [available on DockerHub](https://hub.docker.com/r/pytorch/torchserve). This tutorial was inspired by a [GCP example](https://cloud.google.com/ai-platform/prediction/docs/getting-started-pytorch-container).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.

## Pull Down the Pretrained Model
Pull down the PyTorch example models from GitHub so that we can build one of them into the container image. We'll be using one that recognizes digits from PNG images.

```bash
mkdir my-pytorch-example
cd my-pytorch-example
git clone https://github.com/pytorch/serve.git \
  --branch=v0.3.0 \
  --depth=1
```

## Create a Dockerfile for Your Container Image
The container image you'll build relies upon [PyTorch Serve](https://hub.docker.com/r/pytorch/torchserve) on Docker Hub. We'll be using the MNIST model, which will allow us to do image classification of digits from PNG images.

```dockerfile title=""Dockerfile""
FROM pytorch/torchserve:0.3.0-cpu

COPY serve/examples/image_classifier/mnist/mnist.py \
    serve/examples/image_classifier/mnist/mnist_cnn.pt \
    serve/examples/image_classifier/mnist/mnist_handler.py \
    /home/model-server/

USER root
RUN printf ""\nservice_envelope=json"" >> /home/model-server/config.properties
USER model-server

RUN torch-model-archiver \
  --model-name=mnist \
  --version=1.0 \
  --model-file=/home/model-server/mnist.py \
  --serialized-file=/home/model-server/mnist_cnn.pt \
  --handler=/home/model-server/mnist_handler.py \
  --export-path=/home/model-server/model-store

CMD [""torchserve"", \
     ""--start"", \
     ""--ts-config=/home/model-server/config.properties"", \
     ""--models"", \
     ""mnist=mnist.mar""]
```

## Build and Publish the Image
Build the docker image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.
```bash
docker build -t my-pytorch-image .
docker tag my-pytorch-image YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
docker push YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
```

## Create a Kubernetes Deployment for PyTorch
Next, create the deployment for PyTorch as pytorch-deployment.yaml substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""pytorch-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: pytorch
  name: pytorch
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pytorch
  template:
    metadata:
      labels:
        app: pytorch
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/pytorch:latest
        imagePullPolicy: Always
        name: pytorch
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f pytorch-upstream.yaml`.

## Expose the Service on the Internet
We want to expose the PyTorch service on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8080
  selector:
    app: pytorch
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

## Create a File with an Image
You'll send an image of a '3' to the prediction engine to see if it can figure it out. Place the following JSON into a file called `png-image-of-a-3.json`.

```json title=""png-image-of-a-3.json""
{
  ""instances"": [
    {
      ""data"": {
        ""b64"": ""iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAv0lEQVR4nGNgGKSA03faPyDwxibHu/7vvwfnzz/5tsgRU3LW33uukgwMCi1PdmBKOr7dAmEsuiiIKSssDpX8q4fbYYv/4ZZk3YTNWCg48HcGTrnOf39dcUgpzPv97+/b56LY5PKBIfTi+bt//7ptMSV7Py6NYWCQirn17zymJK8R1PRVd4RxuoqhG6erCEmevoBbbsqvUkxBXWMQabzk+wksOhZ9vHDh4oWPf1d6YZFUuff377+/9zp5cNtIHQAAtP5OgKw1m4AAAAAASUVORK5CYII=""
      }
    }
  ]
}
```

The image looks like this:

![Image of 3](/img/docs/3.png)

## Start Making Predictions at the Edge
Exercise the ML model server substituting `YOUR_ENVIRONMENT_HOSTNAME` accordingly.
```bash
curl -X POST \
  -H ""Content-Type: application/json; charset=utf-8"" \
  -d @png-image-of-a-3.json \
  YOUR_ENVIRONMENT_HOSTNAME/predictions/mnist
```

The result you'll get:
```
{ ""predictions"": [3] }
```",1554
TensorFlow,Distributed Machine Learning Predictions Using Tensorflow,"Achieve faster ML model serving for your users at the edge by running a distributed ML model server. This tutorial will use Section to deploy [TensorFlow Serving](https://github.com/tensorflow/serving) with an example pretrained model.

The TensorFlow container we will use is [available on DockerHub](https://hub.docker.com/r/tensorflow/serving).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",134
TensorFlow,Prerequisites,"* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.",43
TensorFlow,Pull Down the Pretrained Model,"Pull down the TensorFlow example models from GitHub so that we can build one of them into the container image.

```bash
mkdir my-tensorflow-example
cd my-tensorflow-example
git clone https://github.com/tensorflow/serving
```

## Create a Dockerfile for Your Container Image
The container image you'll build relies upon [TensorFlow Serving](https://hub.docker.com/r/tensorflow/serving) on Docker Hub. We'll just use one of those models you downloaded, called `saved_model_half_plus_two_cpu`.  It halves a value, then adds 2.
```dockerfile title=""Dockerfile""
FROM tensorflow/serving

ADD serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu /models/half_plus_two

ENTRYPOINT [""/usr/bin/tf_serving_entrypoint.sh""]
```",180
TensorFlow,Build and Publish the Image,"Build the image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.
```bash
docker build -t my-tensorflow-image .
docker tag my-tensorflow-image YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
docker push YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
```

## Create a Kubernetes Deployment for TensorFlow
Next, create the deployment for TensorFlow as tensorflow-deployment.yaml substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""tensorflow-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tensorflow
  name: tensorflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tensorflow
  template:
    metadata:
      labels:
        app: tensorflow
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
        imagePullPolicy: Always
        name: tensorflow
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: MODEL_NAME
          value: half_plus_two
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f tensorflow-upstream.yaml`.",323
TensorFlow,Expose the Service on the Internet,"We want to expose the TensorFlow service on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8501
  selector:
    app: tensorflow
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

## Start Making Predictions at the Edge
Exercise the ML prediction service substituting `YOUR_ENVIRONMENT_HOSTNAME` accordingly.
```bash
curl -d '{""instances"": [1.0, 2.0, 5.0]}' \
  -X POST http://YOUR_ENVIRONMENT_HOSTNAME/v1/models/half_plus_two:predict
```

The result you'll get:
```
{
    ""predictions"": [2.5, 3.0, 4.5]
}
```",327
TensorFlow,"Learn to deploy a multi-datacenter, multi-provider, machine learning predication capability based upon Tensorflow","
# Distributed Machine Learning Predictions Using Tensorflow

Achieve faster ML model serving for your users at the edge by running a distributed ML model server. This tutorial will use Section to deploy [TensorFlow Serving](https://github.com/tensorflow/serving) with an example pretrained model.

The TensorFlow container we will use is [available on DockerHub](https://hub.docker.com/r/tensorflow/serving).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

### Prerequisites
* You need an account on [Docker Hub](https://hub.docker.com).
* You need [Docker](https://docs.docker.com/get-docker) installed so that you can build a docker image.

## Pull Down the Pretrained Model
Pull down the TensorFlow example models from GitHub so that we can build one of them into the container image.

```bash
mkdir my-tensorflow-example
cd my-tensorflow-example
git clone https://github.com/tensorflow/serving
```

## Create a Dockerfile for Your Container Image
The container image you'll build relies upon [TensorFlow Serving](https://hub.docker.com/r/tensorflow/serving) on Docker Hub. We'll just use one of those models you downloaded, called `saved_model_half_plus_two_cpu`.  It halves a value, then adds 2.
```dockerfile title=""Dockerfile""
FROM tensorflow/serving

ADD serving/tensorflow_serving/servables/tensorflow/testdata/saved_model_half_plus_two_cpu /models/half_plus_two

ENTRYPOINT [""/usr/bin/tf_serving_entrypoint.sh""]
```

## Build and Publish the Image
Build the image and push it to Docker Hub, substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly.
```bash
docker build -t my-tensorflow-image .
docker tag my-tensorflow-image YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
docker push YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
```

## Create a Kubernetes Deployment for TensorFlow
Next, create the deployment for TensorFlow as tensorflow-deployment.yaml substituting `YOUR_DOCKERHUB_ACCOUNT` accordingly. This will direct Section to distribute the container you've pushed to Docker Hub.

```yaml title=""tensorflow-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: tensorflow
  name: tensorflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tensorflow
  template:
    metadata:
      labels:
        app: tensorflow
    spec:
      containers:
      - image: YOUR_DOCKERHUB_ACCOUNT/tensorflow:latest
        imagePullPolicy: Always
        name: tensorflow
        resources:
          requests:
            memory: "".5Gi""
            cpu: ""500m""
          limits:
            memory: "".5Gi""
            cpu: ""500m""
        env:
        - name: MODEL_NAME
          value: half_plus_two
```

Apply this deployment resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f tensorflow-upstream.yaml`.

## Expose the Service on the Internet
We want to expose the TensorFlow service on the Internet. Create ingress-upstream.yaml as defined below.

```yaml title=""ingress-upstream.yaml""
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingress-upstream
  name: ingress-upstream
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 8501
  selector:
    app: tensorflow
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

Apply this service resource to your Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f ingress-upstream.yaml`.

See the pods running on Section's network using `kubectl get pods -o wide`.

The `-o wide` switch shows where your container is running according to the default [AEE location optimization](/explanations/aee) strategy. Your container will be optimally deployed according to traffic.

## Start Making Predictions at the Edge
Exercise the ML prediction service substituting `YOUR_ENVIRONMENT_HOSTNAME` accordingly.
```bash
curl -d '{""instances"": [1.0, 2.0, 5.0]}' \
  -X POST http://YOUR_ENVIRONMENT_HOSTNAME/v1/models/half_plus_two:predict
```

The result you'll get:
```
{
    ""predictions"": [2.5, 3.0, 4.5]
}
```",1047
Database Caching with PolyScale,Reduce Database Latency with Global Caching,"You've decided to distribute your app closer to your users for increased performance. But if you also leverage a central relational database, how do you prevent that from being a performance bottleneck? In this tutorial we use [PolyScale.ai](https://polyscale.ai/) to address this problem. PolyScale provides a global database cache as a service that automatically handles cache lifetimes and invalidation. It is wire-protocol compatible with multiple databases (MySQL, Postgres, etc.). This enables you to scale your single, fixed-location database without requiring changes in your code, operating read-replicas, or incurring the cost of nodes for a distributed database.

What we've built is a simple performance testing app that executes a query:
- from every Section location where your project is deployed,
- into the nearest [PolyScale.ai](https://polyscale.ai) cache location,
- and then in-turn into your origin database for any cache misses.

The query executes every 60 seconds and emits a log entry with latency measurements. For those who want to go further, we provide an Extra Credit section that shows you how to scrape a metrics endpoint and send data to Grafana Cloud for charting.

There is no need to build the Docker image, we provide one for you on https://ghcr.io/section/polyscale-metrics. The deployment yaml refers to that one, although we've provided a Dockerfile if you'd like to customize it.

You just need to substitute your secrets and connection strings and deploy the yamls to your Section Project.

Checkout our [Mastodon hosting tutorial](/tutorials/mastodon) for an example use case!

![Section and Polyscale Deployment](/img/docs/ps4.png)",347
Database Caching with PolyScale,Prerequisites,"* You'll need a small example SQL database from PolyScale's [supported types](https://docs.polyscale.ai/supported-platforms/). The actual contents do not matter. A simple Supabase that you create yourself will suffice, or you can use a sample read-only database URL that we provide so that you don't have to create your own.
* Create an account and global cache for your database at [PolyScale.ai](https://app.polyscale.ai/signup).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::",157
Database Caching with PolyScale,The Basic Idea,"Clone our repo to get started: [https://github.com/section/polyscale-metrics](https://github.com/section/polyscale-metrics).

The meat of the app is this small snippet located in `dbQuery()` in `polyscale_metrics.go`:

```go title=""polyscale-metrics.go""
...
	// Time the query
	start := time.Now()
	rows, err := conn.Query(context.Background(), query)
	duration := time.Since(start)
```
The above query is called twice, once against the cache and once against the origin. It repeats this process forever with a sleep in between, logging the results to stdout. That's all there is to it.

## Setup Your Databases
Here is a snipped of the deployment yaml showing the container environment variables that you'll need pertaining to your database.
```yaml title=""polyscale-metrics-deployment.yaml""
...
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName        
          - name: CACHE_DATABASE_URL
            value: YOUR_CACHE_DATABASE_URL
          - name: ORIGIN_DATABASE_URL
            value: YOUR_ORIGIN_DATABASE_URL
          - name: QUERY
            value: YOUR_QUERY
```

If you don't want to create your own database, use ours:

```
ORIGIN_DATABASE_URL is postgresql://read_only_user:8BiusLd6Z89kjVgS@database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com:5432/postgres
CACHE_DATABASE_URL is whatever PolyScale gives you
QUERY is ""SELECT * from pets.pet_names limit 1;""
```

And then from that database create a PolyScale cache by clicking on the New Cache button in the top-right corner. If you use our database, tt is a Postgres database, host is `database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com`, and port is 5432.",415
Database Caching with PolyScale,Deployment,"Replace the three strings accordingly and apply your modified deployment resource to your Section Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f polyscale-metrics-deployment.yaml`.

See the pods running with `kubectl get pods -o wide`. 

```
$ kubectl get pods -o wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES
polyscale-metrics-86449c4f7d-qtrwd   1/1     Running   0          12m   10.244.70.138   sof-zzbd7   <none>           <none>
polyscale-metrics-86449c4f7d-sgqs9   1/1     Running   0          12m   10.244.48.80    rio-kz6s3   <none>           <none>
```

And pick one of the pod names to see its logs using `kubectl logs POD`.

```
$ kubectl logs polyscale-metrics-86449c4f7d-sgqs9
Node lmn-rio-k1-shared-ingress12 listening to /metrics on port :2112 interval 300 s query select * from foodsales limit 1;
nodename lmn-rio-k1-shared-ingress12 cache 618ms origin 634ms 
nodename lmn-rio-k1-shared-ingress12 cache 9ms origin 316ms 
nodename lmn-rio-k1-shared-ingress12 cache 9ms origin 316ms 
```

Explore the other pods as well to see the speed of the cache from other locations.

That's it!",372
Database Caching with PolyScale,Move the Workload Around,Use the [location optimizer](/guides/projects/set-edge-locations.md) to cause your project to move to other locations so that you can see how Section and PolyScale work together.,38
Database Caching with PolyScale,Extra Credit: Metrics in Grafana,"Our GoLang app exposes a metrics endpoint so that you can scrape p50, p90, and p95 query latencies into a 3rd party metrics system such as Grafana Cloud. Read more in our [tutorial](/guides/monitor/exporting-telemetry/grafana-section/) about scraping metrics into Grafana.

Replace items in the `remote_write` section of the ConfigMap:

```yaml title=""grafana-app-agent-configmap.yaml""
...
    remote_write:
    - url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
      basic_auth:
        username: GRAFANA_METRICS_INSTANCE_ID
        password: GRAFANA_API_KEY
```

And apply with `kubectl apply -f grafana-app-scrape-configmap.yaml`.

In the Deployment resource no substitutions required. So just apply with `kubectl apply -f grafana-app-agent-deployment.yaml`.

And finally, in order for the Grafana agent to contact the polyscale-metrics-service pod, you'll need a Kubernetes Service resource. We've provided that for you, so just apply with `kubectl apply -f polyscale-metrics-service.yaml`. This exposes `http://polyscale-metrics-service:80` within your Section project so that the Grafana agent can access the metrics endpoint.

Then go to the Explore menu in Grafana Cloud to start charting your metrics!",277
Database Caching with PolyScale,Learn how to reduce your database latencies using serverless global caching,"
# Reduce Database Latency with Global Caching
You've decided to distribute your app closer to your users for increased performance. But if you also leverage a central relational database, how do you prevent that from being a performance bottleneck? In this tutorial we use [PolyScale.ai](https://polyscale.ai/) to address this problem. PolyScale provides a global database cache as a service that automatically handles cache lifetimes and invalidation. It is wire-protocol compatible with multiple databases (MySQL, Postgres, etc.). This enables you to scale your single, fixed-location database without requiring changes in your code, operating read-replicas, or incurring the cost of nodes for a distributed database.

What we've built is a simple performance testing app that executes a query:
- from every Section location where your project is deployed,
- into the nearest [PolyScale.ai](https://polyscale.ai) cache location,
- and then in-turn into your origin database for any cache misses.

The query executes every 60 seconds and emits a log entry with latency measurements. For those who want to go further, we provide an Extra Credit section that shows you how to scrape a metrics endpoint and send data to Grafana Cloud for charting.

There is no need to build the Docker image, we provide one for you on https://ghcr.io/section/polyscale-metrics. The deployment yaml refers to that one, although we've provided a Dockerfile if you'd like to customize it.

You just need to substitute your secrets and connection strings and deploy the yamls to your Section Project.

Checkout our [Mastodon hosting tutorial](/tutorials/mastodon) for an example use case!

![Section and Polyscale Deployment](/img/docs/ps4.png)

## Prerequisites
* You'll need a small example SQL database from PolyScale's [supported types](https://docs.polyscale.ai/supported-platforms/). The actual contents do not matter. A simple Supabase that you create yourself will suffice, or you can use a sample read-only database URL that we provide so that you don't have to create your own.
* Create an account and global cache for your database at [PolyScale.ai](https://app.polyscale.ai/signup).

:::note
Before starting, create a new Section Project and then [delete the default Deployment](/guides/projects/manage-resources/#delete-deployment) and [`ingress-upstream` Service](/guides/projects/manage-resources/#delete-service) to prepare the project for your new deployment.
:::

## The Basic Idea
Clone our repo to get started: [https://github.com/section/polyscale-metrics](https://github.com/section/polyscale-metrics).

The meat of the app is this small snippet located in `dbQuery()` in `polyscale_metrics.go`:

```go title=""polyscale-metrics.go""
...
	// Time the query
	start := time.Now()
	rows, err := conn.Query(context.Background(), query)
	duration := time.Since(start)
```
The above query is called twice, once against the cache and once against the origin. It repeats this process forever with a sleep in between, logging the results to stdout. That's all there is to it.

## Setup Your Databases
Here is a snipped of the deployment yaml showing the container environment variables that you'll need pertaining to your database.
```yaml title=""polyscale-metrics-deployment.yaml""
...
        env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName        
          - name: CACHE_DATABASE_URL
            value: YOUR_CACHE_DATABASE_URL
          - name: ORIGIN_DATABASE_URL
            value: YOUR_ORIGIN_DATABASE_URL
          - name: QUERY
            value: YOUR_QUERY
```

If you don't want to create your own database, use ours:

```
ORIGIN_DATABASE_URL is postgresql://read_only_user:8BiusLd6Z89kjVgS@database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com:5432/postgres
CACHE_DATABASE_URL is whatever PolyScale gives you
QUERY is ""SELECT * from pets.pet_names limit 1;""
```

And then from that database create a PolyScale cache by clicking on the New Cache button in the top-right corner. If you use our database, tt is a Postgres database, host is `database-hasura-1.cluster-cf59c7eojxdx.us-west-1.rds.amazonaws.com`, and port is 5432.

## Deployment
Replace the three strings accordingly and apply your modified deployment resource to your Section Project with either the [Kubernetes dashboard](/docs/guides/projects/manage-resources/#add-a-k8s-resource) or `kubectl apply -f polyscale-metrics-deployment.yaml`.

See the pods running with `kubectl get pods -o wide`. 

```
$ kubectl get pods -o wide
NAME                                 READY   STATUS    RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES
polyscale-metrics-86449c4f7d-qtrwd   1/1     Running   0          12m   10.244.70.138   sof-zzbd7   <none>           <none>
polyscale-metrics-86449c4f7d-sgqs9   1/1     Running   0          12m   10.244.48.80    rio-kz6s3   <none>           <none>
```

And pick one of the pod names to see its logs using `kubectl logs POD`.

```
$ kubectl logs polyscale-metrics-86449c4f7d-sgqs9
Node lmn-rio-k1-shared-ingress12 listening to /metrics on port :2112 interval 300 s query select * from foodsales limit 1;
nodename lmn-rio-k1-shared-ingress12 cache 618ms origin 634ms 
nodename lmn-rio-k1-shared-ingress12 cache 9ms origin 316ms 
nodename lmn-rio-k1-shared-ingress12 cache 9ms origin 316ms 
```

Explore the other pods as well to see the speed of the cache from other locations.

That's it!

## Move the Workload Around
Use the [location optimizer](/guides/projects/set-edge-locations.md) to cause your project to move to other locations so that you can see how Section and PolyScale work together.

## Extra Credit: Metrics in Grafana
Our GoLang app exposes a metrics endpoint so that you can scrape p50, p90, and p95 query latencies into a 3rd party metrics system such as Grafana Cloud. Read more in our [tutorial](/guides/monitor/exporting-telemetry/grafana-section/) about scraping metrics into Grafana.

Replace items in the `remote_write` section of the ConfigMap:

```yaml title=""grafana-app-agent-configmap.yaml""
...
    remote_write:
    - url: GRAFANA_METRICS_INSTANCE_REMOTE_WRITE_ENDPOINT
      basic_auth:
        username: GRAFANA_METRICS_INSTANCE_ID
        password: GRAFANA_API_KEY
```

And apply with `kubectl apply -f grafana-app-scrape-configmap.yaml`.

In the Deployment resource no substitutions required. So just apply with `kubectl apply -f grafana-app-agent-deployment.yaml`.

And finally, in order for the Grafana agent to contact the polyscale-metrics-service pod, you'll need a Kubernetes Service resource. We've provided that for you, so just apply with `kubectl apply -f polyscale-metrics-service.yaml`. This exposes `http://polyscale-metrics-service:80` within your Section project so that the Grafana agent can access the metrics endpoint.

Then go to the Explore menu in Grafana Cloud to start charting your metrics!",1645
Distributed Data on Section,Ephemeral Volumes,"Whenever your container opens a file for writing, you are using Section's ephemeral volumes. This storage goes away when the pod terminates.",27
Distributed Data on Section,Persistent Volumes,"By using a [Persistent Volume Claim](/explanations/persistent-volumes.md) you can share a filesystem across multiple pods. Using this technology you can share virtually any data technology between pods, such as relational databases, document stores, object stores, key-value stores, and message queues. Sharing across pods can occur in the following instances:
* horizontal scaling of a pod, so that the multiple replicas have access to shared data
* different pods of a microservice application, giving those pods a common source of truth for whatever data they might need
* data that needs to survive a pod that crashes and restarts

Check out our tutorial to see how this works for a [Postgres deployment](/tutorials/data/postgres-on-pvc).",151
Distributed Data on Section,Streaming Database Backups,"Depending upon your circumstances, when you house your database at an edge location then you may wish to stream a backup to a more permanent location such as AWS S3, Azure Blob Storage, Wasabi, Exoscale, etc. [Litestream.io](https://litestream.io/) supports this idea for SQLite.",65
Distributed Data on Section,Re-populating a Database as Section Relocates It,"Streaming backup technology, such as Litestream.io mentioned previously, can also be used to repopulate a database as Section moves your project from one region to another.",34
Distributed Data on Section,Synchronized Databases,Strategies for sharing data between locations include configuring replication to occur between persistent volumes residing in different locations. We have a guide that explains how you can set this up (coming). You might choose to use a static location configuration for your project and then replicate data between them in order to provide a globally-distributed persistent data.,64
Distributed Data on Section,Serverless Data Integrations,"Below are a few managed serverless data offerings that we are aware of at Section. We've written tutorials for a few. Almost all offer some kind of multi-region replication, which is useful for disaster recovery, high-availability, and low-latency reads for better performance for geographically distributed end users. Those that support multi-region writes are listed separately. The data landscape has [lots of options](https://db-engines.com/) to choose from, many of them [free for limited usage](https://free-for.dev/#/?id=dbaas).

<div class=""table--white-space-normal"">

| Data Type | Single-Region Write | Multi-Region Write |
|-----|--------------------|---------|
|Relational (SQL)|[Supabase](https://supabase.com), [Neon](https://neon.tech), [Aiven for PostgreSQL](https://aiven.io/postgresql), [bit.io](https://bit.io/) (Postgres compatible)|[Polyscale.ai](/tutorials/data/polyscale-caching.md), [CockroachDB Serverless](https://cockroachlabs.cloud) (multi-region write is in preview)|
|Document (NoSQL)|[MongoDB Atlas](https://www.mongodb.com/atlas)|[FaunaDB](https://fauna.com/), [DataStax Astra DB](https://www.datastax.com/products/datastax-astra) (Cassandra-based), [HarperDB](https://harperdb.io/), [Couchbase Capella](https://www.couchbase.com/products/capella)|
|Key-value|[ZippyDB](https://www.zippydb.com/), [Aiven Redis](https://aiven.io/redis), [ScaleGrid Redis](https://scalegrid.io/redis)|[Upstash Redis](https://upstash.com/)|
|Object storage|[Wasabi](https://wasabi.com), [Backblaze](https://backblaze.com)|[Synadia Jetstream Object Store](https://synadia.com/ngs) (NATS.io-based)|
|Message Queue|[Upstash Kafka](https://upstash.com/kafka)|[Synadia](https://synadia.com/ngs) (NATS.io-based)|
||                    ||

</div>",510
Distributed Data on Section,Learn how to support all your data needs on Section,"
## Native Data on Section
### Ephemeral Volumes
Whenever your container opens a file for writing, you are using Section's ephemeral volumes. This storage goes away when the pod terminates.
### Persistent Volumes
By using a [Persistent Volume Claim](/explanations/persistent-volumes.md) you can share a filesystem across multiple pods. Using this technology you can share virtually any data technology between pods, such as relational databases, document stores, object stores, key-value stores, and message queues. Sharing across pods can occur in the following instances:
* horizontal scaling of a pod, so that the multiple replicas have access to shared data
* different pods of a microservice application, giving those pods a common source of truth for whatever data they might need
* data that needs to survive a pod that crashes and restarts

Check out our tutorial to see how this works for a [Postgres deployment](/tutorials/data/postgres-on-pvc).
### Streaming Database Backups
Depending upon your circumstances, when you house your database at an edge location then you may wish to stream a backup to a more permanent location such as AWS S3, Azure Blob Storage, Wasabi, Exoscale, etc. [Litestream.io](https://litestream.io/) supports this idea for SQLite.
### Re-populating a Database as Section Relocates It
Streaming backup technology, such as Litestream.io mentioned previously, can also be used to repopulate a database as Section moves your project from one region to another.
### Synchronized Databases
Strategies for sharing data between locations include configuring replication to occur between persistent volumes residing in different locations. We have a guide that explains how you can set this up (coming). You might choose to use a static location configuration for your project and then replicate data between them in order to provide a globally-distributed persistent data.
## Serverless Data Integrations
Below are a few managed serverless data offerings that we are aware of at Section. We've written tutorials for a few. Almost all offer some kind of multi-region replication, which is useful for disaster recovery, high-availability, and low-latency reads for better performance for geographically distributed end users. Those that support multi-region writes are listed separately. The data landscape has [lots of options](https://db-engines.com/) to choose from, many of them [free for limited usage](https://free-for.dev/#/?id=dbaas).

<div class=""table--white-space-normal"">

| Data Type | Single-Region Write | Multi-Region Write |
|-----|--------------------|---------|
|Relational (SQL)|[Supabase](https://supabase.com), [Neon](https://neon.tech), [Aiven for PostgreSQL](https://aiven.io/postgresql), [bit.io](https://bit.io/) (Postgres compatible)|[Polyscale.ai](/tutorials/data/polyscale-caching.md), [CockroachDB Serverless](https://cockroachlabs.cloud) (multi-region write is in preview)|
|Document (NoSQL)|[MongoDB Atlas](https://www.mongodb.com/atlas)|[FaunaDB](https://fauna.com/), [DataStax Astra DB](https://www.datastax.com/products/datastax-astra) (Cassandra-based), [HarperDB](https://harperdb.io/), [Couchbase Capella](https://www.couchbase.com/products/capella)|
|Key-value|[ZippyDB](https://www.zippydb.com/), [Aiven Redis](https://aiven.io/redis), [ScaleGrid Redis](https://scalegrid.io/redis)|[Upstash Redis](https://upstash.com/)|
|Object storage|[Wasabi](https://wasabi.com), [Backblaze](https://backblaze.com)|[Synadia Jetstream Object Store](https://synadia.com/ngs) (NATS.io-based)|
|Message Queue|[Upstash Kafka](https://upstash.com/kafka)|[Synadia](https://synadia.com/ngs) (NATS.io-based)|
||                    ||

</div>",902
Persistent Volumes,Create and Mount a Persistent Volume Claim,"Cloud applications often require a place to persist data. Containers inside pods have ephemeral filesystems that are lost when a pod restarts or terminates. [Persistent volumes](/explanations/persistent-volumes.md) solve this problem by allowing data to persist beyond a pod's lifetime. This tutorial explains how to create a Persistent Volume Claim, mount it in a pod, and then demonstrate that it works across two replicas of a pod.",87
Persistent Volumes,Make the Claim,"First, here is the yaml the makes a claim to a volume. It will be dynamically created once it is mounted.
* The [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) supported by Section is `nfs-client`.
* The size of my volume below is set to 30Mi.
* The [access mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) for a volume intended to be shared between many pods should be set to `ReadWriteMany`.
* The [volume mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode) is set to `Filesystem`, which mounts it as a directory in the pod's filesystem.

```yaml title=""pvc-claim.yaml""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nginx
  namespace: default
spec:
  storageClassName: nfs-client
  resources:
    requests:
      storage: 30Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany # access by many pods
```

Apply and check the PersistentVolumeClaim resource.

```
$ kubectl apply -f pvc-claim.yaml
persistentvolumeclaim/pvc-nginx created
$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-nginx   Pending                                      nfs-client     14m
$ 
```

## Mount It
Next, we'll mount the claim in a simple NGINX deployment. This will cause the volume to be created dynamically at this time. Things to note:
* We've specifically asked for 2 replicas so that we can demonstrate access to the volume from each replica.
* We've asked for the volume to be mounted in our pod's filesystem at `/tmp/nginx-data`.

```yaml title=""pvc-mount.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - name: nginx-data
            mountPath: /tmp/nginx-data/
        resources:
          requests:
            cpu: 150m
            memory: 100Mi
          limits:
            cpu: 150m
            memory: 100Mi
      volumes:
        - name: nginx-data
          persistentVolumeClaim:
            claimName: pvc-nginx
```

Apply and check the deployment resource.

```
$ kubectl apply -f pvc-mount.yaml 
deployment.apps/nginx created
$ kubectl get pods -o wide
NAME                     READY   STATUS              RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES
nginx-65bbb4d46c-2rwl8   0/1     ContainerCreating   0          72s   <none>          nyc-adljs   <none>           <none>
nginx-65bbb4d46c-4hp4s   1/1     Running             0          72s   10.244.9.148    sfo-gwuie   <none>           <none>
nginx-65bbb4d46c-dng7q   1/1     Running             0          72s   10.245.160.39   nyc-adljs   <none>           <none>
nginx-65bbb4d46c-mg9pr   1/1     Running             0          71s   10.245.182.89   sfo-gwuie   <none>           <none>
$ 
```",823
Persistent Volumes,Test It,"Now, let's exec into a pod running in the NYC node. We'll place a file there, and then demonstrate that it is also available in the other pod in the NYC node.

```
$ kubectl exec -it nginx-65bbb4d46c-dng7q -- sh
# cd /tmp/nginx-data
# echo hi there > file.txt
# ls -l
total 4
-rw-r--r-- 1 nobody nogroup 9 Jan  4 22:09 file.txt
# exit
$
```

Let's now exec into the other pod running in the NYC node and check for that same file.

```
$ kubectl exec -it nginx-65bbb4d46c-2rwl8 -- sh
# cd /tmp/nginx-data
# ls -l
total 4
-rw-r--r-- 1 nobody nogroup 9 Jan  4 22:09 file.txt
# cat file.txt
hi there
# exit
$
```

Success!

Finally, in order to shut everything down, we first need to delete the dployment, so that no pods have the volume mounted. Then we need to delete the PVC itself.

```
$ kubectl delete deploy nginx
deployment.apps ""nginx"" deleted
$ kubectl delete pvc
persistentvolumeclaime/pvc-nginx deleted
$
```

Alternately, you can delete those resources from the [Kubernetes dashboard](/guides/kubernetes-ui/dashboard.md) as well.

And that's it!

Next, try installing [Postgres on your PVC](/tutorials/data/postgres-on-pvc.md).",340
Persistent Volumes,Learn how to create and mount a persistent volume claim for use by your pods,"
# Create and Mount a Persistent Volume Claim

Cloud applications often require a place to persist data. Containers inside pods have ephemeral filesystems that are lost when a pod restarts or terminates. [Persistent volumes](/explanations/persistent-volumes.md) solve this problem by allowing data to persist beyond a pod's lifetime. This tutorial explains how to create a Persistent Volume Claim, mount it in a pod, and then demonstrate that it works across two replicas of a pod.

## Make the Claim
First, here is the yaml the makes a claim to a volume. It will be dynamically created once it is mounted.
* The [storage class](https://kubernetes.io/docs/concepts/storage/storage-classes/) supported by Section is `nfs-client`.
* The size of my volume below is set to 30Mi.
* The [access mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) for a volume intended to be shared between many pods should be set to `ReadWriteMany`.
* The [volume mode](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode) is set to `Filesystem`, which mounts it as a directory in the pod's filesystem.

```yaml title=""pvc-claim.yaml""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nginx
  namespace: default
spec:
  storageClassName: nfs-client
  resources:
    requests:
      storage: 30Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany # access by many pods
```

Apply and check the PersistentVolumeClaim resource.

```
$ kubectl apply -f pvc-claim.yaml
persistentvolumeclaim/pvc-nginx created
$ kubectl get pvc
NAME        STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
pvc-nginx   Pending                                      nfs-client     14m
$ 
```

## Mount It
Next, we'll mount the claim in a simple NGINX deployment. This will cause the volume to be created dynamically at this time. Things to note:
* We've specifically asked for 2 replicas so that we can demonstrate access to the volume from each replica.
* We've asked for the volume to be mounted in our pod's filesystem at `/tmp/nginx-data`.

```yaml title=""pvc-mount.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: default
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - name: nginx-data
            mountPath: /tmp/nginx-data/
        resources:
          requests:
            cpu: 150m
            memory: 100Mi
          limits:
            cpu: 150m
            memory: 100Mi
      volumes:
        - name: nginx-data
          persistentVolumeClaim:
            claimName: pvc-nginx
```

Apply and check the deployment resource.

```
$ kubectl apply -f pvc-mount.yaml 
deployment.apps/nginx created
$ kubectl get pods -o wide
NAME                     READY   STATUS              RESTARTS   AGE   IP              NODE        NOMINATED NODE   READINESS GATES
nginx-65bbb4d46c-2rwl8   0/1     ContainerCreating   0          72s   <none>          nyc-adljs   <none>           <none>
nginx-65bbb4d46c-4hp4s   1/1     Running             0          72s   10.244.9.148    sfo-gwuie   <none>           <none>
nginx-65bbb4d46c-dng7q   1/1     Running             0          72s   10.245.160.39   nyc-adljs   <none>           <none>
nginx-65bbb4d46c-mg9pr   1/1     Running             0          71s   10.245.182.89   sfo-gwuie   <none>           <none>
$ 
```

## Test It
Now, let's exec into a pod running in the NYC node. We'll place a file there, and then demonstrate that it is also available in the other pod in the NYC node.

```
$ kubectl exec -it nginx-65bbb4d46c-dng7q -- sh
# cd /tmp/nginx-data
# echo hi there > file.txt
# ls -l
total 4
-rw-r--r-- 1 nobody nogroup 9 Jan  4 22:09 file.txt
# exit
$
```

Let's now exec into the other pod running in the NYC node and check for that same file.

```
$ kubectl exec -it nginx-65bbb4d46c-2rwl8 -- sh
# cd /tmp/nginx-data
# ls -l
total 4
-rw-r--r-- 1 nobody nogroup 9 Jan  4 22:09 file.txt
# cat file.txt
hi there
# exit
$
```

Success!

Finally, in order to shut everything down, we first need to delete the dployment, so that no pods have the volume mounted. Then we need to delete the PVC itself.

```
$ kubectl delete deploy nginx
deployment.apps ""nginx"" deleted
$ kubectl delete pvc
persistentvolumeclaime/pvc-nginx deleted
$
```

Alternately, you can delete those resources from the [Kubernetes dashboard](/guides/kubernetes-ui/dashboard.md) as well.

And that's it!

Next, try installing [Postgres on your PVC](/tutorials/data/postgres-on-pvc.md).",1270
Postgres on a PVC,Create a Postgres Database using a PVC,"Cloud applications often require a place to persist relational data. Containers inside pods have ephemeral filesystems that are lost when a pod restarts or terminates. [Persistent volumes](/explanations/persistent-volumes.md) solve this problem by allowing data to persist beyond a pod's lifetime. This tutorial explains how to create a Postgres database on a persistent volume, and then demonstrates that the data survives the restart of the Postgres pod.

Note that the PVC data won't move if Section [moves your project](/explanations/aee) to a new location. See [here](/tutorials/data/distributed-data/#re-populating-a-database-as-section-relocates-it) for strategies to handle this. Or upgrade to our [Pro Plan](/explanations/billing/#standard-and-pro-plans) to allow you to specify a [static](/explanations/aee/#static) location configuration.

You'll create the following resources:
* a `location-optimizer` ConfigMap to specify a single location
* a persistent volume claim for the database, `postgres-pvc`
* a Postgres deployment, `postgres-deployment`, using the official [Postgres image](https://hub.docker.com/_/postgres) on Docker Hub
* a Service, `postgres-service`, that points to the Postgres pod",271
Postgres on a PVC,Run in a Single Location,"You'll likely want your database to [run in a single location](/guides/projects/set-edge-locations/), so specify that your project should have `maximumLocations` of 1 in your `location-optimizer` ConfigMap resource:

```yaml title=""postgres-single-location.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```

## Create the PVC

Next, create the PersistentVolumeClaim to hold the database.

```yaml title=""postgres-pvc-claim.yaml""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  storageClassName: nfs-client
  resources:
    requests:
      storage: 50Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
```

Apply and check the PersistentVolumeClaim resource.

```
$ kubectl apply -f postgres-pvc-claim.yaml
persistentvolumeclaim/postgres-pvc created
$ kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
postgres-pvc  Pending                                      nfs-client     14m
$ 
```",295
Postgres on a PVC,Create the Deployment,"Notes about the deployment:
* Write-replicas, which you are creating below, must be no more than 1. If you need horizontal scale then you can make a separate deployment of read replicas.
* The PVC is mounted at `/var/lib/postgresql/data`, and then we place the database files in the `k8s` folder underneath. The `k8s` subfolder and `runAsUser` are specified so that the Postgres container works properly with the security of the NFS volume supporting the PVC.
* In production you should use a Secret resource instead of supplying a password directly.

```yaml title=""postgres-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1 # must be no more than 1
  selector:
    matchLabels:
      app: postgres-deployment
  template:
    metadata:
      labels:
        app: postgres-deployment
    spec:
      securityContext:
        runAsUser: 999
      containers:
        - name: postgres
          image: postgres
          imagePullPolicy: Always
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_PASSWORD
              value: UseASecretInstead
            - name: PGDATA
              value: /var/lib/postgresql/data/k8s
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "".5Gi""
              cpu: ""500m""
            limits:
              memory: "".5Gi""
              cpu: ""500m""
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: postgres-pvc
```

Apply and check the Deployment resource.

```
$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE    IP              NODE        NOMINATED NODE   READINESS GATES
postgres-deployment-74fffd4c8b-98sf9   1/1     Running   0          6m7s   10.244.73.223   sfo-gwuie   <none>           <none>
postgres-deployment-74fffd4c8b-d6d5c   1/1     Running   0          6m7s   10.246.184.74   nyc-adljs   <none>           <none>
$ 
```

## Create the Service

```yaml title=""postgres-service.yaml""
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  ports:
    - port: 5432
  selector:
    app: postgres-deployment
```

Apply and check the Service resource.

```
$ kubectl apply -f postgres-service.yaml
service/postgres-service created
$ kubectl get service
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
ingress-upstream   ClusterIP   10.43.209.247   <none>        80/TCP     25h
kubernetes         ClusterIP   10.43.0.1       <none>        443/TCP    25h
postgres-service   ClusterIP   10.43.227.23    <none>        5432/TCP   6s
$ 
```

The Service gives you a hostname `postgres-service` that points to your deployment, one that you will use with the `-h` argument of `psql` in the next section. This hostname also insulates you from the fact that when the pod restarts it might come back with a different IP address.",786
Postgres on a PVC,Test It,"We'll exec into the SFO pod, run `psql`, make a table, and do a query.

```
$ kubectl exec -it postgres-deployment-74fffd4c8b-98sf9 -- sh
postgres-deployment-74fffd4c8b-98sf9$ psql -h postgres-service
Password for user postgres: XXXXXXXXXXXXX
psql (15.1 (Debian 15.1-1.pgdg110+1))
Type ""help"" for help.

postgres=# \dt
Did not find any relations.
postgres=# CREATE TABLE PETS(
ID SERIAL PRIMARY KEY NOT NULL,
NAME TEXT NOT NULL UNIQUE,
ANIMAL TEXT NOT NULL
);
CREATE TABLE

postgres=# INSERT INTO PETS (NAME,ANIMAL) VALUES('JED','CAT');
INSERT 0 1

postgres=# INSERT INTO PETS (NAME,ANIMAL) VALUES('BUCKLEY','DOG');
INSERT 0 1

postgres=# SELECT * FROM PETS;
 id |  name   | animal 
----+---------+--------
  1 | JED     | CAT
  2 | BUCKLEY | DOG
(2 rows)

postgres=# quit
postgres-deployment-74fffd4c8b-98sf9$ exit
```

Now we'll delete the deployment but not the PVC, recreate the deployment, and then validate that the table is still present in the SFO location. This demonstrates that the PVC retained the data.

```
$ kubectl delete deploy postgres-deployment
deployment.apps ""postgres-deployment"" deleted

$ kubectl get pods -o wide
No resources found in default namespace.

$ kubectl apply -f postgres-deployment.yaml 
deployment.apps/postgres-deployment created

$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES
postgres-deployment-74fffd4c8b-2fxsp   1/1     Running   0          18s   10.246.104.111   nyc-adljs   <none>           <none>
postgres-deployment-74fffd4c8b-8chhc   1/1     Running   0          18s   10.244.73.226    sfo-gwuie   <none>           <none>

$ kubectl exec -it postgres-deployment-74fffd4c8b-8chhc -- sh
postgres-deployment-74fffd4c8b-8chhc$ psql -h postgres-service
Password for user postgres: 
psql (15.1 (Debian 15.1-1.pgdg110+1))
Type ""help"" for help.

postgres=# SELECT * FROM PETS;
 id |  name   | animal 
----+---------+--------
  1 | JED     | CAT
  2 | BUCKLEY | DOG
(2 rows)

postgres=# quit
postgres-deployment-74fffd4c8b-8chhc$ exit
$ 
```

And there you have it! Now you can access your Postgres database from within your Section project at host postgres-service, port 5432.",688
Postgres on a PVC,Learn how to create a PostgreSQL database on a persistent volume,"
# Create a Postgres Database using a PVC

Cloud applications often require a place to persist relational data. Containers inside pods have ephemeral filesystems that are lost when a pod restarts or terminates. [Persistent volumes](/explanations/persistent-volumes.md) solve this problem by allowing data to persist beyond a pod's lifetime. This tutorial explains how to create a Postgres database on a persistent volume, and then demonstrates that the data survives the restart of the Postgres pod.

Note that the PVC data won't move if Section [moves your project](/explanations/aee) to a new location. See [here](/tutorials/data/distributed-data/#re-populating-a-database-as-section-relocates-it) for strategies to handle this. Or upgrade to our [Pro Plan](/explanations/billing/#standard-and-pro-plans) to allow you to specify a [static](/explanations/aee/#static) location configuration.

You'll create the following resources:
* a `location-optimizer` ConfigMap to specify a single location
* a persistent volume claim for the database, `postgres-pvc`
* a Postgres deployment, `postgres-deployment`, using the official [Postgres image](https://hub.docker.com/_/postgres) on Docker Hub
* a Service, `postgres-service`, that points to the Postgres pod

## Run in a Single Location
You'll likely want your database to [run in a single location](/guides/projects/set-edge-locations/), so specify that your project should have `maximumLocations` of 1 in your `location-optimizer` ConfigMap resource:

```yaml title=""postgres-single-location.yaml""
apiVersion: v1
kind: ConfigMap
data:
  strategy: |
    {
        ""strategy"": ""SolverServiceV1"",
        ""params"": {
            ""policy"": ""dynamic"",
            ""minimumLocations"": 1,
            ""maximumLocations"": 1
        }
    }
metadata:
  name: location-optimizer
```

## Create the PVC

Next, create the PersistentVolumeClaim to hold the database.

```yaml title=""postgres-pvc-claim.yaml""
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  storageClassName: nfs-client
  resources:
    requests:
      storage: 50Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
```

Apply and check the PersistentVolumeClaim resource.

```
$ kubectl apply -f postgres-pvc-claim.yaml
persistentvolumeclaim/postgres-pvc created
$ kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
postgres-pvc  Pending                                      nfs-client     14m
$ 
```

## Create the Deployment

Notes about the deployment:
* Write-replicas, which you are creating below, must be no more than 1. If you need horizontal scale then you can make a separate deployment of read replicas.
* The PVC is mounted at `/var/lib/postgresql/data`, and then we place the database files in the `k8s` folder underneath. The `k8s` subfolder and `runAsUser` are specified so that the Postgres container works properly with the security of the NFS volume supporting the PVC.
* In production you should use a Secret resource instead of supplying a password directly.

```yaml title=""postgres-deployment.yaml""
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres-deployment
spec:
  replicas: 1 # must be no more than 1
  selector:
    matchLabels:
      app: postgres-deployment
  template:
    metadata:
      labels:
        app: postgres-deployment
    spec:
      securityContext:
        runAsUser: 999
      containers:
        - name: postgres
          image: postgres
          imagePullPolicy: Always
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_PASSWORD
              value: UseASecretInstead
            - name: PGDATA
              value: /var/lib/postgresql/data/k8s
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
          resources:
            requests:
              memory: "".5Gi""
              cpu: ""500m""
            limits:
              memory: "".5Gi""
              cpu: ""500m""
      volumes:
        - name: postgres-data
          persistentVolumeClaim:
            claimName: postgres-pvc
```

Apply and check the Deployment resource.

```
$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE    IP              NODE        NOMINATED NODE   READINESS GATES
postgres-deployment-74fffd4c8b-98sf9   1/1     Running   0          6m7s   10.244.73.223   sfo-gwuie   <none>           <none>
postgres-deployment-74fffd4c8b-d6d5c   1/1     Running   0          6m7s   10.246.184.74   nyc-adljs   <none>           <none>
$ 
```

## Create the Service

```yaml title=""postgres-service.yaml""
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  ports:
    - port: 5432
  selector:
    app: postgres-deployment
```

Apply and check the Service resource.

```
$ kubectl apply -f postgres-service.yaml
service/postgres-service created
$ kubectl get service
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
ingress-upstream   ClusterIP   10.43.209.247   <none>        80/TCP     25h
kubernetes         ClusterIP   10.43.0.1       <none>        443/TCP    25h
postgres-service   ClusterIP   10.43.227.23    <none>        5432/TCP   6s
$ 
```

The Service gives you a hostname `postgres-service` that points to your deployment, one that you will use with the `-h` argument of `psql` in the next section. This hostname also insulates you from the fact that when the pod restarts it might come back with a different IP address.

## Test It
We'll exec into the SFO pod, run `psql`, make a table, and do a query.

```
$ kubectl exec -it postgres-deployment-74fffd4c8b-98sf9 -- sh
postgres-deployment-74fffd4c8b-98sf9$ psql -h postgres-service
Password for user postgres: XXXXXXXXXXXXX
psql (15.1 (Debian 15.1-1.pgdg110+1))
Type ""help"" for help.

postgres=# \dt
Did not find any relations.
postgres=# CREATE TABLE PETS(
ID SERIAL PRIMARY KEY NOT NULL,
NAME TEXT NOT NULL UNIQUE,
ANIMAL TEXT NOT NULL
);
CREATE TABLE

postgres=# INSERT INTO PETS (NAME,ANIMAL) VALUES('JED','CAT');
INSERT 0 1

postgres=# INSERT INTO PETS (NAME,ANIMAL) VALUES('BUCKLEY','DOG');
INSERT 0 1

postgres=# SELECT * FROM PETS;
 id |  name   | animal 
----+---------+--------
  1 | JED     | CAT
  2 | BUCKLEY | DOG
(2 rows)

postgres=# quit
postgres-deployment-74fffd4c8b-98sf9$ exit
```

Now we'll delete the deployment but not the PVC, recreate the deployment, and then validate that the table is still present in the SFO location. This demonstrates that the PVC retained the data.

```
$ kubectl delete deploy postgres-deployment
deployment.apps ""postgres-deployment"" deleted

$ kubectl get pods -o wide
No resources found in default namespace.

$ kubectl apply -f postgres-deployment.yaml 
deployment.apps/postgres-deployment created

$ kubectl get pods -o wide
NAME                                   READY   STATUS    RESTARTS   AGE   IP               NODE        NOMINATED NODE   READINESS GATES
postgres-deployment-74fffd4c8b-2fxsp   1/1     Running   0          18s   10.246.104.111   nyc-adljs   <none>           <none>
postgres-deployment-74fffd4c8b-8chhc   1/1     Running   0          18s   10.244.73.226    sfo-gwuie   <none>           <none>

$ kubectl exec -it postgres-deployment-74fffd4c8b-8chhc -- sh
postgres-deployment-74fffd4c8b-8chhc$ psql -h postgres-service
Password for user postgres: 
psql (15.1 (Debian 15.1-1.pgdg110+1))
Type ""help"" for help.

postgres=# SELECT * FROM PETS;
 id |  name   | animal 
----+---------+--------
  1 | JED     | CAT
  2 | BUCKLEY | DOG
(2 rows)

postgres=# quit
postgres-deployment-74fffd4c8b-8chhc$ exit
$ 
```

And there you have it! Now you can access your Postgres database from within your Section project at host postgres-service, port 5432.",2069
Solver Service Parameters,Solver Service Parameters A reference of available location options for Section's AEE,"The following table provides all available location options you can use to define the location strategy when configuring your environment.

| Parameter | Value | Value Name |
|---|---|---|
| locationcode | ams | Amsterdam, Netherlands |
| locationcode | atl | Atlanta GA, USA |
| locationcode | bah | Bahrain | 
| locationcode | ord | Chicago IL, USA |
| locationcode | cmh | Columbus OH, USA |
| locationcode | dfw | Dallas TX, USA |
| locationcode | den | Denver CO, USA |
| locationcode | dus | Dusseldorf, Germany |
| locationcode | hkg | Hong Kong | 
| locationcode | lon | London, UK |
| locationcode | mad | Madrid, Spain |
| locationcode | mel | Melbourne VIC, AU |
| locationcode | mia | Miami FL, USA |
| locationcode | ewr | Newark NJ, USA |
| locationcode | nyc | New York NY, USA |
| locationcode | cdg | Paris, France |
| locationcode | per | Perth WA, AU |
| locationcode | pdx | Portland OR, USA |
| locationcode | sfo | San Francisco CA, USA |
| locationcode | sjc | San Jose CA, USA |
| locationcode | gru | Sao Paolo, Brazil |
| locationcode | sea | Seattle WA, USA |
| locationcode | sin | Singapore | 
| locationcode | arn | Stockholm, Sweden |
| locationcode | syd | Sydney NSW, AU |
| locationcode | hnd | Tokyo, Japan |
| locationcode | iad | Washington DC, USA |
| locationcode | bna | Buenos Aires, Argentina |
| locationcode | rio | Rio de Janeiro, Brazil |
| region | asia | Asia |
| region | europe | Europe |
| region | middleeast | Middle East |
| region | northamerica | North America |
| region | oceania | Oceania |
| region | southamerica | South America |",419
Solver Service Parameters,A reference of available location options for Section's AEE,"
The following table provides all available location options you can use to define the location strategy when configuring your environment.

| Parameter | Value | Value Name |
|---|---|---|
| locationcode | ams | Amsterdam, Netherlands |
| locationcode | atl | Atlanta GA, USA |
| locationcode | bah | Bahrain | 
| locationcode | ord | Chicago IL, USA |
| locationcode | cmh | Columbus OH, USA |
| locationcode | dfw | Dallas TX, USA |
| locationcode | den | Denver CO, USA |
| locationcode | dus | Dusseldorf, Germany |
| locationcode | hkg | Hong Kong | 
| locationcode | lon | London, UK |
| locationcode | mad | Madrid, Spain |
| locationcode | mel | Melbourne VIC, AU |
| locationcode | mia | Miami FL, USA |
| locationcode | ewr | Newark NJ, USA |
| locationcode | nyc | New York NY, USA |
| locationcode | cdg | Paris, France |
| locationcode | per | Perth WA, AU |
| locationcode | pdx | Portland OR, USA |
| locationcode | sfo | San Francisco CA, USA |
| locationcode | sjc | San Jose CA, USA |
| locationcode | gru | Sao Paolo, Brazil |
| locationcode | sea | Seattle WA, USA |
| locationcode | sin | Singapore | 
| locationcode | arn | Stockholm, Sweden |
| locationcode | syd | Sydney NSW, AU |
| locationcode | hnd | Tokyo, Japan |
| locationcode | iad | Washington DC, USA |
| locationcode | bna | Buenos Aires, Argentina |
| locationcode | rio | Rio de Janeiro, Brazil |
| region | asia | Asia |
| region | europe | Europe |
| region | middleeast | Middle East |
| region | northamerica | North America |
| region | oceania | Oceania |
| region | southamerica | South America |
",420
kubectl Supported Commands,kubectl Supported Commands A reference of available kubectl commands when working with Section,"The following are the available kubectl commands when working with Section.

<div class=""table--white-space-normal"">

| kubectl commands | Objects it applies to                             | Description |
|---|---------------------------------------------------|---|
| [get](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get) | pod, service, deployment, hpa.v2beta2.autoscaling | Prints a table of the most important information about the specified resources. |
| [create](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#create) | pod, service, deployment, hpa.v2beta2.autoscaling | Create a resource from a file or from stdin. |
| [edit](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#edit) | pod, service, deployment, hpa.v2beta2.autoscaling | Edit a resource from the default editor. |
| [delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete) | pod, service, deployment, hpa.v2beta2.autoscaling | Delete resources by file names, stdin, resources and names, or by resources and label selector. |
| [apply](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#apply) | pod, service, deployment, hpa.v2beta2.autoscaling                        | Apply a configuration to a resource by file name or stdin. |
| [describe](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe) | node, pod, service, deployment                    | Show details of a specific resource or group of resources. |
| [logs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs) | pod                                               | Print the logs for a container in a pod or specified resource. |
| [exec](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#exec) | pod                                               | Execute a command in a container. |
| [attach](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#attach) | pod                                               | Attach to a process that is already running inside an existing container. |

</div>",456
kubectl Supported Commands,A reference of available kubectl commands when working with Section,"
The following are the available kubectl commands when working with Section.

<div class=""table--white-space-normal"">

| kubectl commands | Objects it applies to                             | Description |
|---|---------------------------------------------------|---|
| [get](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get) | pod, service, deployment, hpa.v2beta2.autoscaling | Prints a table of the most important information about the specified resources. |
| [create](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#create) | pod, service, deployment, hpa.v2beta2.autoscaling | Create a resource from a file or from stdin. |
| [edit](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#edit) | pod, service, deployment, hpa.v2beta2.autoscaling | Edit a resource from the default editor. |
| [delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete) | pod, service, deployment, hpa.v2beta2.autoscaling | Delete resources by file names, stdin, resources and names, or by resources and label selector. |
| [apply](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#apply) | pod, service, deployment, hpa.v2beta2.autoscaling                        | Apply a configuration to a resource by file name or stdin. |
| [describe](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#describe) | node, pod, service, deployment                    | Show details of a specific resource or group of resources. |
| [logs](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs) | pod                                               | Print the logs for a container in a pod or specified resource. |
| [exec](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#exec) | pod                                               | Execute a command in a container. |
| [attach](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#attach) | pod                                               | Attach to a process that is already running inside an existing container. |

</div>
",457
Section Metrics Reference,Section Metrics Reference A reference of metrics and labels generated by the Section Platform,"The following metrics are available from the Section Platform in OpenMetrics format for export to third part systems such as Datadog and Grafana Cloud, etc. See our [guide](/guides/monitor/exporting-telemetry/) for examples on how to set this up.

<div class=""table--white-space-normal"">

| Metric | Definition |
|---|---|
| section_container_cpu_usage_seconds_total | From container_cpu_usage_seconds_total. This is the actual CPU usage of a pod, in vCPU (Kubernetes ""cores""), averaged over the last 1 minute. It is a sum of all replicas of a pod in a given popname (see ""popname"" in the labels below). |
| section_container_memory_usage_bytes | From container_memory_usage_bytes. This is the actual memory usage, in GiB, averaged over the last 1 minute. |
| section_http_bytes_total_sum_rate (section_http_bytes_total:sum_rate) | This is an HTTP byte transmission rate, request bytes per second, calculated over a 2 minute span. It is called a ""sum_rate"" because it is doing its calculation over all the replicas of a given container that are running for that environment. In other words, it is a sum of all bytes in all replicas. |
| section_http_request_count_total_sum_rate (section_http_request_count_total:sum_rate) | This is an HTTP request count rate, requests per second, calculated over a 5 minute span. It is called a ""sum_rate"" because it is doing its calculation over all the replicas of a given container that are running for that environment. In other words, it is a sum of all requests in all replicas. |
| section_kube_pod_container_resource_requests | Comes from kube_pod_container_resource_requests, this gives either memory or cpu requests, depending upon the value of the labels ""resource"" or ""unit"", below. |
| section_kube_pod_status_phase | From kube_pod_status_phase |
| section_kube_pod_status_ready | From kube_pod_status_ready |

</div>

<div class=""table--white-space-normal"">

| Label | Definition |
|---|---|
| popname | This is a location in the form xxx-yyyyyyyy, where xxx is a three letter airport code (with a few exceptions), along with a yyyyyyyy hash. |
| endpoint | Deprecated |
| geo_hash | The [GeoHash](https://chrishewett.com/blog/geohash-explorer/) from where the end user is making their request |
| host | Deprecated |
| instance | Deprecated |
| job | Deprecated |
| prometheus | Deprecated |
| prometheus_replica | Deprecated |
| section_io_account_id | Section Account ID |
| section_io_application_id | Section Application ID |
| section_io_environment_id | Section Environment ID |
| section_io_module_name | The Section module for our Better CDN product |
| status_bucket | HTTP status codes from Section to the end user for our Better CDN product |
| traffic_monitor_region | The region that is handling the traffic |
| upstream_label | Deprecated |
| varnish_handling | Deprecated |
| resource | This is ""memory"" or ""cpu"" |
| unit | This is ""byte"" for resource=""memory"", and ""core"" for resource=""cpu"" |

</div>",669
Section Metrics Reference,A reference of metrics and labels generated by the Section Platform,"
The following metrics are available from the Section Platform in OpenMetrics format for export to third part systems such as Datadog and Grafana Cloud, etc. See our [guide](/guides/monitor/exporting-telemetry/) for examples on how to set this up.

<div class=""table--white-space-normal"">

| Metric | Definition |
|---|---|
| section_container_cpu_usage_seconds_total | From container_cpu_usage_seconds_total. This is the actual CPU usage of a pod, in vCPU (Kubernetes ""cores""), averaged over the last 1 minute. It is a sum of all replicas of a pod in a given popname (see ""popname"" in the labels below). |
| section_container_memory_usage_bytes | From container_memory_usage_bytes. This is the actual memory usage, in GiB, averaged over the last 1 minute. |
| section_http_bytes_total_sum_rate (section_http_bytes_total:sum_rate) | This is an HTTP byte transmission rate, request bytes per second, calculated over a 2 minute span. It is called a ""sum_rate"" because it is doing its calculation over all the replicas of a given container that are running for that environment. In other words, it is a sum of all bytes in all replicas. |
| section_http_request_count_total_sum_rate (section_http_request_count_total:sum_rate) | This is an HTTP request count rate, requests per second, calculated over a 5 minute span. It is called a ""sum_rate"" because it is doing its calculation over all the replicas of a given container that are running for that environment. In other words, it is a sum of all requests in all replicas. |
| section_kube_pod_container_resource_requests | Comes from kube_pod_container_resource_requests, this gives either memory or cpu requests, depending upon the value of the labels ""resource"" or ""unit"", below. |
| section_kube_pod_status_phase | From kube_pod_status_phase |
| section_kube_pod_status_ready | From kube_pod_status_ready |

</div>

<div class=""table--white-space-normal"">

| Label | Definition |
|---|---|
| popname | This is a location in the form xxx-yyyyyyyy, where xxx is a three letter airport code (with a few exceptions), along with a yyyyyyyy hash. |
| endpoint | Deprecated |
| geo_hash | The [GeoHash](https://chrishewett.com/blog/geohash-explorer/) from where the end user is making their request |
| host | Deprecated |
| instance | Deprecated |
| job | Deprecated |
| prometheus | Deprecated |
| prometheus_replica | Deprecated |
| section_io_account_id | Section Account ID |
| section_io_application_id | Section Application ID |
| section_io_environment_id | Section Environment ID |
| section_io_module_name | The Section module for our Better CDN product |
| status_bucket | HTTP status codes from Section to the end user for our Better CDN product |
| traffic_monitor_region | The region that is handling the traffic |
| upstream_label | Deprecated |
| varnish_handling | Deprecated |
| resource | This is ""memory"" or ""cpu"" |
| unit | This is ""byte"" for resource=""memory"", and ""core"" for resource=""cpu"" |

</div>
",670
HTTP Ingress,HTTP Ingress Overview,"Section HTTP Ingress is implemented by an enhanced Kubernetes Ingress Controller that is built upon the open source [Nginx Ingress](https://github.com/kubernetes/ingress-nginx). Section's enhancements give application developers special capabilities such as IP blocking, handling of SSL certificates, maintenance pages, and HTTP request metadata enhancement with geo IP properties of the end user.


Section HTTP Ingress has has the following responsibilities:
* Performing the TLS handshake for HTTPS connections
* Request enrichment
* Routing HTTP requests to your applications and services
* Implementing the HTTP/2 protocol
* Request correlation

This ingress controller supports TCP port 80 for HTTP and 443 for HTTPS.

The ingress will route traffic to your applications and services based on the host header over HTTP/1.1 regardless of the incoming protocol.",162
HTTP Ingress,Activating HTTP Ingress,"HTTP Ingress is enabled by declaring in your environment a single Kubernetes service that you name `ingress-upstream`. Specify the type of Service as ClusterIP. Note that Service resources of type NodePort and LoadBalancer services are not accepted. Note that the standard Kubernetes Ingress resource is not leveraged nor recognized by the Platform.

Check our [guide](/guides/http-extensions/http-ingress/) on this topic.",85
HTTP Ingress,How It Works,"Use of Section HTTP Ingress causes a deployment of small, special ingress pods that will be charged to your account. There are 2 pods in the deployment with horizontal pod autoscaling turned on. This deployment is [charged to your account](https://section.io/pricing/) like any other application containers. You will not be able to see the containers using kubectl.",74
HTTP Ingress,TLS metadata,"Section HTTP Ingress performs the TLS handshake with the client on behalf of your application. Some details of this TLS handshake are then relayed to your application by adding HTTP headers to the request.

An `x-forwarded-proto` header with a value of either `http` or `https` is added to indicate if the client used HTTP (i.e. no TLS) or HTTPS to connect. For HTTPS requests, the following headers will also be present.

* `section-io-tls-protocol` - the TLS protocol version negotiated with the client. Examples include `TLSv1.2` and `TLSv1.3`.
* `section-io-tls-cipher` - the TLS cipher suite negotiated with the client, in OpenSSL naming convention. Examples include `ECDHE-RSA-AES128-GCM-SHA256` and `TLS_AES_256_GCM_SHA384`. For translations to other cipher suite naming conventions see [ciphersuite.info](https://ciphersuite.info/) or [testssl.sh](https://testssl.sh/openssl-iana.mapping.html).",220
HTTP Ingress,IP geolocation,"For each incoming request, the ingress controller will attempt to resolve the client's connecting IP address to a geographic location. The results of the lookup are then exposed as HTTP request headers. These headers are:
* `section-io-geo-country-code` - the [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) country code e.g. `US` for the United States.
* `section-io-geo-country-name` - the country name e.g. `United States`.
* `section-io-geo-asn` - ASN number.
* `section-io-geo-region-code` - examples include `CA` and `VA` for US IPs or `NSW` for Australian IPs.
* `section-io-geo-region-name` - examples include `California` and `Virginia` for US IPs or `New South Wales` for Australian IPs.
* `section-io-geo-city` - examples include `New York`, `Mountain View`, `Sydney`, and `Drummoyne`.
* `section-io-geo-latlon` - the approximate latitude and longitude in the format `-33.8696,151.2099`.
* `section-io-geo-postal-code` - examples include `2047` for Drummoyne, NSW, Australia and `80302` for Boulder, CO, USA.
* `section-io-geo-dma-code` - The Nielsen Designated Market Area ID as used by DoubleClick. Only for US IPs.

The ingress controller leverages the open source [MaxMind GeoIP2](https://dev.maxmind.com/geoip) library for geolocation data.",345
HTTP Ingress,Client IP Detection,"Section adds the `True-Client-IP` HTTP header which is equivalent to the `remote_addr` value. This request header can be used in a number of ways:
* Fraud detection
* Logging client usage
* Rate limiting

Section also sets the [`X-Forwarded-For`](https://en.wikipedia.org/wiki/X-Forwarded-For) header if you need to use that, however this header will often be a list of IP addresses depending on how the request has been proxied prior to arriving at Section and is not always reliable.",112
HTTP Ingress,Request correlation,"When the ingress controller handles each incoming request, a unique identifier is generated and is added to the request via a `section-io-id` HTTP request header. You can use this value to trace the request through your application stack.

The format of the `section-io-id` identifier is subject to change without notice so it should be treated as an opaque string and no meaning should be inferred from its value.",80
HTTP Ingress,Description for Section HTTP Ingress,"
## HTTP Ingress Overview
Section HTTP Ingress is implemented by an enhanced Kubernetes Ingress Controller that is built upon the open source [Nginx Ingress](https://github.com/kubernetes/ingress-nginx). Section's enhancements give application developers special capabilities such as IP blocking, handling of SSL certificates, maintenance pages, and HTTP request metadata enhancement with geo IP properties of the end user.


Section HTTP Ingress has has the following responsibilities:
* Performing the TLS handshake for HTTPS connections
* Request enrichment
* Routing HTTP requests to your applications and services
* Implementing the HTTP/2 protocol
* Request correlation

This ingress controller supports TCP port 80 for HTTP and 443 for HTTPS.

The ingress will route traffic to your applications and services based on the host header over HTTP/1.1 regardless of the incoming protocol.

### Activating HTTP Ingress
HTTP Ingress is enabled by declaring in your environment a single Kubernetes service that you name `ingress-upstream`. Specify the type of Service as ClusterIP. Note that Service resources of type NodePort and LoadBalancer services are not accepted. Note that the standard Kubernetes Ingress resource is not leveraged nor recognized by the Platform.

Check our [guide](/guides/http-extensions/http-ingress/) on this topic.

### How It Works
Use of Section HTTP Ingress causes a deployment of small, special ingress pods that will be charged to your account. There are 2 pods in the deployment with horizontal pod autoscaling turned on. This deployment is [charged to your account](https://section.io/pricing/) like any other application containers. You will not be able to see the containers using kubectl.

### TLS metadata

Section HTTP Ingress performs the TLS handshake with the client on behalf of your application. Some details of this TLS handshake are then relayed to your application by adding HTTP headers to the request.

An `x-forwarded-proto` header with a value of either `http` or `https` is added to indicate if the client used HTTP (i.e. no TLS) or HTTPS to connect. For HTTPS requests, the following headers will also be present.

* `section-io-tls-protocol` - the TLS protocol version negotiated with the client. Examples include `TLSv1.2` and `TLSv1.3`.
* `section-io-tls-cipher` - the TLS cipher suite negotiated with the client, in OpenSSL naming convention. Examples include `ECDHE-RSA-AES128-GCM-SHA256` and `TLS_AES_256_GCM_SHA384`. For translations to other cipher suite naming conventions see [ciphersuite.info](https://ciphersuite.info/) or [testssl.sh](https://testssl.sh/openssl-iana.mapping.html).

### IP geolocation
For each incoming request, the ingress controller will attempt to resolve the client's connecting IP address to a geographic location. The results of the lookup are then exposed as HTTP request headers. These headers are:
* `section-io-geo-country-code` - the [ISO 3166-1 alpha-2](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) country code e.g. `US` for the United States.
* `section-io-geo-country-name` - the country name e.g. `United States`.
* `section-io-geo-asn` - ASN number.
* `section-io-geo-region-code` - examples include `CA` and `VA` for US IPs or `NSW` for Australian IPs.
* `section-io-geo-region-name` - examples include `California` and `Virginia` for US IPs or `New South Wales` for Australian IPs.
* `section-io-geo-city` - examples include `New York`, `Mountain View`, `Sydney`, and `Drummoyne`.
* `section-io-geo-latlon` - the approximate latitude and longitude in the format `-33.8696,151.2099`.
* `section-io-geo-postal-code` - examples include `2047` for Drummoyne, NSW, Australia and `80302` for Boulder, CO, USA.
* `section-io-geo-dma-code` - The Nielsen Designated Market Area ID as used by DoubleClick. Only for US IPs.

The ingress controller leverages the open source [MaxMind GeoIP2](https://dev.maxmind.com/geoip) library for geolocation data.

### Client IP Detection
Section adds the `True-Client-IP` HTTP header which is equivalent to the `remote_addr` value. This request header can be used in a number of ways:
* Fraud detection
* Logging client usage
* Rate limiting

Section also sets the [`X-Forwarded-For`](https://en.wikipedia.org/wiki/X-Forwarded-For) header if you need to use that, however this header will often be a list of IP addresses depending on how the request has been proxied prior to arriving at Section and is not always reliable.

### Request correlation
When the ingress controller handles each incoming request, a unique identifier is generated and is added to the request via a `section-io-id` HTTP request header. You can use this value to trace the request through your application stack.

The format of the `section-io-id` identifier is subject to change without notice so it should be treated as an opaque string and no meaning should be inferred from its value.
",1115
HTTP Egress,HTTP Egress Description for Section HTTP Egress,"The Section HTTP Egress is a Kubernetes deployment that you can add to your KEI environment. It is particularly useful if you are using Section to build a CDN-like application, but may have other applications as well. It provides:

**Accurate DNS resolution for origin services**:

**HTTP keepalive management**: When a client browser visits your website, an HTTP connection opens and closes for every request. Therefore, every page that is downloaded requires a new connection. Websites are typically comprised of hundreds of files, such as images, stylesheets, JavaScript, etc., so the impact can be heavy, thereby causing slow page load times. By enabling the keep-alive header, your website can serve all the resources over a single connection.

Read our [guide](/guides/http-extensions/http-egress/) to learn how to set it up.",170
HTTP Egress,Description for Section HTTP Egress,"
The Section HTTP Egress is a Kubernetes deployment that you can add to your KEI environment. It is particularly useful if you are using Section to build a CDN-like application, but may have other applications as well. It provides:

**Accurate DNS resolution for origin services**:

**HTTP keepalive management**: When a client browser visits your website, an HTTP connection opens and closes for every request. Therefore, every page that is downloaded requires a new connection. Websites are typically comprised of hundreds of files, such as images, stylesheets, JavaScript, etc., so the impact can be heavy, thereby causing slow page load times. By enabling the keep-alive header, your website can serve all the resources over a single connection.

Read our [guide](/guides/http-extensions/http-egress/) to learn how to set it up.
",171
Sumo Logic,Sumo Logic Sumo Logic Integration with Section for metrics and logs,"Learn how Section integrates with Sumo Logic.

* [Logging](/guides/monitor/logs/log-streaming/)",23
Sumo Logic,Sumo Logic Integration with Section for metrics and logs,"Learn how Section integrates with Sumo Logic.

* [Logging](/guides/monitor/logs/log-streaming/)
",23
Datadog,Datadog Datadog Integration with Section for metrics and logs,"Learn how Section integrates with Datadog.

* [Metrics](/guides/monitor/exporting-telemetry/datadog-section/)
* [Logging](/guides/monitor/logs/datadog-logs/)",43
Datadog,Datadog Integration with Section for metrics and logs,"Learn how Section integrates with Datadog.

* [Metrics](/guides/monitor/exporting-telemetry/datadog-section/)
* [Logging](/guides/monitor/logs/datadog-logs/)
",43
Splunk,Splunk Splunk Integration with Section for metrics and logs,"Learn how Section integrates with Splunk.

* [Logging](/guides/monitor/logs/log-streaming/)",22
Splunk,Splunk Integration with Section for metrics and logs,"Learn how Section integrates with Splunk.

* [Logging](/guides/monitor/logs/log-streaming/)
",22
Google Cloud,Google Cloud Google Cloud Integration with Section for metrics and logs,"Learn how Section integrates with Google Cloud.

* [Logging](/guides/monitor/logs/log-streaming/)",22
Google Cloud,Google Cloud Integration with Section for metrics and logs,"Learn how Section integrates with Google Cloud.

* [Logging](/guides/monitor/logs/log-streaming/)
",22
S3,S3 S3 Integration with Section for metrics and logs,"Learn how Section integrates with S3.

* [Logging](/guides/monitor/logs/log-streaming/)",22
S3,S3 Integration with Section for metrics and logs,"Learn how Section integrates with S3.

* [Logging](/guides/monitor/logs/log-streaming/)
",22
Terraform,Terraform Terraform Integration with Section,The [Terraform Kubernetes Provider](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs) can be used to spin up supported [Kubernetes resources](/explanations/kubernetes/#what-kubernetes-resources-are-supported-by-section) in Section. Learn more about [Terraform](https://developer.hashicorp.com/terraform) and how it facilitates [infrastructure as code (IaC)](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/infrastructure-as-code).,107
Terraform,Terraform Integration with Section,The [Terraform Kubernetes Provider](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs) can be used to spin up supported [Kubernetes resources](/explanations/kubernetes/#what-kubernetes-resources-are-supported-by-section) in Section. Learn more about [Terraform](https://developer.hashicorp.com/terraform) and how it facilitates [infrastructure as code (IaC)](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/infrastructure-as-code).,107
PolyScale,PolyScale PolyScale integration with Section for Global Database Edge Cache,"Learn how Section integrates with PolyScale.

* [Reduce Database Latency with Global Caching](/tutorials/data/polyscale-caching.md)
* [Mastodon Hosting at the Edge](/tutorials/mastodon.md)",47
PolyScale,PolyScale integration with Section for Global Database Edge Cache,"Learn how Section integrates with PolyScale.

* [Reduce Database Latency with Global Caching](/tutorials/data/polyscale-caching.md)
* [Mastodon Hosting at the Edge](/tutorials/mastodon.md)",47
Dynatrace,Dynatrace Dynatrace Integration with Section for metrics and logs,"Learn how Section integrates with Dynatrace.

* [Logging](/guides/monitor/logs/log-streaming/)",23
Dynatrace,Dynatrace Integration with Section for metrics and logs,"Learn how Section integrates with Dynatrace.

* [Logging](/guides/monitor/logs/log-streaming/)
",23
New Relic,New Relic New Relic Integration with Section for metrics and logs,"Learn how Section integrates with New Relic.

* [Metrics](/guides/monitor/exporting-telemetry/newrelic-section/)
* [Logging](/guides/monitor/logs/newrelic-logs/)",43
New Relic,New Relic Integration with Section for metrics and logs,"Learn how Section integrates with New Relic.

* [Metrics](/guides/monitor/exporting-telemetry/newrelic-section/)
* [Logging](/guides/monitor/logs/newrelic-logs/)
",43
Lens,Lens Lens Integration with Section for Kubernetes Integrated Development Environment,"Learn how Section integrates with Lens.

* [Lens IDE](/docs/guides/kubernetes-ui/other-ui/lens-integration/)",27
Lens,Lens Integration with Section for Kubernetes Integrated Development Environment,"Learn how Section integrates with Lens.

* [Lens IDE](/docs/guides/kubernetes-ui/other-ui/lens-integration/)
",27
Logtail,Logtail Logtail Integration with Section for metrics and logs,"Learn how Section integrates with Logtail.

* [Logging](/guides/monitor/logs/log-streaming/)",22
Logtail,Logtail Integration with Section for metrics and logs,"Learn how Section integrates with Logtail.

* [Logging](/guides/monitor/logs/log-streaming/)
",22
Elasticsearch,Elasticsearch Elasticsearch Integration with Section for metrics and logs,"Learn how Section integrates with Elasticsearch.

* [Logging](/guides/monitor/logs/log-streaming/)",21
Elasticsearch,Elasticsearch Integration with Section for metrics and logs,"Learn how Section integrates with Elasticsearch.

* [Logging](/guides/monitor/logs/log-streaming/)
",21
Grafana Cloud,Grafana Cloud Grafana Cloud Integration with Section for metrics and logs,"Learn how Section integrates with Grafana Cloud.

* [Metrics](/guides/monitor/exporting-telemetry/grafana-section/)
* [Logging](/guides/monitor/logs/grafana-loki/)",43
Grafana Cloud,Grafana Cloud Integration with Section for metrics and logs,"Learn how Section integrates with Grafana Cloud.

* [Metrics](/guides/monitor/exporting-telemetry/grafana-section/)
* [Logging](/guides/monitor/logs/grafana-loki/)
",43
1. Deploy A Project,Create a Project,"To begin navigate to the [Section Console](https://console.section.io/) and select **CREATE PROJECT**

![create-project](/img/docs/getting-started-create-project-select.png)

Provide the following information:
* Image Name - The name of your container image
* Port - The port you want to expose from your container
* Plan - The [Section Pricing Plan](https://section.io/pricing/) you want to use for this project

If you do not have a container you want to use for your first project, Section provides an example container, prefilled in the form, for you to test out.

When you are ready, select **Deploy** to launch your project.

![deploy-project](/img/docs/getting-started-deploy-project.png)

**That's It!** You now have a Project running on the Section platform. 

![project-card](/img/docs/getting-started-project-card.png)

Section automatically generates the following:

* Placement locations - We will configure your Section Project with a default [Location Optization](/explanations/aee/#locationoptimizer) strategy
* Global Routing - Your global HTTP traffic will be routed to the placement locations using our [Adaptive Edge Engine](/explanations/aee/)
* Project URL - Navigate to your application in the browser 
* DNS and SSL - Your Project URL will automatically have DNS routed and an SSL certificate provisioned 
* Kubernetes API - Run commands and make updates to your project through the Kubernetes API
* Kubernetes Dashboard - See the status of your deployments, optimize your locations and define resource allocations using the native Kubernetes dashboard


Next we will walk through how to check to see when your project URL has completed [Global DNS Propogation](/get-started/replace-demo-app/) so that you can navigate to your application in a browser.",370
1. Deploy A Project,Create your First Section Project and Deploy it to the Section Platform,"
Creating a Section Project is the first step when getting started with Section. If you do not already have a **Section Account**, you can make one for free, [here](https://section.io/). 

## Create a Project

To begin navigate to the [Section Console](https://console.section.io/) and select **CREATE PROJECT**

![create-project](/img/docs/getting-started-create-project-select.png)

Provide the following information:
* Image Name - The name of your container image
* Port - The port you want to expose from your container
* Plan - The [Section Pricing Plan](https://section.io/pricing/) you want to use for this project

If you do not have a container you want to use for your first project, Section provides an example container, prefilled in the form, for you to test out.

When you are ready, select **Deploy** to launch your project.

![deploy-project](/img/docs/getting-started-deploy-project.png)

**That's It!** You now have a Project running on the Section platform. 

![project-card](/img/docs/getting-started-project-card.png)

Section automatically generates the following:

* Placement locations - We will configure your Section Project with a default [Location Optization](/explanations/aee/#locationoptimizer) strategy
* Global Routing - Your global HTTP traffic will be routed to the placement locations using our [Adaptive Edge Engine](/explanations/aee/)
* Project URL - Navigate to your application in the browser 
* DNS and SSL - Your Project URL will automatically have DNS routed and an SSL certificate provisioned 
* Kubernetes API - Run commands and make updates to your project through the Kubernetes API
* Kubernetes Dashboard - See the status of your deployments, optimize your locations and define resource allocations using the native Kubernetes dashboard


Next we will walk through how to check to see when your project URL has completed [Global DNS Propogation](/get-started/replace-demo-app/) so that you can navigate to your application in a browser.
",418
5. Assign Domains,Add a Domain to Project,"To begin navigate to the [Section Console](https://console.section.io/) and for the Section Project you want to add a **Custom Domain** to and go into Project Settings by selecting the gear icon.

![project-settings](/img/docs/getting-started-project-settings.png)

Next, select **Domains** in the navigation menu. 

Add your new domain(s) by selecting **Add Domains** and putting in the hostname of your new domain, then hit **Save**.

Section will generate a new CNAME record for your project. Copy the CNAME record and configure your DNS to point to the Section provided CNAME record.

![custom-domain](/img/docs/getting-started-custom-domain.png)

You are now up and running with your own **Custom Domain**. You can verify that the domain's DNS is engaged by using the **Verify** option on the Domains settings page. 

You may now delete the section provided URL if you no longer have use for it. 

Next we will go over [Next Steps](/get-started/next-steps/) you can take with your **Section Project** such as additional Guides, Tutorials and ways to Monitor your project.",241
5. Assign Domains,Assign your own custom domain to your Section Project.,"
Now that you have your Section Project configured and running in the optimal locations, you may want to add your own **Custom Domain** that you control.  

## Add a Domain to Project

To begin navigate to the [Section Console](https://console.section.io/) and for the Section Project you want to add a **Custom Domain** to and go into Project Settings by selecting the gear icon.

![project-settings](/img/docs/getting-started-project-settings.png)

Next, select **Domains** in the navigation menu. 

Add your new domain(s) by selecting **Add Domains** and putting in the hostname of your new domain, then hit **Save**.

Section will generate a new CNAME record for your project. Copy the CNAME record and configure your DNS to point to the Section provided CNAME record.

![custom-domain](/img/docs/getting-started-custom-domain.png)

You are now up and running with your own **Custom Domain**. You can verify that the domain's DNS is engaged by using the **Verify** option on the Domains settings page. 

You may now delete the section provided URL if you no longer have use for it. 

Next we will go over [Next Steps](/get-started/next-steps/) you can take with your **Section Project** such as additional Guides, Tutorials and ways to Monitor your project.",280
3. Configure Your Project,Launch the Kubernetes Dashboard,"From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page. 

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.",63
3. Configure Your Project,Edit Deployment,"On the Kubernetes Dashboard you should see your deployment. In this example and if you chose to deploy our example container the deployment is named `section-project-deployment` and is running the image `sectionio/my-first-distributed-app:latest`. If you wanted to change out this image for another you will need to edit the deployment. 

![edit-deployment](/img/docs/getting-started-edit-deployment.png)

Edit the deployment YAML Section generated when you deployed your first project. To change the container image you will need to update the container spec with the `image` and `containerPort`.

![update-image-name](/img/docs/getting-started-update-image-name.png)

Select **Update** and your changes will be applied to your deployment. You can then use the Kubernetes dashboard to monitor the changes happening to your workload in real time. 

Your application will be running on a select set of Section's available global edge locations. In the next step we will walk through how to [Update App Locations](/get-started/demo-app-locations/) so you can optimize your deployment for performance and make full use of Section's dynamic and distributed platform.",232
3. Configure Your Project,Use the Kubernetes Dashboard to Manage the your containers running on the Section Platform,"
Once you have [Created a Project](/get-started/create-project/) you may want to make changes to the container running on the Section Platform. 

You can make changes to your project's containers using the Kubernetes Dashboard provided in the Section Console or by using the Kubernetes API. This doc will walk through how to make changes using the Kubernetes dashboard, for more information on how to connect via API see our [Kubernetes API documentation](/guides/kubernetes-ui/kubernetes-api/basics/).

## Launch the Kubernetes Dashboard

From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page. 

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.

## Edit Deployment

On the Kubernetes Dashboard you should see your deployment. In this example and if you chose to deploy our example container the deployment is named `section-project-deployment` and is running the image `sectionio/my-first-distributed-app:latest`. If you wanted to change out this image for another you will need to edit the deployment. 

![edit-deployment](/img/docs/getting-started-edit-deployment.png)

Edit the deployment YAML Section generated when you deployed your first project. To change the container image you will need to update the container spec with the `image` and `containerPort`.

![update-image-name](/img/docs/getting-started-update-image-name.png)

Select **Update** and your changes will be applied to your deployment. You can then use the Kubernetes dashboard to monitor the changes happening to your workload in real time. 

Your application will be running on a select set of Section's available global edge locations. In the next step we will walk through how to [Update App Locations](/get-started/demo-app-locations/) so you can optimize your deployment for performance and make full use of Section's dynamic and distributed platform.",404
2. Check DNS Propagation,Copy Project URL,"To begin navigate to the [Section Console](https://console.section.io/) and copy your **PROJECT URL** from the project you just created.

![project-url](/img/docs/getting-started-project-url.png)",44
2. Check DNS Propagation,Check DNS,"Next, bring up a [DNS Checker](https://dnschecker.org/) tool and paste the **PROJECT URL** and hit search. As your DNS propagates you will see more and more global locations resolve. Once you see all green checks your DNS has propagated and your **PROJECT URL** is live. 

![dns-checker](/img/docs/getting-started-dns-checker.png)",80
2. Check DNS Propagation,Visit Your Application,"You can now navigate to your application in the browser using the provided **PROJECT URL**. 


Now that you have an application running on Section, we will walk you through how to [Configure Your Project](/get-started/replace-demo-app/) deployment so that you can run your own application and learn to make changes.",65
2. Check DNS Propagation,Check to see the Section has propagated your DNS globally for your project,"
Now that you have created a [Section Project](/get-started/create-project/) we can now check to see that your projects DNS records have propagated across the internet. This process should only take a few minutes and there are a number of ways to check, in this example we will be using a free [DNS Checker](https://dnschecker.org/).  

## Copy Project URL 

To begin navigate to the [Section Console](https://console.section.io/) and copy your **PROJECT URL** from the project you just created.

![project-url](/img/docs/getting-started-project-url.png)

## Check DNS 

Next, bring up a [DNS Checker](https://dnschecker.org/) tool and paste the **PROJECT URL** and hit search. As your DNS propagates you will see more and more global locations resolve. Once you see all green checks your DNS has propagated and your **PROJECT URL** is live. 

![dns-checker](/img/docs/getting-started-dns-checker.png)

## Visit Your Application

You can now navigate to your application in the browser using the provided **PROJECT URL**. 


Now that you have an application running on Section, we will walk you through how to [Configure Your Project](/get-started/replace-demo-app/) deployment so that you can run your own application and learn to make changes. 
",278
6. Next Steps,Tutorials,"Section offers a wide range of **Tutorials** so you can further your learning around what is possible with Section.

We have tutorials that walk through a number of different **Frameworks** such as:
* [Node Express](/tutorials/frameworks/)
* [React](/tutorials/frameworks/)
* [NextJS](/tutorials/frameworks/)",71
6. Next Steps,Section Platform,"The Section Platform offers many additional features that may be applicaple to your **Section Project**. To read more more see our [Explanations](/docs/explanations/) docs for more information on things such as:

* [Adaptive Edge Engine](/explanations/aee/)
* [Anycast Network](/explanations/anycast/)
* [Horizontal Pod Autoscaler](/explanations/horizontal-pod-autoscaler/)
* [Composable Edge Cloud](explanations/cec/)",107
6. Next Steps,Upgrade your Plan,"To access more features, locations and higher resource limits, Upgrade your Project Plan to Standard or Pro Plan. 
* [Review Plans and Pricing](https://section.io/pricing/)
* [Upgrade your Plan](/docs/guides/projects/upgrade-plan/)",52
6. Next Steps,Monitoring,"Section offers a variety of ways to monitor your Section Project. 

* [Kubernetes Dashboard](/guides/monitor/monitoring-using-k8s-dashboard/): You can make use of the **Kubernetes Dashboard** to get real time information using standard Kubernetes outputs
* [Export Telemetry](/guides/monitor/exporting-telemetry/): Export metrics to tools such as Datadog and Grafana Cloud.
* [Project Logs](/guides/monitor/logs/): Ship your Project's logs to external tools such as Datadog and Grafana Loki.",117
6. Next Steps,"Learn more about our available Guides, Tutorials and options for Monitoring your Project","
Now that you have finished getting started with your first **Section Project**, you may want to continue your learning by using one of our many **Tutorials**, learning more about how **Section's Platform** works or by **Monitoring** your project to get logs and metrics. 

## Tutorials

Section offers a wide range of **Tutorials** so you can further your learning around what is possible with Section.

We have tutorials that walk through a number of different **Frameworks** such as:
* [Node Express](/tutorials/frameworks/)
* [React](/tutorials/frameworks/)
* [NextJS](/tutorials/frameworks/)

## Section Platform

The Section Platform offers many additional features that may be applicaple to your **Section Project**. To read more more see our [Explanations](/docs/explanations/) docs for more information on things such as:

* [Adaptive Edge Engine](/explanations/aee/)
* [Anycast Network](/explanations/anycast/)
* [Horizontal Pod Autoscaler](/explanations/horizontal-pod-autoscaler/)
* [Composable Edge Cloud](explanations/cec/)

## Upgrade your Plan
To access more features, locations and higher resource limits, Upgrade your Project Plan to Standard or Pro Plan. 
* [Review Plans and Pricing](https://section.io/pricing/)
* [Upgrade your Plan](/docs/guides/projects/upgrade-plan/)


## Monitoring
Section offers a variety of ways to monitor your Section Project. 

* [Kubernetes Dashboard](/guides/monitor/monitoring-using-k8s-dashboard/): You can make use of the **Kubernetes Dashboard** to get real time information using standard Kubernetes outputs
* [Export Telemetry](/guides/monitor/exporting-telemetry/): Export metrics to tools such as Datadog and Grafana Cloud.
* [Project Logs](/guides/monitor/logs/): Ship your Project's logs to external tools such as Datadog and Grafana Loki.

",422
4. Configure Locations,Launch the Kubernetes Dashboard,"From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page.

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.",62
4. Configure Locations,Create New Resource,"To update your Project location optimization strategy you must supply a **ConfigMap** named `location-optimizer`. For an explanation on the configuration of the ConfigMap see our [AEE Explanation](/explanations/aee/).

To add the ConfigMap to your deployment from the Kubernetes Dashboard you will add a new **resources** by selecting the plus in the upper right hand corner of the dashboard.

Select **Create from input** and paste in your `location-optimizer` **ConfigMap**

![location-optimizer](/img/docs/getting-started-location-optimizer.png)


Select **Upload** and Section will begin to apply your location optimization strategy to your deployment. You can monitor this on the overview page on the Kubernetes dashboard.

***NOTE:*** Projects using the free plan have limited access to location optimization strategies. For more see our [Pricing Information](https://section.io/pricing/)

Next we will walk you through how to assign your own [Custom Domain](/get-started/assign-domains/) to a project.",209
4. Configure Locations,Use the Kubernetes Dashboard to Manage the Locations for your Project,"
Now that you have [Created a Project](/get-started/create-project/) and [Configured the Deployment](/get-started/replace-demo-app/), you may want to adjust the [Location Strategy](/explanations/aee/#locationoptimizer) so that you can define where your workload will be deployed across Section's global network of compute locations.

You can make changes to your project's location strategy by using the Kubernetes Dashboard provided in the Section Console or by using the Kubernetes API. This doc will walk through how to make changes using the Kubernetes dashboard, for more information on how to connect via API see our [Kubernetes API documentation](/docs/guides/kubernetes-ui/kubernetes-api/basics/).

## Launch the Kubernetes Dashboard

From the [Section Console](https://console.section.io/) select **Launch Dashboard** On the Projects page.

![launch-dashboard](/img/docs/getting-started-launch-dashboard.png)

This will load up the native Kubernetes dashboard for this project. From here you can see and interact with all of your deployments.

## Create New Resource

To update your Project location optimization strategy you must supply a **ConfigMap** named `location-optimizer`. For an explanation on the configuration of the ConfigMap see our [AEE Explanation](/explanations/aee/).

To add the ConfigMap to your deployment from the Kubernetes Dashboard you will add a new **resources** by selecting the plus in the upper right hand corner of the dashboard.

Select **Create from input** and paste in your `location-optimizer` **ConfigMap**

![location-optimizer](/img/docs/getting-started-location-optimizer.png)


Select **Upload** and Section will begin to apply your location optimization strategy to your deployment. You can monitor this on the overview page on the Kubernetes dashboard.

***NOTE:*** Projects using the free plan have limited access to location optimization strategies. For more see our [Pricing Information](https://section.io/pricing/)

Next we will walk you through how to assign your own [Custom Domain](/get-started/assign-domains/) to a project.
",427
